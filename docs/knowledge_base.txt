

### Extracted from BERT-Pre-training of Deep Bidirectional Transformers for language understanding.pdf ###

BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding
Jacob Devlin
Ming-Wei Chang
Kenton Lee
Kristina Toutanova
Google AI Language
{jacobdevlin,mingweichang,kentonl,kristout}@google.com
Abstract
We introduce a new language representa-
tion model called BERT, which stands for
Bidirectional Encoder Representations from
Transformers. Unlike recent language repre-
sentation models (Peters et al., 2018a; Rad-
ford et al., 2018), BERT is designed to pre-
train deep bidirectional representations from
unlabeled text by jointly conditioning on both
left and right context in all layers. As a re-
sult, the pre-trained BERT model can be ﬁne-
tuned with just one additional output layer
to create state-of-the-art models for a wide
range of tasks, such as question answering and
language inference, without substantial task-
speciﬁc architecture modiﬁcations.
BERT is conceptually simple and empirically
powerful.
It obtains new state-of-the-art re-
sults on eleven natural language processing
tasks, including pushing the GLUE score to
80.5% (7.7% point absolute improvement),
MultiNLI accuracy to 86.7% (4.6% absolute
improvement), SQuAD v1.1 question answer-
ing Test F1 to 93.2 (1.5 point absolute im-
provement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
1
Introduction
Language model pre-training has been shown to
be effective for improving many natural language
processing tasks (Dai and Le, 2015; Peters et al.,
2018a; Radford et al., 2018; Howard and Ruder,
2018). These include sentence-level tasks such as
natural language inference (Bowman et al., 2015;
Williams et al., 2018) and paraphrasing (Dolan
and Brockett, 2005), which aim to predict the re-
lationships between sentences by analyzing them
holistically, as well as token-level tasks such as
named entity recognition and question answering,
where models are required to produce ﬁne-grained
output at the token level (Tjong Kim Sang and
De Meulder, 2003; Rajpurkar et al., 2016).
There are two existing strategies for apply-
ing pre-trained language representations to down-
stream tasks: feature-based and ﬁne-tuning. The
feature-based approach, such as ELMo (Peters
et al., 2018a), uses task-speciﬁc architectures that
include the pre-trained representations as addi-
tional features. The ﬁne-tuning approach, such as
the Generative Pre-trained Transformer (OpenAI
GPT) (Radford et al., 2018), introduces minimal
task-speciﬁc parameters, and is trained on the
downstream tasks by simply ﬁne-tuning all pre-
trained parameters. The two approaches share the
same objective function during pre-training, where
they use unidirectional language models to learn
general language representations.
We argue that current techniques restrict the
power of the pre-trained representations, espe-
cially for the ﬁne-tuning approaches.
The ma-
jor limitation is that standard language models are
unidirectional, and this limits the choice of archi-
tectures that can be used during pre-training. For
example, in OpenAI GPT, the authors use a left-to-
right architecture, where every token can only at-
tend to previous tokens in the self-attention layers
of the Transformer (Vaswani et al., 2017). Such re-
strictions are sub-optimal for sentence-level tasks,
and could be very harmful when applying ﬁne-
tuning based approaches to token-level tasks such
as question answering, where it is crucial to incor-
porate context from both directions.
In this paper, we improve the ﬁne-tuning based
approaches by proposing BERT: Bidirectional
Encoder
Representations
from
Transformers.
BERT alleviates the previously mentioned unidi-
rectionality constraint by using a “masked lan-
guage model” (MLM) pre-training objective, in-
spired by the Cloze task (Taylor, 1953).
The
masked language model randomly masks some of
the tokens from the input, and the objective is to
predict the original vocabulary id of the masked
arXiv:1810.04805v2  [cs.CL]  24 May 2019

116.51899719238281 | 70.6820068359375 | 481.02740478515625 | 100.96820831298828 | BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding
 |  | 
107.78799438476562 | 130.8321533203125 | 492.7443542480469 | 170.3608856201172 | Jacob Devlin
Ming-Wei Chang
Kenton Lee
Kristina Toutanova
Google AI Language
{jacobdevlin,mingweichang,kentonl,kristout}@google.com
 | 1 | 
158.89096069335938 | 224.32415771484375 | 203.37625122070312 | 236.27935791015625 | Abstract
 | 2 | 
89.00794982910156 | 245.81947326660156 | 273.2627868652344 | 423.1550598144531 | We introduce a new language representa-
tion model called BERT, which stands for
Bidirectional Encoder Representations from
Transformers. Unlike recent language repre-
sentation models (Peters et al., 2018a; Rad-
ford et al., 2018), BERT is designed to pre-
train deep bidirectional representations from
unlabeled text by jointly conditioning on both
left and right context in all layers. As a re-
sult, the pre-trained BERT model can be ﬁne-
tuned with just one additional output layer
to create state-of-the-art models for a wide
range of tasks, such as question answering and
language inference, without substantial task-
speciﬁc architecture modiﬁcations.
 | 3 | 
89.00794982910156 | 429.2594299316406 | 273.2563171386719 | 546.8179321289062 | BERT is conceptually simple and empirically
powerful.
It obtains new state-of-the-art re-
sults on eleven natural language processing
tasks, including pushing the GLUE score to
80.5% (7.7% point absolute improvement),
MultiNLI accuracy to 86.7% (4.6% absolute
improvement), SQuAD v1.1 question answer-
ing Test F1 to 93.2 (1.5 point absolute im-
provement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
 | 4 | 
71.99995422363281 | 557.10302734375 | 154.81362915039062 | 569.0582275390625 | 1
Introduction
 | 5 | 
71.99995422363281 | 578.5525512695312 | 290.2692565917969 | 765.6015625 | Language model pre-training has been shown to
be effective for improving many natural language
processing tasks (Dai and Le, 2015; Peters et al.,
2018a; Radford et al., 2018; Howard and Ruder,
2018). These include sentence-level tasks such as
natural language inference (Bowman et al., 2015;
Williams et al., 2018) and paraphrasing (Dolan
and Brockett, 2005), which aim to predict the re-
lationships between sentences by analyzing them
holistically, as well as token-level tasks such as
named entity recognition and question answering,
where models are required to produce ﬁne-grained
output at the token level (Tjong Kim Sang and
De Meulder, 2003; Rajpurkar et al., 2016).
 | 6 | 
307.27593994140625 | 225.22647094726562 | 525.5454711914062 | 765.6016845703125 | There are two existing strategies for apply-
ing pre-trained language representations to down-
stream tasks: feature-based and ﬁne-tuning. The
feature-based approach, such as ELMo (Peters
et al., 2018a), uses task-speciﬁc architectures that
include the pre-trained representations as addi-
tional features. The ﬁne-tuning approach, such as
the Generative Pre-trained Transformer (OpenAI
GPT) (Radford et al., 2018), introduces minimal
task-speciﬁc parameters, and is trained on the
downstream tasks by simply ﬁne-tuning all pre-
trained parameters. The two approaches share the
same objective function during pre-training, where
they use unidirectional language models to learn
general language representations.
We argue that current techniques restrict the
power of the pre-trained representations, espe-
cially for the ﬁne-tuning approaches.
The ma-
jor limitation is that standard language models are
unidirectional, and this limits the choice of archi-
tectures that can be used during pre-training. For
example, in OpenAI GPT, the authors use a left-to-
right architecture, where every token can only at-
tend to previous tokens in the self-attention layers
of the Transformer (Vaswani et al., 2017). Such re-
strictions are sub-optimal for sentence-level tasks,
and could be very harmful when applying ﬁne-
tuning based approaches to token-level tasks such
as question answering, where it is crucial to incor-
porate context from both directions.
In this paper, we improve the ﬁne-tuning based
approaches by proposing BERT: Bidirectional
Encoder
Representations
from
Transformers.
BERT alleviates the previously mentioned unidi-
rectionality constraint by using a “masked lan-
guage model” (MLM) pre-training objective, in-
spired by the Cloze task (Taylor, 1953).
The
masked language model randomly masks some of
the tokens from the input, and the objective is to
predict the original vocabulary id of the masked
 | 7 | 
10.940000534057617 | 256.5899658203125 | 37.619998931884766 | 609.8900146484375 | arXiv:1810.04805v2  [cs.CL]  24 May 2019
 | 8 | 
word based only on its context.
Unlike left-to-
right language model pre-training, the MLM ob-
jective enables the representation to fuse the left
and the right context, which allows us to pre-
train a deep bidirectional Transformer. In addi-
tion to the masked language model, we also use
a “next sentence prediction” task that jointly pre-
trains text-pair representations. The contributions
of our paper are as follows:
• We demonstrate the importance of bidirectional
pre-training for language representations. Un-
like Radford et al. (2018), which uses unidirec-
tional language models for pre-training, BERT
uses masked language models to enable pre-
trained deep bidirectional representations. This
is also in contrast to Peters et al. (2018a), which
uses a shallow concatenation of independently
trained left-to-right and right-to-left LMs.
• We show that pre-trained representations reduce
the need for many heavily-engineered task-
speciﬁc architectures. BERT is the ﬁrst ﬁne-
tuning based representation model that achieves
state-of-the-art performance on a large suite
of sentence-level and token-level tasks, outper-
forming many task-speciﬁc architectures.
• BERT advances the state of the art for eleven
NLP tasks.
The code and pre-trained mod-
els are available at https://github.com/
google-research/bert.
2
Related Work
There is a long history of pre-training general lan-
guage representations, and we brieﬂy review the
most widely-used approaches in this section.
2.1
Unsupervised Feature-based Approaches
Learning widely applicable representations of
words has been an active area of research for
decades, including non-neural (Brown et al., 1992;
Ando and Zhang, 2005; Blitzer et al., 2006) and
neural (Mikolov et al., 2013; Pennington et al.,
2014) methods.
Pre-trained word embeddings
are an integral part of modern NLP systems, of-
fering signiﬁcant improvements over embeddings
learned from scratch (Turian et al., 2010). To pre-
train word embedding vectors, left-to-right lan-
guage modeling objectives have been used (Mnih
and Hinton, 2009), as well as objectives to dis-
criminate correct from incorrect words in left and
right context (Mikolov et al., 2013).
These approaches have been generalized to
coarser granularities, such as sentence embed-
dings (Kiros et al., 2015; Logeswaran and Lee,
2018) or paragraph embeddings (Le and Mikolov,
2014).
To train sentence representations, prior
work has used objectives to rank candidate next
sentences (Jernite et al., 2017; Logeswaran and
Lee, 2018), left-to-right generation of next sen-
tence words given a representation of the previous
sentence (Kiros et al., 2015), or denoising auto-
encoder derived objectives (Hill et al., 2016).
ELMo and its predecessor (Peters et al., 2017,
2018a) generalize traditional word embedding re-
search along a different dimension. They extract
context-sensitive features from a left-to-right and a
right-to-left language model. The contextual rep-
resentation of each token is the concatenation of
the left-to-right and right-to-left representations.
When integrating contextual word embeddings
with existing task-speciﬁc architectures, ELMo
advances the state of the art for several major NLP
benchmarks (Peters et al., 2018a) including ques-
tion answering (Rajpurkar et al., 2016), sentiment
analysis (Socher et al., 2013), and named entity
recognition (Tjong Kim Sang and De Meulder,
2003). Melamud et al. (2016) proposed learning
contextual representations through a task to pre-
dict a single word from both left and right context
using LSTMs. Similar to ELMo, their model is
feature-based and not deeply bidirectional. Fedus
et al. (2018) shows that the cloze task can be used
to improve the robustness of text generation mod-
els.
2.2
Unsupervised Fine-tuning Approaches
As with the feature-based approaches, the ﬁrst
works in this direction only pre-trained word em-
bedding parameters from unlabeled text
(Col-
lobert and Weston, 2008).
More recently, sentence or document encoders
which produce contextual token representations
have been pre-trained from unlabeled text and
ﬁne-tuned for a supervised downstream task (Dai
and Le, 2015; Howard and Ruder, 2018; Radford
et al., 2018). The advantage of these approaches
is that few parameters need to be learned from
scratch.
At least partly due to this advantage,
OpenAI GPT (Radford et al., 2018) achieved pre-
viously state-of-the-art results on many sentence-
level tasks from the GLUE benchmark (Wang
et al., 2018a).
Left-to-right language model-

72.0 | 65.49368286132812 | 290.2693176269531 | 184.7968292236328 | word based only on its context.
Unlike left-to-
right language model pre-training, the MLM ob-
jective enables the representation to fuse the left
and the right context, which allows us to pre-
train a deep bidirectional Transformer. In addi-
tion to the masked language model, we also use
a “next sentence prediction” task that jointly pre-
trains text-pair representations. The contributions
of our paper are as follows:
 |  | 
74.11000061035156 | 194.37173461914062 | 290.2691955566406 | 313.67486572265625 | • We demonstrate the importance of bidirectional
pre-training for language representations. Un-
like Radford et al. (2018), which uses unidirec-
tional language models for pre-training, BERT
uses masked language models to enable pre-
trained deep bidirectional representations. This
is also in contrast to Peters et al. (2018a), which
uses a shallow concatenation of independently
trained left-to-right and right-to-left LMs.
 | 1 | 
74.11000061035156 | 324.4687805175781 | 290.2691955566406 | 416.67291259765625 | • We show that pre-trained representations reduce
the need for many heavily-engineered task-
speciﬁc architectures. BERT is the ﬁrst ﬁne-
tuning based representation model that achieves
state-of-the-art performance on a large suite
of sentence-level and token-level tasks, outper-
forming many task-speciﬁc architectures.
 | 2 | 
74.1099853515625 | 427.4668273925781 | 290.2691650390625 | 479.02392578125 | • BERT advances the state of the art for eleven
NLP tasks.
The code and pre-trained mod-
els are available at https://github.com/
google-research/bert.
 | 3 | 
71.99998474121094 | 490.5783386230469 | 161.0901336669922 | 502.5335388183594 | 2
Related Work
 | 4 | 
71.99998474121094 | 511.9928283691406 | 290.26922607421875 | 549.9999389648438 | There is a long history of pre-training general lan-
guage representations, and we brieﬂy review the
most widely-used approaches in this section.
 | 5 | 
71.99998474121094 | 560.977294921875 | 287.00750732421875 | 571.8863525390625 | 2.1
Unsupervised Feature-based Approaches
 | 6 | 
71.99998474121094 | 578.5528564453125 | 290.2693176269531 | 765.6019287109375 | Learning widely applicable representations of
words has been an active area of research for
decades, including non-neural (Brown et al., 1992;
Ando and Zhang, 2005; Blitzer et al., 2006) and
neural (Mikolov et al., 2013; Pennington et al.,
2014) methods.
Pre-trained word embeddings
are an integral part of modern NLP systems, of-
fering signiﬁcant improvements over embeddings
learned from scratch (Turian et al., 2010). To pre-
train word embedding vectors, left-to-right lan-
guage modeling objectives have been used (Mnih
and Hinton, 2009), as well as objectives to dis-
criminate correct from incorrect words in left and
right context (Mikolov et al., 2013).
 | 7 | 
307.2760009765625 | 65.49386596679688 | 525.5470581054688 | 511.6591796875 | These approaches have been generalized to
coarser granularities, such as sentence embed-
dings (Kiros et al., 2015; Logeswaran and Lee,
2018) or paragraph embeddings (Le and Mikolov,
2014).
To train sentence representations, prior
work has used objectives to rank candidate next
sentences (Jernite et al., 2017; Logeswaran and
Lee, 2018), left-to-right generation of next sen-
tence words given a representation of the previous
sentence (Kiros et al., 2015), or denoising auto-
encoder derived objectives (Hill et al., 2016).
ELMo and its predecessor (Peters et al., 2017,
2018a) generalize traditional word embedding re-
search along a different dimension. They extract
context-sensitive features from a left-to-right and a
right-to-left language model. The contextual rep-
resentation of each token is the concatenation of
the left-to-right and right-to-left representations.
When integrating contextual word embeddings
with existing task-speciﬁc architectures, ELMo
advances the state of the art for several major NLP
benchmarks (Peters et al., 2018a) including ques-
tion answering (Rajpurkar et al., 2016), sentiment
analysis (Socher et al., 2013), and named entity
recognition (Tjong Kim Sang and De Meulder,
2003). Melamud et al. (2016) proposed learning
contextual representations through a task to pre-
dict a single word from both left and right context
using LSTMs. Similar to ELMo, their model is
feature-based and not deeply bidirectional. Fedus
et al. (2018) shows that the cloze task can be used
to improve the robustness of text generation mod-
els.
 | 8 | 
307.2760009765625 | 528.8594970703125 | 510.6542663574219 | 539.7685546875 | 2.2
Unsupervised Fine-tuning Approaches
 | 9 | 
307.2760009765625 | 549.7730712890625 | 525.5452880859375 | 765.6021118164062 | As with the feature-based approaches, the ﬁrst
works in this direction only pre-trained word em-
bedding parameters from unlabeled text
(Col-
lobert and Weston, 2008).
More recently, sentence or document encoders
which produce contextual token representations
have been pre-trained from unlabeled text and
ﬁne-tuned for a supervised downstream task (Dai
and Le, 2015; Howard and Ruder, 2018; Radford
et al., 2018). The advantage of these approaches
is that few parameters need to be learned from
scratch.
At least partly due to this advantage,
OpenAI GPT (Radford et al., 2018) achieved pre-
viously state-of-the-art results on many sentence-
level tasks from the GLUE benchmark (Wang
et al., 2018a).
Left-to-right language model-
 | 10 | 
BERT
BERT
E[CLS]
E1
 E[SEP]
...
EN
E1’
...
EM’
C
T1
T[SEP]
...
TN
T1’
...
TM’
[CLS]
Tok 1
 [SEP]
...
Tok N
Tok 1
...
TokM
Question
Paragraph
Start/End Span
BERT
E[CLS]
E1
 E[SEP]
...
EN
E1’
...
EM’
C
T1
T[SEP]
...
TN
T1’
...
TM’
[CLS]
Tok 1
 [SEP]
...
Tok N
Tok 1
...
TokM
Masked Sentence A
Masked Sentence B
Pre-training
Fine-Tuning
NSP
Mask LM
Mask LM
Unlabeled Sentence A and B Pair 
SQuAD
Question Answer Pair
NER
MNLI
Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-
tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize
models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special
symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-
tions/answers).
ing and auto-encoder objectives have been used
for pre-training such models (Howard and Ruder,
2018; Radford et al., 2018; Dai and Le, 2015).
2.3
Transfer Learning from Supervised Data
There has also been work showing effective trans-
fer from supervised tasks with large datasets, such
as natural language inference (Conneau et al.,
2017) and machine translation (McCann et al.,
2017). Computer vision research has also demon-
strated the importance of transfer learning from
large pre-trained models, where an effective recipe
is to ﬁne-tune models pre-trained with Ima-
geNet (Deng et al., 2009; Yosinski et al., 2014).
3
BERT
We introduce BERT and its detailed implementa-
tion in this section. There are two steps in our
framework: pre-training and ﬁne-tuning.
Dur-
ing pre-training, the model is trained on unlabeled
data over different pre-training tasks.
For ﬁne-
tuning, the BERT model is ﬁrst initialized with
the pre-trained parameters, and all of the param-
eters are ﬁne-tuned using labeled data from the
downstream tasks. Each downstream task has sep-
arate ﬁne-tuned models, even though they are ini-
tialized with the same pre-trained parameters. The
question-answering example in Figure 1 will serve
as a running example for this section.
A distinctive feature of BERT is its uniﬁed ar-
chitecture across different tasks. There is mini-
mal difference between the pre-trained architec-
ture and the ﬁnal downstream architecture.
Model Architecture
BERT’s model architec-
ture is a multi-layer bidirectional Transformer en-
coder based on the original implementation de-
scribed in Vaswani et al. (2017) and released in
the tensor2tensor library.1 Because the use
of Transformers has become common and our im-
plementation is almost identical to the original,
we will omit an exhaustive background descrip-
tion of the model architecture and refer readers to
Vaswani et al. (2017) as well as excellent guides
such as “The Annotated Transformer.”2
In this work, we denote the number of layers
(i.e., Transformer blocks) as L, the hidden size as
H, and the number of self-attention heads as A.3
We primarily report results on two model sizes:
BERTBASE (L=12, H=768, A=12, Total Param-
eters=110M) and BERTLARGE (L=24, H=1024,
A=16, Total Parameters=340M).
BERTBASE was chosen to have the same model
size as OpenAI GPT for comparison purposes.
Critically, however, the BERT Transformer uses
bidirectional self-attention, while the GPT Trans-
former uses constrained self-attention where every
token can only attend to context to its left.4
1https://github.com/tensorﬂow/tensor2tensor
2http://nlp.seas.harvard.edu/2018/04/03/attention.html
3In all cases we set the feed-forward/ﬁlter size to be 4H,
i.e., 3072 for the H = 768 and 4096 for the H = 1024.
4We note that in the literature the bidirectional Trans-

371.2417297363281 | 123.7138671875 | 443.2789001464844 | 134.82106018066406 | BERT
BERT
 |  | 
351.0950927734375 | 147.98489379882812 | 501.73468017578125 | 155.25082397460938 | E[CLS]
E1
 E[SEP]
...
EN
E1’
...
EM’
 | 1 | 
354.1130065917969 | 99.09750366210938 | 501.5787353515625 | 106.67436218261719 | C
T1
T[SEP]
...
TN
T1’
...
TM’
 | 2 | 
352.0694885253906 | 172.30682373046875 | 502.1527404785156 | 178.7860107421875 | [CLS]
Tok 1
 [SEP]
...
Tok N
Tok 1
...
TokM
 | 3 | 
380.7937927246094 | 196.17564392089844 | 492.612548828125 | 202.95819091796875 | Question
Paragraph
 | 4 | 
458.29461669921875 | 77.65646362304688 | 502.19122314453125 | 84.13566589355469 | Start/End Span
 | 5 | 
151.5334014892578 | 123.7138671875 | 180.9230499267578 | 134.82106018066406 | BERT
 | 6 | 
91.4997329711914 | 147.98489379882812 | 236.72508239746094 | 155.25082397460938 | E[CLS]
E1
 E[SEP]
...
EN
E1’
...
EM’
 | 7 | 
94.52854919433594 | 99.09750366210938 | 236.56915283203125 | 106.67436218261719 | C
T1
T[SEP]
...
TN
T1’
...
TM’
 | 8 | 
92.48503875732422 | 172.30682373046875 | 237.1431884765625 | 178.7860107421875 | [CLS]
Tok 1
 [SEP]
...
Tok N
Tok 1
...
TokM
 | 9 | 
103.36181640625 | 196.1752471923828 | 241.6565704345703 | 202.9578094482422 | Masked Sentence A
Masked Sentence B
 | 10 | 
139.20904541015625 | 230.23052978515625 | 447.9937744140625 | 241.3377227783203 | Pre-training
Fine-Tuning
 | 11 | 
89.39872741699219 | 75.22953033447266 | 223.96707153320312 | 81.70873260498047 | NSP
Mask LM
Mask LM
 | 12 | 
121.31953430175781 | 213.16416931152344 | 218.85739135742188 | 219.6433563232422 | Unlabeled Sentence A and B Pair 
 | 13 | 
347.6594543457031 | 73.08335876464844 | 371.93560791015625 | 80.02535247802734 | SQuAD
 | 14 | 
405.9870300292969 | 210.7372283935547 | 468.24566650390625 | 217.21641540527344 | Question Answer Pair
 | 15 | 
303.20013427734375 | 73.24008178710938 | 340.0621643066406 | 80.18207550048828 | NER
MNLI
 | 16 | 
71.99996948242188 | 259.05645751953125 | 525.5473022460938 | 316.8401184082031 | Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-
tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize
models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special
symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-
tions/answers).
 | 17 | 
71.99996948242188 | 340.6307067871094 | 290.269287109375 | 378.6378173828125 | ing and auto-encoder objectives have been used
for pre-training such models (Howard and Ruder,
2018; Radford et al., 2018; Dai and Le, 2015).
 | 18 | 
71.99996948242188 | 391.2281188964844 | 287.37835693359375 | 402.1372375488281 | 2.3
Transfer Learning from Supervised Data
 | 19 | 
71.99996948242188 | 409.4997253417969 | 290.2692565917969 | 528.8028564453125 | There has also been work showing effective trans-
fer from supervised tasks with large datasets, such
as natural language inference (Conneau et al.,
2017) and machine translation (McCann et al.,
2017). Computer vision research has also demon-
strated the importance of transfer learning from
large pre-trained models, where an effective recipe
is to ﬁne-tune models pre-trained with Ima-
geNet (Deng et al., 2009; Yosinski et al., 2014).
 | 20 | 
71.99996948242188 | 541.9702758789062 | 122.00857543945312 | 553.9254760742188 | 3
BERT
 | 21 | 
71.99996948242188 | 564.65380859375 | 290.269287109375 | 765.601806640625 | We introduce BERT and its detailed implementa-
tion in this section. There are two steps in our
framework: pre-training and ﬁne-tuning.
Dur-
ing pre-training, the model is trained on unlabeled
data over different pre-training tasks.
For ﬁne-
tuning, the BERT model is ﬁrst initialized with
the pre-trained parameters, and all of the param-
eters are ﬁne-tuned using labeled data from the
downstream tasks. Each downstream task has sep-
arate ﬁne-tuned models, even though they are ini-
tialized with the same pre-trained parameters. The
question-answering example in Figure 1 will serve
as a running example for this section.
A distinctive feature of BERT is its uniﬁed ar-
chitecture across different tasks. There is mini-
 | 22 | 
307.2760009765625 | 340.6307373046875 | 525.5450439453125 | 365.0888366699219 | mal difference between the pre-trained architec-
ture and the ﬁnal downstream architecture.
 | 23 | 
307.2760009765625 | 377.2501525878906 | 525.5486450195312 | 523.7509155273438 | Model Architecture
BERT’s model architec-
ture is a multi-layer bidirectional Transformer en-
coder based on the original implementation de-
scribed in Vaswani et al. (2017) and released in
the tensor2tensor library.1 Because the use
of Transformers has become common and our im-
plementation is almost identical to the original,
we will omit an exhaustive background descrip-
tion of the model architecture and refer readers to
Vaswani et al. (2017) as well as excellent guides
such as “The Annotated Transformer.”2
 | 24 | 
307.2760009765625 | 527.0428466796875 | 525.5450439453125 | 565.0509033203125 | In this work, we denote the number of layers
(i.e., Transformer blocks) as L, the hidden size as
H, and the number of self-attention heads as A.3
 | 25 | 
307.2760009765625 | 567.6907958984375 | 525.5484619140625 | 701.1948852539062 | We primarily report results on two model sizes:
BERTBASE (L=12, H=768, A=12, Total Param-
eters=110M) and BERTLARGE (L=24, H=1024,
A=16, Total Parameters=340M).
BERTBASE was chosen to have the same model
size as OpenAI GPT for comparison purposes.
Critically, however, the BERT Transformer uses
bidirectional self-attention, while the GPT Trans-
former uses constrained self-attention where every
token can only attend to context to its left.4
 | 26 | 
307.2760009765625 | 712.0377197265625 | 525.5443725585938 | 765.1323852539062 | 1https://github.com/tensorﬂow/tensor2tensor
2http://nlp.seas.harvard.edu/2018/04/03/attention.html
3In all cases we set the feed-forward/ﬁlter size to be 4H,
i.e., 3072 for the H = 768 and 4096 for the H = 1024.
4We note that in the literature the bidirectional Trans-
 | 27 | 
Input/Output Representations
To make BERT
handle a variety of down-stream tasks, our input
representation is able to unambiguously represent
both a single sentence and a pair of sentences
(e.g., ⟨Question, Answer ⟩) in one token sequence.
Throughout this work, a “sentence” can be an arbi-
trary span of contiguous text, rather than an actual
linguistic sentence. A “sequence” refers to the in-
put token sequence to BERT, which may be a sin-
gle sentence or two sentences packed together.
We use WordPiece embeddings (Wu et al.,
2016) with a 30,000 token vocabulary. The ﬁrst
token of every sequence is always a special clas-
siﬁcation token ([CLS]). The ﬁnal hidden state
corresponding to this token is used as the ag-
gregate sequence representation for classiﬁcation
tasks. Sentence pairs are packed together into a
single sequence. We differentiate the sentences in
two ways. First, we separate them with a special
token ([SEP]). Second, we add a learned embed-
ding to every token indicating whether it belongs
to sentence A or sentence B. As shown in Figure 1,
we denote input embedding as E, the ﬁnal hidden
vector of the special [CLS] token as C ∈RH,
and the ﬁnal hidden vector for the ith input token
as Ti ∈RH.
For a given token, its input representation is
constructed by summing the corresponding token,
segment, and position embeddings. A visualiza-
tion of this construction can be seen in Figure 2.
3.1
Pre-training BERT
Unlike Peters et al. (2018a) and Radford et al.
(2018), we do not use traditional left-to-right or
right-to-left language models to pre-train BERT.
Instead, we pre-train BERT using two unsuper-
vised tasks, described in this section. This step
is presented in the left part of Figure 1.
Task #1: Masked LM
Intuitively, it is reason-
able to believe that a deep bidirectional model is
strictly more powerful than either a left-to-right
model or the shallow concatenation of a left-to-
right and a right-to-left model.
Unfortunately,
standard conditional language models can only be
trained left-to-right or right-to-left, since bidirec-
tional conditioning would allow each word to in-
directly “see itself”, and the model could trivially
predict the target word in a multi-layered context.
former is often referred to as a “Transformer encoder” while
the left-context-only version is referred to as a “Transformer
decoder” since it can be used for text generation.
In order to train a deep bidirectional representa-
tion, we simply mask some percentage of the input
tokens at random, and then predict those masked
tokens. We refer to this procedure as a “masked
LM” (MLM), although it is often referred to as a
Cloze task in the literature (Taylor, 1953). In this
case, the ﬁnal hidden vectors corresponding to the
mask tokens are fed into an output softmax over
the vocabulary, as in a standard LM. In all of our
experiments, we mask 15% of all WordPiece to-
kens in each sequence at random. In contrast to
denoising auto-encoders (Vincent et al., 2008), we
only predict the masked words rather than recon-
structing the entire input.
Although this allows us to obtain a bidirec-
tional pre-trained model, a downside is that we
are creating a mismatch between pre-training and
ﬁne-tuning, since the [MASK] token does not ap-
pear during ﬁne-tuning. To mitigate this, we do
not always replace “masked” words with the ac-
tual [MASK] token. The training data generator
chooses 15% of the token positions at random for
prediction. If the i-th token is chosen, we replace
the i-th token with (1) the [MASK] token 80% of
the time (2) a random token 10% of the time (3)
the unchanged i-th token 10% of the time. Then,
Ti will be used to predict the original token with
cross entropy loss. We compare variations of this
procedure in Appendix C.2.
Task #2:
Next Sentence Prediction (NSP)
Many important downstream tasks such as Ques-
tion Answering (QA) and Natural Language Infer-
ence (NLI) are based on understanding the rela-
tionship between two sentences, which is not di-
rectly captured by language modeling. In order
to train a model that understands sentence rela-
tionships, we pre-train for a binarized next sen-
tence prediction task that can be trivially gener-
ated from any monolingual corpus. Speciﬁcally,
when choosing the sentences A and B for each pre-
training example, 50% of the time B is the actual
next sentence that follows A (labeled as IsNext),
and 50% of the time it is a random sentence from
the corpus (labeled as NotNext).
As we show
in Figure 1, C is used for next sentence predic-
tion (NSP).5 Despite its simplicity, we demon-
strate in Section 5.1 that pre-training towards this
task is very beneﬁcial to both QA and NLI. 6
5The ﬁnal model achieves 97%-98% accuracy on NSP.
6The vector C is not a meaningful sentence representation
without ﬁne-tuning, since it was trained with NSP.

71.99998474121094 | 65.39408111572266 | 290.2729187011719 | 470.3929443359375 | Input/Output Representations
To make BERT
handle a variety of down-stream tasks, our input
representation is able to unambiguously represent
both a single sentence and a pair of sentences
(e.g., ⟨Question, Answer ⟩) in one token sequence.
Throughout this work, a “sentence” can be an arbi-
trary span of contiguous text, rather than an actual
linguistic sentence. A “sequence” refers to the in-
put token sequence to BERT, which may be a sin-
gle sentence or two sentences packed together.
We use WordPiece embeddings (Wu et al.,
2016) with a 30,000 token vocabulary. The ﬁrst
token of every sequence is always a special clas-
siﬁcation token ([CLS]). The ﬁnal hidden state
corresponding to this token is used as the ag-
gregate sequence representation for classiﬁcation
tasks. Sentence pairs are packed together into a
single sequence. We differentiate the sentences in
two ways. First, we separate them with a special
token ([SEP]). Second, we add a learned embed-
ding to every token indicating whether it belongs
to sentence A or sentence B. As shown in Figure 1,
we denote input embedding as E, the ﬁnal hidden
vector of the special [CLS] token as C ∈RH,
and the ﬁnal hidden vector for the ith input token
as Ti ∈RH.
For a given token, its input representation is
constructed by summing the corresponding token,
segment, and position embeddings. A visualiza-
tion of this construction can be seen in Figure 2.
 |  | 
71.99998474121094 | 483.61224365234375 | 185.9127960205078 | 494.5213623046875 | 3.1
Pre-training BERT
 | 1 | 
71.99998474121094 | 502.2438659667969 | 290.2693176269531 | 580.8989868164062 | Unlike Peters et al. (2018a) and Radford et al.
(2018), we do not use traditional left-to-right or
right-to-left language models to pre-train BERT.
Instead, we pre-train BERT using two unsuper-
vised tasks, described in this section. This step
is presented in the left part of Figure 1.
 | 2 | 
71.99998474121094 | 592.6453247070312 | 290.27069091796875 | 725.595947265625 | Task #1: Masked LM
Intuitively, it is reason-
able to believe that a deep bidirectional model is
strictly more powerful than either a left-to-right
model or the shallow concatenation of a left-to-
right and a right-to-left model.
Unfortunately,
standard conditional language models can only be
trained left-to-right or right-to-left, since bidirec-
tional conditioning would allow each word to in-
directly “see itself”, and the model could trivially
predict the target word in a multi-layered context.
 | 3 | 
72.0 | 736.240966796875 | 290.26904296875 | 765.1323852539062 | former is often referred to as a “Transformer encoder” while
the left-context-only version is referred to as a “Transformer
decoder” since it can be used for text generation.
 | 4 | 
307.2760009765625 | 65.49368286132812 | 525.548828125 | 455.78094482421875 | In order to train a deep bidirectional representa-
tion, we simply mask some percentage of the input
tokens at random, and then predict those masked
tokens. We refer to this procedure as a “masked
LM” (MLM), although it is often referred to as a
Cloze task in the literature (Taylor, 1953). In this
case, the ﬁnal hidden vectors corresponding to the
mask tokens are fed into an output softmax over
the vocabulary, as in a standard LM. In all of our
experiments, we mask 15% of all WordPiece to-
kens in each sequence at random. In contrast to
denoising auto-encoders (Vincent et al., 2008), we
only predict the masked words rather than recon-
structing the entire input.
Although this allows us to obtain a bidirec-
tional pre-trained model, a downside is that we
are creating a mismatch between pre-training and
ﬁne-tuning, since the [MASK] token does not ap-
pear during ﬁne-tuning. To mitigate this, we do
not always replace “masked” words with the ac-
tual [MASK] token. The training data generator
chooses 15% of the token positions at random for
prediction. If the i-th token is chosen, we replace
the i-th token with (1) the [MASK] token 80% of
the time (2) a random token 10% of the time (3)
the unchanged i-th token 10% of the time. Then,
Ti will be used to predict the original token with
cross entropy loss. We compare variations of this
procedure in Appendix C.2.
 | 5 | 
307.2760009765625 | 471.1242370605469 | 525.5462036132812 | 726.0179443359375 | Task #2:
Next Sentence Prediction (NSP)
Many important downstream tasks such as Ques-
tion Answering (QA) and Natural Language Infer-
ence (NLI) are based on understanding the rela-
tionship between two sentences, which is not di-
rectly captured by language modeling. In order
to train a model that understands sentence rela-
tionships, we pre-train for a binarized next sen-
tence prediction task that can be trivially gener-
ated from any monolingual corpus. Speciﬁcally,
when choosing the sentences A and B for each pre-
training example, 50% of the time B is the actual
next sentence that follows A (labeled as IsNext),
and 50% of the time it is a random sentence from
the corpus (labeled as NotNext).
As we show
in Figure 1, C is used for next sentence predic-
tion (NSP).5 Despite its simplicity, we demon-
strate in Section 5.1 that pre-training towards this
task is very beneﬁcial to both QA and NLI. 6
 | 6 | 
307.2760009765625 | 733.7866821289062 | 525.5399780273438 | 765.1323852539062 | 5The ﬁnal model achieves 97%-98% accuracy on NSP.
6The vector C is not a meaningful sentence representation
without ﬁne-tuning, since it was trained with NSP.
 | 7 | 
[CLS]
he
likes
play
##ing
[SEP]
my
dog
is
cute
[SEP]
Input
E[CLS]
Ehe
Elikes
Eplay
E##ing
E[SEP]
Emy
Edog
Eis
Ecute
E[SEP]
Token
Embeddings
EA
EB
EB
EB
EB
EB
EA
EA
EA
EA
EA
Segment
Embeddings
E0
E6
E7
E8
E9
E10
E1
E2
E3
E4
E5
Position
Embeddings
Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-
tion embeddings and the position embeddings.
The NSP task is closely related to representation-
learning objectives used in Jernite et al. (2017) and
Logeswaran and Lee (2018). However, in prior
work, only sentence embeddings are transferred to
down-stream tasks, where BERT transfers all pa-
rameters to initialize end-task model parameters.
Pre-training data The pre-training procedure
largely follows the existing literature on language
model pre-training. For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al.,
2015) and English Wikipedia (2,500M words).
For Wikipedia we extract only the text passages
and ignore lists, tables, and headers. It is criti-
cal to use a document-level corpus rather than a
shufﬂed sentence-level corpus such as the Billion
Word Benchmark (Chelba et al., 2013) in order to
extract long contiguous sequences.
3.2
Fine-tuning BERT
Fine-tuning is straightforward since the self-
attention mechanism in the Transformer al-
lows BERT to model many downstream tasks—
whether they involve single text or text pairs—by
swapping out the appropriate inputs and outputs.
For applications involving text pairs, a common
pattern is to independently encode text pairs be-
fore applying bidirectional cross attention, such
as Parikh et al. (2016); Seo et al. (2017). BERT
instead uses the self-attention mechanism to unify
these two stages, as encoding a concatenated text
pair with self-attention effectively includes bidi-
rectional cross attention between two sentences.
For each task, we simply plug in the task-
speciﬁc inputs and outputs into BERT and ﬁne-
tune all the parameters end-to-end.
At the in-
put, sentence A and sentence B from pre-training
are analogous to (1) sentence pairs in paraphras-
ing, (2) hypothesis-premise pairs in entailment, (3)
question-passage pairs in question answering, and
(4) a degenerate text-∅pair in text classiﬁcation
or sequence tagging. At the output, the token rep-
resentations are fed into an output layer for token-
level tasks, such as sequence tagging or question
answering, and the [CLS] representation is fed
into an output layer for classiﬁcation, such as en-
tailment or sentiment analysis.
Compared to pre-training, ﬁne-tuning is rela-
tively inexpensive. All of the results in the pa-
per can be replicated in at most 1 hour on a sin-
gle Cloud TPU, or a few hours on a GPU, starting
from the exact same pre-trained model.7 We de-
scribe the task-speciﬁc details in the correspond-
ing subsections of Section 4. More details can be
found in Appendix A.5.
4
Experiments
In this section, we present BERT ﬁne-tuning re-
sults on 11 NLP tasks.
4.1
GLUE
The General Language Understanding Evaluation
(GLUE) benchmark (Wang et al., 2018a) is a col-
lection of diverse natural language understanding
tasks. Detailed descriptions of GLUE datasets are
included in Appendix B.1.
To ﬁne-tune on GLUE, we represent the input
sequence (for single sentence or sentence pairs)
as described in Section 3, and use the ﬁnal hid-
den vector C ∈RH corresponding to the ﬁrst
input token ([CLS]) as the aggregate representa-
tion. The only new parameters introduced during
ﬁne-tuning are classiﬁcation layer weights W ∈
RK×H, where K is the number of labels. We com-
pute a standard classiﬁcation loss with C and W,
i.e., log(softmax(CW T )).
7For example, the BERT SQuAD model can be trained in
around 30 minutes on a single Cloud TPU to achieve a Dev
F1 score of 91.0%.
8See (10) in https://gluebenchmark.com/faq.

117.34532928466797 | 68.17323303222656 | 465.1729431152344 | 76.82730865478516 | [CLS]
he
likes
play
##ing
[SEP]
my
dog
is
cute
[SEP]
Input
 |  | 
117.34532928466797 | 90.58194732666016 | 466.16741943359375 | 105.2615966796875 | E[CLS]
Ehe
Elikes
Eplay
E##ing
E[SEP]
Emy
Edog
Eis
Ecute
E[SEP]
Token
Embeddings
 | 1 | 
117.34532928466797 | 119.4599380493164 | 462.4053955078125 | 134.1395721435547 | EA
EB
EB
EB
EB
EB
EA
EA
EA
EA
EA
Segment
Embeddings
 | 2 | 
117.34532928466797 | 151.2257537841797 | 464.092041015625 | 165.90538024902344 | E0
E6
E7
E8
E9
E10
E1
E2
E3
E4
E5
Position
Embeddings
 | 3 | 
72.0 | 184.44947814941406 | 525.5471801757812 | 206.36709594726562 | Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-
tion embeddings and the position embeddings.
 | 4 | 
72.0 | 230.15768432617188 | 290.2693176269531 | 308.81280517578125 | The NSP task is closely related to representation-
learning objectives used in Jernite et al. (2017) and
Logeswaran and Lee (2018). However, in prior
work, only sentence embeddings are transferred to
down-stream tasks, where BERT transfers all pa-
rameters to initialize end-task model parameters.
 | 5 | 
72.0 | 320.1191101074219 | 290.2693176269531 | 466.618896484375 | Pre-training data The pre-training procedure
largely follows the existing literature on language
model pre-training. For the pre-training corpus we
use the BooksCorpus (800M words) (Zhu et al.,
2015) and English Wikipedia (2,500M words).
For Wikipedia we extract only the text passages
and ignore lists, tables, and headers. It is criti-
cal to use a document-level corpus rather than a
shufﬂed sentence-level corpus such as the Billion
Word Benchmark (Chelba et al., 2013) in order to
extract long contiguous sequences.
 | 6 | 
72.00000762939453 | 478.9012145996094 | 183.0982666015625 | 489.8103332519531 | 3.2
Fine-tuning BERT
 | 7 | 
72.00000762939453 | 496.996826171875 | 290.2706604003906 | 765.6019287109375 | Fine-tuning is straightforward since the self-
attention mechanism in the Transformer al-
lows BERT to model many downstream tasks—
whether they involve single text or text pairs—by
swapping out the appropriate inputs and outputs.
For applications involving text pairs, a common
pattern is to independently encode text pairs be-
fore applying bidirectional cross attention, such
as Parikh et al. (2016); Seo et al. (2017). BERT
instead uses the self-attention mechanism to unify
these two stages, as encoding a concatenated text
pair with self-attention effectively includes bidi-
rectional cross attention between two sentences.
For each task, we simply plug in the task-
speciﬁc inputs and outputs into BERT and ﬁne-
tune all the parameters end-to-end.
At the in-
put, sentence A and sentence B from pre-training
are analogous to (1) sentence pairs in paraphras-
ing, (2) hypothesis-premise pairs in entailment, (3)
question-passage pairs in question answering, and
 | 8 | 
307.2760009765625 | 227.52210998535156 | 525.545166015625 | 430.75604248046875 | (4) a degenerate text-∅pair in text classiﬁcation
or sequence tagging. At the output, the token rep-
resentations are fed into an output layer for token-
level tasks, such as sequence tagging or question
answering, and the [CLS] representation is fed
into an output layer for classiﬁcation, such as en-
tailment or sentiment analysis.
Compared to pre-training, ﬁne-tuning is rela-
tively inexpensive. All of the results in the pa-
per can be replicated in at most 1 hour on a sin-
gle Cloud TPU, or a few hours on a GPU, starting
from the exact same pre-trained model.7 We de-
scribe the task-speciﬁc details in the correspond-
ing subsections of Section 4. More details can be
found in Appendix A.5.
 | 9 | 
307.2760009765625 | 442.0684509277344 | 390.29290771484375 | 454.0236511230469 | 4
Experiments
 | 10 | 
307.2760009765625 | 463.241943359375 | 525.5452270507812 | 487.7000427246094 | In this section, we present BERT ﬁne-tuning re-
sults on 11 NLP tasks.
 | 11 | 
307.2760009765625 | 498.4353332519531 | 362.73785400390625 | 509.3444519042969 | 4.1
GLUE
 | 12 | 
307.2760009765625 | 516.010986328125 | 525.5490112304688 | 716.6090087890625 | The General Language Understanding Evaluation
(GLUE) benchmark (Wang et al., 2018a) is a col-
lection of diverse natural language understanding
tasks. Detailed descriptions of GLUE datasets are
included in Appendix B.1.
To ﬁne-tune on GLUE, we represent the input
sequence (for single sentence or sentence pairs)
as described in Section 3, and use the ﬁnal hid-
den vector C ∈RH corresponding to the ﬁrst
input token ([CLS]) as the aggregate representa-
tion. The only new parameters introduced during
ﬁne-tuning are classiﬁcation layer weights W ∈
RK×H, where K is the number of labels. We com-
pute a standard classiﬁcation loss with C and W,
i.e., log(softmax(CW T )).
 | 13 | 
307.2760009765625 | 723.8236694335938 | 525.5449829101562 | 765.1323852539062 | 7For example, the BERT SQuAD model can be trained in
around 30 minutes on a single Cloud TPU to achieve a Dev
F1 score of 91.0%.
8See (10) in https://gluebenchmark.com/faq.
 | 14 | 
System
MNLI-(m/mm)
QQP
QNLI
SST-2
CoLA
STS-B
MRPC
RTE
Average
392k
363k
108k
67k
8.5k
5.7k
3.5k
2.5k
-
Pre-OpenAI SOTA
80.6/80.1
66.1
82.3
93.2
35.0
81.0
86.0
61.7
74.0
BiLSTM+ELMo+Attn
76.4/76.1
64.8
79.8
90.4
36.0
73.3
84.9
56.8
71.0
OpenAI GPT
82.1/81.4
70.3
87.4
91.3
45.4
80.0
82.3
56.0
75.1
BERTBASE
84.6/83.4
71.2
90.5
93.5
52.1
85.8
88.9
66.4
79.6
BERTLARGE
86.7/85.9
72.1
92.7
94.9
60.5
86.5
89.3
70.1
82.1
Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).
The number below each task denotes the number of training examples. The “Average” column is slightly different
than the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single-
model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and
accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.
We use a batch size of 32 and ﬁne-tune for 3
epochs over the data for all GLUE tasks. For each
task, we selected the best ﬁne-tuning learning rate
(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.
Additionally, for BERTLARGE we found that ﬁne-
tuning was sometimes unstable on small datasets,
so we ran several random restarts and selected the
best model on the Dev set. With random restarts,
we use the same pre-trained checkpoint but per-
form different ﬁne-tuning data shufﬂing and clas-
siﬁer layer initialization.9
Results are presented in Table 1.
Both
BERTBASE and BERTLARGE outperform all sys-
tems on all tasks by a substantial margin, obtaining
4.5% and 7.0% respective average accuracy im-
provement over the prior state of the art. Note that
BERTBASE and OpenAI GPT are nearly identical
in terms of model architecture apart from the at-
tention masking. For the largest and most widely
reported GLUE task, MNLI, BERT obtains a 4.6%
absolute accuracy improvement. On the ofﬁcial
GLUE leaderboard10, BERTLARGE obtains a score
of 80.5, compared to OpenAI GPT, which obtains
72.8 as of the date of writing.
We ﬁnd that BERTLARGE signiﬁcantly outper-
forms BERTBASE across all tasks, especially those
with very little training data. The effect of model
size is explored more thoroughly in Section 5.2.
4.2
SQuAD v1.1
The
Stanford
Question
Answering
Dataset
(SQuAD v1.1) is a collection of 100k crowd-
sourced question/answer pairs (Rajpurkar et al.,
2016).
Given a question and a passage from
9The GLUE data set distribution does not include the Test
labels, and we only made a single GLUE evaluation server
submission for each of BERTBASE and BERTLARGE.
10https://gluebenchmark.com/leaderboard
Wikipedia containing the answer, the task is to
predict the answer text span in the passage.
As shown in Figure 1, in the question answer-
ing task, we represent the input question and pas-
sage as a single packed sequence, with the ques-
tion using the A embedding and the passage using
the B embedding. We only introduce a start vec-
tor S ∈RH and an end vector E ∈RH during
ﬁne-tuning. The probability of word i being the
start of the answer span is computed as a dot prod-
uct between Ti and S followed by a softmax over
all of the words in the paragraph: Pi =
eS·Ti
P
j eS·Tj .
The analogous formula is used for the end of the
answer span. The score of a candidate span from
position i to position j is deﬁned as S·Ti + E·Tj,
and the maximum scoring span where j ≥i is
used as a prediction. The training objective is the
sum of the log-likelihoods of the correct start and
end positions. We ﬁne-tune for 3 epochs with a
learning rate of 5e-5 and a batch size of 32.
Table 2 shows top leaderboard entries as well
as results from top published systems (Seo et al.,
2017; Clark and Gardner, 2018; Peters et al.,
2018a; Hu et al., 2018). The top results from the
SQuAD leaderboard do not have up-to-date public
system descriptions available,11 and are allowed to
use any public data when training their systems.
We therefore use modest data augmentation in
our system by ﬁrst ﬁne-tuning on TriviaQA (Joshi
et al., 2017) befor ﬁne-tuning on SQuAD.
Our best performing system outperforms the top
leaderboard system by +1.5 F1 in ensembling and
+1.3 F1 as a single system. In fact, our single
BERT model outperforms the top ensemble sys-
tem in terms of F1 score. Without TriviaQA ﬁne-
11QANet is described in Yu et al. (2018), but the system
has improved substantially after publication.

75.27300262451172 | 68.22113800048828 | 522.2713623046875 | 150.07127380371094 | System
MNLI-(m/mm)
QQP
QNLI
SST-2
CoLA
STS-B
MRPC
RTE
Average
392k
363k
108k
67k
8.5k
5.7k
3.5k
2.5k
-
Pre-OpenAI SOTA
80.6/80.1
66.1
82.3
93.2
35.0
81.0
86.0
61.7
74.0
BiLSTM+ELMo+Attn
76.4/76.1
64.8
79.8
90.4
36.0
73.3
84.9
56.8
71.0
OpenAI GPT
82.1/81.4
70.3
87.4
91.3
45.4
80.0
82.3
56.0
75.1
BERTBASE
84.6/83.4
71.2
90.5
93.5
52.1
85.8
88.9
66.4
79.6
BERTLARGE
86.7/85.9
72.1
92.7
94.9
60.5
86.5
89.3
70.1
82.1
 |  | 
72.0 | 164.7444610595703 | 525.5474243164062 | 222.52713012695312 | Table 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).
The number below each task denotes the number of training examples. The “Average” column is slightly different
than the ofﬁcial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single-
model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and
accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.
 | 1 | 
72.0 | 251.29873657226562 | 290.2719421386719 | 397.69989013671875 | We use a batch size of 32 and ﬁne-tune for 3
epochs over the data for all GLUE tasks. For each
task, we selected the best ﬁne-tuning learning rate
(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.
Additionally, for BERTLARGE we found that ﬁne-
tuning was sometimes unstable on small datasets,
so we ran several random restarts and selected the
best model on the Dev set. With random restarts,
we use the same pre-trained checkpoint but per-
form different ﬁne-tuning data shufﬂing and clas-
siﬁer layer initialization.9
 | 2 | 
71.99993133544922 | 400.9557800292969 | 290.2693176269531 | 629.2669677734375 | Results are presented in Table 1.
Both
BERTBASE and BERTLARGE outperform all sys-
tems on all tasks by a substantial margin, obtaining
4.5% and 7.0% respective average accuracy im-
provement over the prior state of the art. Note that
BERTBASE and OpenAI GPT are nearly identical
in terms of model architecture apart from the at-
tention masking. For the largest and most widely
reported GLUE task, MNLI, BERT obtains a 4.6%
absolute accuracy improvement. On the ofﬁcial
GLUE leaderboard10, BERTLARGE obtains a score
of 80.5, compared to OpenAI GPT, which obtains
72.8 as of the date of writing.
We ﬁnd that BERTLARGE signiﬁcantly outper-
forms BERTBASE across all tasks, especially those
with very little training data. The effect of model
size is explored more thoroughly in Section 5.2.
 | 3 | 
71.99993133544922 | 642.7742919921875 | 154.7345428466797 | 653.683349609375 | 4.2
SQuAD v1.1
 | 4 | 
71.99993133544922 | 661.5709228515625 | 290.2691650390625 | 713.1279296875 | The
Stanford
Question
Answering
Dataset
(SQuAD v1.1) is a collection of 100k crowd-
sourced question/answer pairs (Rajpurkar et al.,
2016).
Given a question and a passage from
 | 5 | 
72.0 | 723.8236694335938 | 290.26898193359375 | 765.1323852539062 | 9The GLUE data set distribution does not include the Test
labels, and we only made a single GLUE evaluation server
submission for each of BERTBASE and BERTLARGE.
10https://gluebenchmark.com/leaderboard
 | 6 | 
307.27593994140625 | 251.29867553710938 | 525.548828125 | 420.99420166015625 | Wikipedia containing the answer, the task is to
predict the answer text span in the passage.
As shown in Figure 1, in the question answer-
ing task, we represent the input question and pas-
sage as a single packed sequence, with the ques-
tion using the A embedding and the passage using
the B embedding. We only introduce a start vec-
tor S ∈RH and an end vector E ∈RH during
ﬁne-tuning. The probability of word i being the
start of the answer span is computed as a dot prod-
uct between Ti and S followed by a softmax over
all of the words in the paragraph: Pi =
eS·Ti
P
j eS·Tj .
 | 7 | 
307.2760009765625 | 420.5776672363281 | 525.5453491210938 | 731.996826171875 | The analogous formula is used for the end of the
answer span. The score of a candidate span from
position i to position j is deﬁned as S·Ti + E·Tj,
and the maximum scoring span where j ≥i is
used as a prediction. The training objective is the
sum of the log-likelihoods of the correct start and
end positions. We ﬁne-tune for 3 epochs with a
learning rate of 5e-5 and a batch size of 32.
Table 2 shows top leaderboard entries as well
as results from top published systems (Seo et al.,
2017; Clark and Gardner, 2018; Peters et al.,
2018a; Hu et al., 2018). The top results from the
SQuAD leaderboard do not have up-to-date public
system descriptions available,11 and are allowed to
use any public data when training their systems.
We therefore use modest data augmentation in
our system by ﬁrst ﬁne-tuning on TriviaQA (Joshi
et al., 2017) befor ﬁne-tuning on SQuAD.
Our best performing system outperforms the top
leaderboard system by +1.5 F1 in ensembling and
+1.3 F1 as a single system. In fact, our single
BERT model outperforms the top ensemble sys-
tem in terms of F1 score. Without TriviaQA ﬁne-
 | 8 | 
307.2760009765625 | 744.6607055664062 | 525.5444946289062 | 765.1323852539062 | 11QANet is described in Yu et al. (2018), but the system
has improved substantially after publication.
 | 9 | 
System
Dev
Test
EM
F1
EM
F1
Top Leaderboard Systems (Dec 10th, 2018)
Human
-
-
82.3 91.2
#1 Ensemble - nlnet
-
-
86.0 91.7
#2 Ensemble - QANet
-
-
84.5 90.5
Published
BiDAF+ELMo (Single)
-
85.6
-
85.8
R.M. Reader (Ensemble)
81.2 87.9 82.3 88.5
Ours
BERTBASE (Single)
80.8 88.5
-
-
BERTLARGE (Single)
84.1 90.9
-
-
BERTLARGE (Ensemble)
85.8 91.8
-
-
BERTLARGE (Sgl.+TriviaQA)
84.2 91.1 85.1 91.8
BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2
Table 2:
SQuAD 1.1 results. The BERT ensemble
is 7x systems which use different pre-training check-
points and ﬁne-tuning seeds.
System
Dev
Test
EM
F1
EM
F1
Top Leaderboard Systems (Dec 10th, 2018)
Human
86.3 89.0 86.9 89.5
#1 Single - MIR-MRC (F-Net)
-
-
74.8 78.0
#2 Single - nlnet
-
-
74.2 77.1
Published
unet (Ensemble)
-
-
71.4 74.9
SLQA+ (Single)
-
71.4 74.4
Ours
BERTLARGE (Single)
78.7 81.9 80.0 83.1
Table 3: SQuAD 2.0 results. We exclude entries that
use BERT as one of their components.
tuning data, we only lose 0.1-0.4 F1, still outper-
forming all existing systems by a wide margin.12
4.3
SQuAD v2.0
The SQuAD 2.0 task extends the SQuAD 1.1
problem deﬁnition by allowing for the possibility
that no short answer exists in the provided para-
graph, making the problem more realistic.
We use a simple approach to extend the SQuAD
v1.1 BERT model for this task. We treat ques-
tions that do not have an answer as having an an-
swer span with start and end at the [CLS] to-
ken. The probability space for the start and end
answer span positions is extended to include the
position of the [CLS] token. For prediction, we
compare the score of the no-answer span: snull =
S·C + E·C to the score of the best non-null span
12The TriviaQA data we used consists of paragraphs from
TriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,
that contain at least one of the provided possible answers.
System
Dev
Test
ESIM+GloVe
51.9 52.7
ESIM+ELMo
59.1 59.2
OpenAI GPT
-
78.0
BERTBASE
81.6
-
BERTLARGE
86.6 86.3
Human (expert)†
-
85.0
Human (5 annotations)†
-
88.0
Table 4: SWAG Dev and Test accuracies. †Human per-
formance is measured with 100 samples, as reported in
the SWAG paper.
ˆ
si,j = maxj≥iS·Ti + E·Tj. We predict a non-null
answer when ˆ
si,j > snull + τ, where the thresh-
old τ is selected on the dev set to maximize F1.
We did not use TriviaQA data for this model. We
ﬁne-tuned for 2 epochs with a learning rate of 5e-5
and a batch size of 48.
The results compared to prior leaderboard en-
tries and top published work (Sun et al., 2018;
Wang et al., 2018b) are shown in Table 3, exclud-
ing systems that use BERT as one of their com-
ponents. We observe a +5.1 F1 improvement over
the previous best system.
4.4
SWAG
The Situations With Adversarial Generations
(SWAG) dataset contains 113k sentence-pair com-
pletion examples that evaluate grounded common-
sense inference (Zellers et al., 2018). Given a sen-
tence, the task is to choose the most plausible con-
tinuation among four choices.
When ﬁne-tuning on the SWAG dataset, we
construct four input sequences, each containing
the concatenation of the given sentence (sentence
A) and a possible continuation (sentence B). The
only task-speciﬁc parameters introduced is a vec-
tor whose dot product with the [CLS] token rep-
resentation C denotes a score for each choice
which is normalized with a softmax layer.
We ﬁne-tune the model for 3 epochs with a
learning rate of 2e-5 and a batch size of 16. Re-
sults are presented in Table 4. BERTLARGE out-
performs the authors’ baseline ESIM+ELMo sys-
tem by +27.1% and OpenAI GPT by 8.3%.
5
Ablation Studies
In this section, we perform ablation experiments
over a number of facets of BERT in order to better
understand their relative importance. Additional

125.09400177001953 | 66.90798950195312 | 276.2861328125 | 85.83740234375 | System
Dev
Test
EM
F1
EM
F1
 |  | 
82.87100219726562 | 92.30996704101562 | 279.3966064453125 | 131.16436767578125 | Top Leaderboard Systems (Dec 10th, 2018)
Human
-
-
82.3 91.2
#1 Ensemble - nlnet
-
-
86.0 91.7
#2 Ensemble - QANet
-
-
84.5 90.5
 | 1 | 
82.8709945678711 | 137.63796997070312 | 279.39654541015625 | 166.5303955078125 | Published
BiDAF+ELMo (Single)
-
85.6
-
85.8
R.M. Reader (Ensemble)
81.2 87.9 82.3 88.5
 | 2 | 
82.87100219726562 | 173.00302124023438 | 279.397216796875 | 232.7794189453125 | Ours
BERTBASE (Single)
80.8 88.5
-
-
BERTLARGE (Single)
84.1 90.9
-
-
BERTLARGE (Ensemble)
85.8 91.8
-
-
BERTLARGE (Sgl.+TriviaQA)
84.2 91.1 85.1 91.8
BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2
 | 3 | 
72.0 | 251.1134796142578 | 290.2706298828125 | 284.9870910644531 | Table 2:
SQuAD 1.1 results. The BERT ensemble
is 7x systems which use different pre-training check-
points and ﬁne-tuning seeds.
 | 4 | 
125.09400177001953 | 303.2270202636719 | 277.7861328125 | 322.15643310546875 | System
Dev
Test
EM
F1
EM
F1
 | 5 | 
81.37000274658203 | 328.6289978027344 | 280.89935302734375 | 367.4834289550781 | Top Leaderboard Systems (Dec 10th, 2018)
Human
86.3 89.0 86.9 89.5
#1 Single - MIR-MRC (F-Net)
-
-
74.8 78.0
#2 Single - nlnet
-
-
74.2 77.1
 | 6 | 
81.3699951171875 | 373.9570007324219 | 280.8992919921875 | 402.84942626953125 | Published
unet (Ensemble)
-
-
71.4 74.9
SLQA+ (Single)
-
71.4 74.4
 | 7 | 
81.37000274658203 | 409.3219909667969 | 280.90069580078125 | 429.2474060058594 | Ours
BERTLARGE (Single)
78.7 81.9 80.0 83.1
 | 8 | 
72.0 | 447.58245849609375 | 290.27056884765625 | 469.50006103515625 | Table 3: SQuAD 2.0 results. We exclude entries that
use BERT as one of their components.
 | 9 | 
72.0 | 494.296630859375 | 290.26922607421875 | 518.7547607421875 | tuning data, we only lose 0.1-0.4 F1, still outper-
forming all existing systems by a wide margin.12
 | 10 | 
72.0 | 531.8740234375 | 154.734619140625 | 542.7830810546875 | 4.3
SQuAD v2.0
 | 11 | 
72.0 | 550.4486694335938 | 290.2702941894531 | 724.4507446289062 | The SQuAD 2.0 task extends the SQuAD 1.1
problem deﬁnition by allowing for the possibility
that no short answer exists in the provided para-
graph, making the problem more realistic.
We use a simple approach to extend the SQuAD
v1.1 BERT model for this task. We treat ques-
tions that do not have an answer as having an an-
swer span with start and end at the [CLS] to-
ken. The probability space for the start and end
answer span positions is extended to include the
position of the [CLS] token. For prediction, we
compare the score of the no-answer span: snull =
S·C + E·C to the score of the best non-null span
 | 12 | 
72.0 | 734.6986694335938 | 290.2695617675781 | 765.1323852539062 | 12The TriviaQA data we used consists of paragraphs from
TriviaQA-Wiki formed of the ﬁrst 400 tokens in documents,
that contain at least one of the provided possible answers.
 | 13 | 
349.74798583984375 | 66.90798950195312 | 480.388427734375 | 75.8743896484375 | System
Dev
Test
 | 14 | 
349.74798583984375 | 82.34799194335938 | 480.827880859375 | 111.2393798828125 | ESIM+GloVe
51.9 52.7
ESIM+ELMo
59.1 59.2
OpenAI GPT
-
78.0
 | 15 | 
349.74798583984375 | 117.71298217773438 | 480.828857421875 | 136.91624450683594 | BERTBASE
81.6
-
BERTLARGE
86.6 86.3
 | 16 | 
349.74798583984375 | 142.3438720703125 | 480.83367919921875 | 164.01739501953125 | Human (expert)†
-
85.0
Human (5 annotations)†
-
88.0
 | 17 | 
307.2760009765625 | 176.85641479492188 | 525.5465698242188 | 212.32913208007812 | Table 4: SWAG Dev and Test accuracies. †Human per-
formance is measured with 100 samples, as reported in
the SWAG paper.
 | 18 | 
307.2760009765625 | 241.8088836669922 | 525.5482177734375 | 402.702880859375 | ˆ
si,j = maxj≥iS·Ti + E·Tj. We predict a non-null
answer when ˆ
si,j > snull + τ, where the thresh-
old τ is selected on the dev set to maximize F1.
We did not use TriviaQA data for this model. We
ﬁne-tuned for 2 epochs with a learning rate of 5e-5
and a batch size of 48.
The results compared to prior leaderboard en-
tries and top published work (Sun et al., 2018;
Wang et al., 2018b) are shown in Table 3, exclud-
ing systems that use BERT as one of their com-
ponents. We observe a +5.1 F1 improvement over
the previous best system.
 | 19 | 
307.27606201171875 | 415.9871826171875 | 363.2506408691406 | 426.89630126953125 | 4.4
SWAG
 | 20 | 
307.27606201171875 | 434.65679931640625 | 525.5452880859375 | 690.5529174804688 | The Situations With Adversarial Generations
(SWAG) dataset contains 113k sentence-pair com-
pletion examples that evaluate grounded common-
sense inference (Zellers et al., 2018). Given a sen-
tence, the task is to choose the most plausible con-
tinuation among four choices.
When ﬁne-tuning on the SWAG dataset, we
construct four input sequences, each containing
the concatenation of the given sentence (sentence
A) and a possible continuation (sentence B). The
only task-speciﬁc parameters introduced is a vec-
tor whose dot product with the [CLS] token rep-
resentation C denotes a score for each choice
which is normalized with a softmax layer.
We ﬁne-tune the model for 3 epochs with a
learning rate of 2e-5 and a batch size of 16. Re-
sults are presented in Table 4. BERTLARGE out-
performs the authors’ baseline ESIM+ELMo sys-
tem by +27.1% and OpenAI GPT by 8.3%.
 | 21 | 
307.27606201171875 | 704.4143676757812 | 409.9114685058594 | 716.3695678710938 | 5
Ablation Studies
 | 22 | 
307.27606201171875 | 727.5938720703125 | 525.5452270507812 | 765.6019287109375 | In this section, we perform ablation experiments
over a number of facets of BERT in order to better
understand their relative importance. Additional
 | 23 | 
Dev Set
Tasks
MNLI-m QNLI MRPC SST-2 SQuAD
(Acc)
(Acc)
(Acc)
(Acc)
(F1)
BERTBASE
84.4
88.4
86.7
92.7
88.5
No NSP
83.9
84.9
86.5
92.6
87.9
LTR & No NSP
82.1
84.3
77.5
92.1
77.8
+ BiLSTM
82.1
84.1
75.7
91.6
84.9
Table 5: Ablation over the pre-training tasks using the
BERTBASE architecture. “No NSP” is trained without
the next sentence prediction task. “LTR & No NSP” is
trained as a left-to-right LM without the next sentence
prediction, like OpenAI GPT. “+ BiLSTM” adds a ran-
domly initialized BiLSTM on top of the “LTR + No
NSP” model during ﬁne-tuning.
ablation studies can be found in Appendix C.
5.1
Effect of Pre-training Tasks
We demonstrate the importance of the deep bidi-
rectionality of BERT by evaluating two pre-
training objectives using exactly the same pre-
training data, ﬁne-tuning scheme, and hyperpa-
rameters as BERTBASE:
No NSP: A bidirectional model which is trained
using the “masked LM” (MLM) but without the
“next sentence prediction” (NSP) task.
LTR & No NSP: A left-context-only model which
is trained using a standard Left-to-Right (LTR)
LM, rather than an MLM. The left-only constraint
was also applied at ﬁne-tuning, because removing
it introduced a pre-train/ﬁne-tune mismatch that
degraded downstream performance. Additionally,
this model was pre-trained without the NSP task.
This is directly comparable to OpenAI GPT, but
using our larger training dataset, our input repre-
sentation, and our ﬁne-tuning scheme.
We ﬁrst examine the impact brought by the NSP
task.
In Table 5, we show that removing NSP
hurts performance signiﬁcantly on QNLI, MNLI,
and SQuAD 1.1. Next, we evaluate the impact
of training bidirectional representations by com-
paring “No NSP” to “LTR & No NSP”. The LTR
model performs worse than the MLM model on all
tasks, with large drops on MRPC and SQuAD.
For SQuAD it is intuitively clear that a LTR
model will perform poorly at token predictions,
since the token-level hidden states have no right-
side context. In order to make a good faith at-
tempt at strengthening the LTR system, we added
a randomly initialized BiLSTM on top. This does
signiﬁcantly improve results on SQuAD, but the
results are still far worse than those of the pre-
trained bidirectional models. The BiLSTM hurts
performance on the GLUE tasks.
We recognize that it would also be possible to
train separate LTR and RTL models and represent
each token as the concatenation of the two mod-
els, as ELMo does. However: (a) this is twice as
expensive as a single bidirectional model; (b) this
is non-intuitive for tasks like QA, since the RTL
model would not be able to condition the answer
on the question; (c) this it is strictly less powerful
than a deep bidirectional model, since it can use
both left and right context at every layer.
5.2
Effect of Model Size
In this section, we explore the effect of model size
on ﬁne-tuning task accuracy. We trained a number
of BERT models with a differing number of layers,
hidden units, and attention heads, while otherwise
using the same hyperparameters and training pro-
cedure as described previously.
Results on selected GLUE tasks are shown in
Table 6. In this table, we report the average Dev
Set accuracy from 5 random restarts of ﬁne-tuning.
We can see that larger models lead to a strict ac-
curacy improvement across all four datasets, even
for MRPC which only has 3,600 labeled train-
ing examples, and is substantially different from
the pre-training tasks. It is also perhaps surpris-
ing that we are able to achieve such signiﬁcant
improvements on top of models which are al-
ready quite large relative to the existing literature.
For example, the largest Transformer explored in
Vaswani et al. (2017) is (L=6, H=1024, A=16)
with 100M parameters for the encoder, and the
largest Transformer we have found in the literature
is (L=64, H=512, A=2) with 235M parameters
(Al-Rfou et al., 2018). By contrast, BERTBASE
contains 110M parameters and BERTLARGE con-
tains 340M parameters.
It has long been known that increasing the
model size will lead to continual improvements
on large-scale tasks such as machine translation
and language modeling, which is demonstrated
by the LM perplexity of held-out training data
shown in Table 6.
However, we believe that
this is the ﬁrst work to demonstrate convinc-
ingly that scaling to extreme model sizes also
leads to large improvements on very small scale
tasks, provided that the model has been sufﬁ-
ciently pre-trained. Peters et al. (2018b) presented

72.00000762939453 | 66.90798950195312 | 292.12518310546875 | 95.79937744140625 | Dev Set
Tasks
MNLI-m QNLI MRPC SST-2 SQuAD
(Acc)
(Acc)
(Acc)
(Acc)
(F1)
 |  | 
72.0 | 102.27297973632812 | 285.52587890625 | 141.12738037109375 | BERTBASE
84.4
88.4
86.7
92.7
88.5
No NSP
83.9
84.9
86.5
92.6
87.9
LTR & No NSP
82.1
84.3
77.5
92.1
77.8
+ BiLSTM
82.1
84.1
75.7
91.6
84.9
 | 1 | 
72.0 | 155.4774932861328 | 290.27264404296875 | 237.17117309570312 | Table 5: Ablation over the pre-training tasks using the
BERTBASE architecture. “No NSP” is trained without
the next sentence prediction task. “LTR & No NSP” is
trained as a left-to-right LM without the next sentence
prediction, like OpenAI GPT. “+ BiLSTM” adds a ran-
domly initialized BiLSTM on top of the “LTR + No
NSP” model during ﬁne-tuning.
 | 2 | 
72.0 | 263.4567565917969 | 268.9528503417969 | 274.3658447265625 | ablation studies can be found in Appendix C.
 | 3 | 
72.0 | 290.0651550292969 | 225.32742309570312 | 300.9742736816406 | 5.1
Effect of Pre-training Tasks
 | 4 | 
72.0 | 310.1187438964844 | 290.2692565917969 | 376.16876220703125 | We demonstrate the importance of the deep bidi-
rectionality of BERT by evaluating two pre-
training objectives using exactly the same pre-
training data, ﬁne-tuning scheme, and hyperpa-
rameters as BERTBASE:
 | 5 | 
71.99999237060547 | 386.2691650390625 | 290.2693176269531 | 765.6019287109375 | No NSP: A bidirectional model which is trained
using the “masked LM” (MLM) but without the
“next sentence prediction” (NSP) task.
LTR & No NSP: A left-context-only model which
is trained using a standard Left-to-Right (LTR)
LM, rather than an MLM. The left-only constraint
was also applied at ﬁne-tuning, because removing
it introduced a pre-train/ﬁne-tune mismatch that
degraded downstream performance. Additionally,
this model was pre-trained without the NSP task.
This is directly comparable to OpenAI GPT, but
using our larger training dataset, our input repre-
sentation, and our ﬁne-tuning scheme.
We ﬁrst examine the impact brought by the NSP
task.
In Table 5, we show that removing NSP
hurts performance signiﬁcantly on QNLI, MNLI,
and SQuAD 1.1. Next, we evaluate the impact
of training bidirectional representations by com-
paring “No NSP” to “LTR & No NSP”. The LTR
model performs worse than the MLM model on all
tasks, with large drops on MRPC and SQuAD.
For SQuAD it is intuitively clear that a LTR
model will perform poorly at token predictions,
since the token-level hidden states have no right-
side context. In order to make a good faith at-
tempt at strengthening the LTR system, we added
a randomly initialized BiLSTM on top. This does
signiﬁcantly improve results on SQuAD, but the
 | 6 | 
307.2760009765625 | 65.49386596679688 | 525.5452270507812 | 240.4760284423828 | results are still far worse than those of the pre-
trained bidirectional models. The BiLSTM hurts
performance on the GLUE tasks.
We recognize that it would also be possible to
train separate LTR and RTL models and represent
each token as the concatenation of the two mod-
els, as ELMo does. However: (a) this is twice as
expensive as a single bidirectional model; (b) this
is non-intuitive for tasks like QA, since the RTL
model would not be able to condition the answer
on the question; (c) this it is strictly less powerful
than a deep bidirectional model, since it can use
both left and right context at every layer.
 | 7 | 
307.2760009765625 | 256.9873352050781 | 425.43231201171875 | 267.8964538574219 | 5.2
Effect of Model Size
 | 8 | 
307.2760009765625 | 277.5059509277344 | 525.5452880859375 | 765.6021728515625 | In this section, we explore the effect of model size
on ﬁne-tuning task accuracy. We trained a number
of BERT models with a differing number of layers,
hidden units, and attention heads, while otherwise
using the same hyperparameters and training pro-
cedure as described previously.
Results on selected GLUE tasks are shown in
Table 6. In this table, we report the average Dev
Set accuracy from 5 random restarts of ﬁne-tuning.
We can see that larger models lead to a strict ac-
curacy improvement across all four datasets, even
for MRPC which only has 3,600 labeled train-
ing examples, and is substantially different from
the pre-training tasks. It is also perhaps surpris-
ing that we are able to achieve such signiﬁcant
improvements on top of models which are al-
ready quite large relative to the existing literature.
For example, the largest Transformer explored in
Vaswani et al. (2017) is (L=6, H=1024, A=16)
with 100M parameters for the encoder, and the
largest Transformer we have found in the literature
is (L=64, H=512, A=2) with 235M parameters
(Al-Rfou et al., 2018). By contrast, BERTBASE
contains 110M parameters and BERTLARGE con-
tains 340M parameters.
It has long been known that increasing the
model size will lead to continual improvements
on large-scale tasks such as machine translation
and language modeling, which is demonstrated
by the LM perplexity of held-out training data
shown in Table 6.
However, we believe that
this is the ﬁrst work to demonstrate convinc-
ingly that scaling to extreme model sizes also
leads to large improvements on very small scale
tasks, provided that the model has been sufﬁ-
ciently pre-trained. Peters et al. (2018b) presented
 | 9 | 
mixed results on the downstream task impact of
increasing the pre-trained bi-LM size from two
to four layers and Melamud et al. (2016) men-
tioned in passing that increasing hidden dimen-
sion size from 200 to 600 helped, but increasing
further to 1,000 did not bring further improve-
ments. Both of these prior works used a feature-
based approach — we hypothesize that when the
model is ﬁne-tuned directly on the downstream
tasks and uses only a very small number of ran-
domly initialized additional parameters, the task-
speciﬁc models can beneﬁt from the larger, more
expressive pre-trained representations even when
downstream task data is very small.
5.3
Feature-based Approach with BERT
All of the BERT results presented so far have used
the ﬁne-tuning approach, where a simple classiﬁ-
cation layer is added to the pre-trained model, and
all parameters are jointly ﬁne-tuned on a down-
stream task. However, the feature-based approach,
where ﬁxed features are extracted from the pre-
trained model, has certain advantages. First, not
all tasks can be easily represented by a Trans-
former encoder architecture, and therefore require
a task-speciﬁc model architecture to be added.
Second, there are major computational beneﬁts
to pre-compute an expensive representation of the
training data once and then run many experiments
with cheaper models on top of this representation.
In this section, we compare the two approaches
by applying BERT to the CoNLL-2003 Named
Entity Recognition (NER) task (Tjong Kim Sang
and De Meulder, 2003). In the input to BERT, we
use a case-preserving WordPiece model, and we
include the maximal document context provided
by the data. Following standard practice, we for-
mulate this as a tagging task but do not use a CRF
Hyperparams
Dev Set Accuracy
#L
#H #A LM (ppl) MNLI-m MRPC SST-2
3
768
12
5.84
77.9
79.8
88.4
6
768
3
5.24
80.6
82.2
90.7
6
768
12
4.68
81.9
84.8
91.3
12
768
12
3.99
84.4
86.7
92.9
12 1024
16
3.54
85.7
86.9
93.3
24 1024
16
3.23
86.6
87.8
93.7
Table 6:
Ablation over BERT model size. #L = the
number of layers; #H = hidden size; #A = number of at-
tention heads. “LM (ppl)” is the masked LM perplexity
of held-out training data.
System
Dev F1 Test F1
ELMo (Peters et al., 2018a)
95.7
92.2
CVT (Clark et al., 2018)
-
92.6
CSE (Akbik et al., 2018)
-
93.1
Fine-tuning approach
BERTLARGE
96.6
92.8
BERTBASE
96.4
92.4
Feature-based approach (BERTBASE)
Embeddings
91.0
-
Second-to-Last Hidden
95.6
-
Last Hidden
94.9
-
Weighted Sum Last Four Hidden
95.9
-
Concat Last Four Hidden
96.1
-
Weighted Sum All 12 Layers
95.5
-
Table 7: CoNLL-2003 Named Entity Recognition re-
sults. Hyperparameters were selected using the Dev
set. The reported Dev and Test scores are averaged over
5 random restarts using those hyperparameters.
layer in the output. We use the representation of
the ﬁrst sub-token as the input to the token-level
classiﬁer over the NER label set.
To ablate the ﬁne-tuning approach, we apply the
feature-based approach by extracting the activa-
tions from one or more layers without ﬁne-tuning
any parameters of BERT. These contextual em-
beddings are used as input to a randomly initial-
ized two-layer 768-dimensional BiLSTM before
the classiﬁcation layer.
Results are presented in Table 7. BERTLARGE
performs competitively with state-of-the-art meth-
ods. The best performing method concatenates the
token representations from the top four hidden lay-
ers of the pre-trained Transformer, which is only
0.3 F1 behind ﬁne-tuning the entire model. This
demonstrates that BERT is effective for both ﬁne-
tuning and feature-based approaches.
6
Conclusion
Recent empirical improvements due to transfer
learning with language models have demonstrated
that rich, unsupervised pre-training is an integral
part of many language understanding systems. In
particular, these results enable even low-resource
tasks to beneﬁt from deep unidirectional architec-
tures. Our major contribution is further general-
izing these ﬁndings to deep bidirectional architec-
tures, allowing the same pre-trained model to suc-
cessfully tackle a broad set of NLP tasks.

72.0 | 65.49368286132812 | 290.2693176269531 | 252.54286193847656 | mixed results on the downstream task impact of
increasing the pre-trained bi-LM size from two
to four layers and Melamud et al. (2016) men-
tioned in passing that increasing hidden dimen-
sion size from 200 to 600 helped, but increasing
further to 1,000 did not bring further improve-
ments. Both of these prior works used a feature-
based approach — we hypothesize that when the
model is ﬁne-tuned directly on the downstream
tasks and uses only a very small number of ran-
domly initialized additional parameters, the task-
speciﬁc models can beneﬁt from the larger, more
expressive pre-trained representations even when
downstream task data is very small.
 |  | 
72.0 | 266.4941711425781 | 266.9892578125 | 277.4032897949219 | 5.3
Feature-based Approach with BERT
 | 1 | 
72.0 | 285.5458068847656 | 290.2693176269531 | 581.7310791015625 | All of the BERT results presented so far have used
the ﬁne-tuning approach, where a simple classiﬁ-
cation layer is added to the pre-trained model, and
all parameters are jointly ﬁne-tuned on a down-
stream task. However, the feature-based approach,
where ﬁxed features are extracted from the pre-
trained model, has certain advantages. First, not
all tasks can be easily represented by a Trans-
former encoder architecture, and therefore require
a task-speciﬁc model architecture to be added.
Second, there are major computational beneﬁts
to pre-compute an expensive representation of the
training data once and then run many experiments
with cheaper models on top of this representation.
In this section, we compare the two approaches
by applying BERT to the CoNLL-2003 Named
Entity Recognition (NER) task (Tjong Kim Sang
and De Meulder, 2003). In the input to BERT, we
use a case-preserving WordPiece model, and we
include the maximal document context provided
by the data. Following standard practice, we for-
mulate this as a tagging task but do not use a CRF
 | 2 | 
89.0270004272461 | 606.9679565429688 | 261.01153564453125 | 615.9343872070312 | Hyperparams
Dev Set Accuracy
 | 3 | 
85.56999969482422 | 622.407958984375 | 276.69775390625 | 631.3743896484375 | #L
#H #A LM (ppl) MNLI-m MRPC SST-2
 | 4 | 
86.56600189208984 | 637.8479614257812 | 273.49755859375 | 696.6273803710938 | 3
768
12
5.84
77.9
79.8
88.4
6
768
3
5.24
80.6
82.2
90.7
6
768
12
4.68
81.9
84.8
91.3
12
768
12
3.99
84.4
86.7
92.9
12 1024
16
3.54
85.7
86.9
93.3
24 1024
16
3.23
86.6
87.8
93.7
 | 5 | 
72.0 | 715.95849609375 | 290.2705993652344 | 761.7860717773438 | Table 6:
Ablation over BERT model size. #L = the
number of layers; #H = hidden size; #A = number of at-
tention heads. “LM (ppl)” is the masked LM perplexity
of held-out training data.
 | 6 | 
317.0150146484375 | 66.90798950195312 | 515.800048828125 | 75.8743896484375 | System
Dev F1 Test F1
 | 7 | 
317.0150146484375 | 82.34799194335938 | 510.38824462890625 | 111.2393798828125 | ELMo (Peters et al., 2018a)
95.7
92.2
CVT (Clark et al., 2018)
-
92.6
CSE (Akbik et al., 2018)
-
93.1
 | 8 | 
317.0150146484375 | 117.71298217773438 | 510.3924560546875 | 146.87925720214844 | Fine-tuning approach
BERTLARGE
96.6
92.8
BERTBASE
96.4
92.4
 | 9 | 
317.0150146484375 | 153.07797241210938 | 504.03729248046875 | 221.82037353515625 | Feature-based approach (BERTBASE)
Embeddings
91.0
-
Second-to-Last Hidden
95.6
-
Last Hidden
94.9
-
Weighted Sum Last Four Hidden
95.9
-
Concat Last Four Hidden
96.1
-
Weighted Sum All 12 Layers
95.5
-
 | 10 | 
307.2760009765625 | 236.1695098876953 | 525.546630859375 | 281.9981384277344 | Table 7: CoNLL-2003 Named Entity Recognition re-
sults. Hyperparameters were selected using the Dev
set. The reported Dev and Test scores are averaged over
5 random restarts using those hyperparameters.
 | 11 | 
307.2760009765625 | 313.5127258300781 | 525.5452880859375 | 351.51983642578125 | layer in the output. We use the representation of
the ﬁrst sub-token as the input to the token-level
classiﬁer over the NER label set.
 | 12 | 
307.2759704589844 | 358.0227355957031 | 525.5452270507812 | 450.22686767578125 | To ablate the ﬁne-tuning approach, we apply the
feature-based approach by extracting the activa-
tions from one or more layers without ﬁne-tuning
any parameters of BERT. These contextual em-
beddings are used as input to a randomly initial-
ized two-layer 768-dimensional BiLSTM before
the classiﬁcation layer.
 | 13 | 
307.2760009765625 | 456.7287902832031 | 525.5452270507812 | 562.48193359375 | Results are presented in Table 7. BERTLARGE
performs competitively with state-of-the-art meth-
ods. The best performing method concatenates the
token representations from the top four hidden lay-
ers of the pre-trained Transformer, which is only
0.3 F1 behind ﬁne-tuning the entire model. This
demonstrates that BERT is effective for both ﬁne-
tuning and feature-based approaches.
 | 14 | 
307.2760009765625 | 587.8143310546875 | 382.34271240234375 | 599.76953125 | 6
Conclusion
 | 15 | 
307.2760009765625 | 619.2008056640625 | 525.5452880859375 | 752.0518798828125 | Recent empirical improvements due to transfer
learning with language models have demonstrated
that rich, unsupervised pre-training is an integral
part of many language understanding systems. In
particular, these results enable even low-resource
tasks to beneﬁt from deep unidirectional architec-
tures. Our major contribution is further general-
izing these ﬁndings to deep bidirectional architec-
tures, allowing the same pre-trained model to suc-
cessfully tackle a broad set of NLP tasks.
 | 16 | 
References
Alan Akbik, Duncan Blythe, and Roland Vollgraf.
2018. Contextual string embeddings for sequence
labeling. In Proceedings of the 27th International
Conference on Computational Linguistics, pages
1638–1649.
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy
Guo, and Llion Jones. 2018.
Character-level lan-
guage modeling with deeper self-attention.
arXiv
preprint arXiv:1808.04444.
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research, 6(Nov):1817–1853.
Luisa Bentivogli,
Bernardo Magnini,
Ido Dagan,
Hoa Trang Dang, and Danilo Giampiccolo. 2009.
The ﬁfth PASCAL recognizing textual entailment
challenge. In TAC. NIST.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 confer-
ence on empirical methods in natural language pro-
cessing, pages 120–128. Association for Computa-
tional Linguistics.
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In EMNLP. Association for Computational Linguis-
tics.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017.
Semeval-2017
task 1: Semantic textual similarity multilingual and
crosslingual focused evaluation.
In Proceedings
of the 11th International Workshop on Semantic
Evaluation (SemEval-2017), pages 1–14, Vancou-
ver, Canada. Association for Computational Lin-
guistics.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.
Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.
Quora question pairs.
Christopher Clark and Matt Gardner. 2018.
Simple
and effective multi-paragraph reading comprehen-
sion. In ACL.
Kevin Clark, Minh-Thang Luong, Christopher D Man-
ning, and Quoc Le. 2018.
Semi-supervised se-
quence modeling with cross-view training. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1914–
1925.
Ronan Collobert and Jason Weston. 2008. A uniﬁed
architecture for natural language processing: Deep
neural networks with multitask learning.
In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017.
Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680, Copen-
hagen, Denmark. Association for Computational
Linguistics.
Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In Advances in neural informa-
tion processing systems, pages 3079–3087.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09.
William B Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005).
William Fedus, Ian Goodfellow, and Andrew M Dai.
2018. Maskgan: Better text generation via ﬁlling in
the . arXiv preprint arXiv:1801.07736.
Dan Hendrycks and Kevin Gimpel. 2016.
Bridging
nonlinearities and stochastic regularizers with gaus-
sian error linear units. CoRR, abs/1606.08415.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics.
Jeremy Howard and Sebastian Ruder. 2018. Universal
language model ﬁne-tuning for text classiﬁcation. In
ACL. Association for Computational Linguistics.
Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,
Furu Wei, and Ming Zhou. 2018.
Reinforced
mnemonic reader for machine reading comprehen-
sion. In IJCAI.
Yacine Jernite, Samuel R. Bowman, and David Son-
tag. 2017. Discourse-based objectives for fast un-
supervised sentence representation learning. CoRR,
abs/1705.00557.

72.0 | 64.59117889404297 | 127.54383850097656 | 76.54637908935547 | References
 |  | 
72.0 | 86.38551330566406 | 290.2723083496094 | 140.18307495117188 | Alan Akbik, Duncan Blythe, and Roland Vollgraf.
2018. Contextual string embeddings for sequence
labeling. In Proceedings of the 27th International
Conference on Computational Linguistics, pages
1638–1649.
 | 1 | 
72.00001525878906 | 154.3834991455078 | 290.2705383300781 | 197.22305297851562 | Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy
Guo, and Llion Jones. 2018.
Character-level lan-
guage modeling with deeper self-attention.
arXiv
preprint arXiv:1808.04444.
 | 2 | 
72.00001525878906 | 211.4224395751953 | 290.27056884765625 | 254.26199340820312 | Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research, 6(Nov):1817–1853.
 | 3 | 
72.00001525878906 | 268.46240234375 | 290.2705993652344 | 311.3019714355469 | Luisa Bentivogli,
Bernardo Magnini,
Ido Dagan,
Hoa Trang Dang, and Danilo Giampiccolo. 2009.
The ﬁfth PASCAL recognizing textual entailment
challenge. In TAC. NIST.
 | 4 | 
72.00001525878906 | 325.5013427734375 | 290.2705993652344 | 390.2588806152344 | John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 confer-
ence on empirical methods in natural language pro-
cessing, pages 120–128. Association for Computa-
tional Linguistics.
 | 5 | 
72.00001525878906 | 404.4592590332031 | 290.2724914550781 | 458.2568664550781 | Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In EMNLP. Association for Computational Linguis-
tics.
 | 6 | 
72.00001525878906 | 472.4572448730469 | 290.2706604003906 | 515.2968139648438 | Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
 | 7 | 
72.0 | 529.4971923828125 | 290.2705993652344 | 616.1718139648438 | Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017.
Semeval-2017
task 1: Semantic textual similarity multilingual and
crosslingual focused evaluation.
In Proceedings
of the 11th International Workshop on Semantic
Evaluation (SemEval-2017), pages 1–14, Vancou-
ver, Canada. Association for Computational Lin-
guistics.
 | 8 | 
72.0 | 630.3721923828125 | 290.27056884765625 | 684.1697998046875 | Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005.
 | 9 | 
71.9999771118164 | 698.3702392578125 | 290.2704772949219 | 708.3328247070312 | Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.
 | 10 | 
82.90898132324219 | 709.3292236328125 | 168.40802001953125 | 719.2918090820312 | Quora question pairs.
 | 11 | 
71.99998474121094 | 733.4921875 | 290.27056884765625 | 765.372802734375 | Christopher Clark and Matt Gardner. 2018.
Simple
and effective multi-paragraph reading comprehen-
sion. In ACL.
 | 12 | 
307.2760009765625 | 66.21125793457031 | 525.5465698242188 | 130.96878051757812 | Kevin Clark, Minh-Thang Luong, Christopher D Man-
ning, and Quoc Le. 2018.
Semi-supervised se-
quence modeling with cross-view training. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1914–
1925.
 | 13 | 
307.2760009765625 | 142.8131866455078 | 525.5465087890625 | 196.61172485351562 | Ronan Collobert and Jason Weston. 2008. A uniﬁed
architecture for natural language processing: Deep
neural networks with multitask learning.
In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
 | 14 | 
307.2760009765625 | 208.40711975097656 | 525.5465087890625 | 295.1316223144531 | Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017.
Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680, Copen-
hagen, Denmark. Association for Computational
Linguistics.
 | 15 | 
307.2760009765625 | 306.97601318359375 | 525.546630859375 | 338.8565979003906 | Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In Advances in neural informa-
tion processing systems, pages 3079–3087.
 | 16 | 
307.2760009765625 | 350.7019958496094 | 525.5467529296875 | 382.5826110839844 | J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09.
 | 17 | 
307.2759704589844 | 394.427001953125 | 525.54638671875 | 437.2665710449219 | William B Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop
on Paraphrasing (IWP2005).
 | 18 | 
307.27593994140625 | 449.1109619140625 | 525.5465087890625 | 480.9920959472656 | William Fedus, Ian Goodfellow, and Andrew M Dai.
2018. Maskgan: Better text generation via ﬁlling in
the . arXiv preprint arXiv:1801.07736.
 | 19 | 
307.2760009765625 | 492.8374938964844 | 525.5465698242188 | 524.7180786132812 | Dan Hendrycks and Kevin Gimpel. 2016.
Bridging
nonlinearities and stochastic regularizers with gaus-
sian error linear units. CoRR, abs/1606.08415.
 | 20 | 
307.2760009765625 | 536.5625 | 525.5465087890625 | 612.279052734375 | Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics.
 | 21 | 
307.2760009765625 | 624.1234130859375 | 525.5464477539062 | 656.0040283203125 | Jeremy Howard and Sebastian Ruder. 2018. Universal
language model ﬁne-tuning for text classiﬁcation. In
ACL. Association for Computational Linguistics.
 | 22 | 
307.2759704589844 | 667.8494262695312 | 525.546630859375 | 710.6880493164062 | Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,
Furu Wei, and Ming Zhou. 2018.
Reinforced
mnemonic reader for machine reading comprehen-
sion. In IJCAI.
 | 23 | 
307.27593994140625 | 722.533447265625 | 525.5466918945312 | 765.3729858398438 | Yacine Jernite, Samuel R. Bowman, and David Son-
tag. 2017. Discourse-based objectives for fast un-
supervised sentence representation learning. CoRR,
abs/1705.00557.
 | 24 | 
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In ACL.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems,
pages 3294–3302.
Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Inter-
national Conference on Machine Learning, pages
1188–1196.
Hector J Levesque, Ernest Davis, and Leora Morgen-
stern. 2011. The winograd schema challenge. In
Aaai spring symposium: Logical formalizations of
commonsense reasoning, volume 46, page 47.
Lajanugen Logeswaran and Honglak Lee. 2018. An
efﬁcient framework for learning sentence represen-
tations.
In International Conference on Learning
Representations.
Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In NIPS.
Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016. context2vec: Learning generic context em-
bedding with bidirectional LSTM. In CoNLL.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119. Curran Associates,
Inc.
Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model.
In
D. Koller, D. Schuurmans, Y. Bengio, and L. Bot-
tou, editors, Advances in Neural Information Pro-
cessing Systems 21, pages 1081–1088. Curran As-
sociates, Inc.
Ankur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In EMNLP.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.
Matthew Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In ACL.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018a. Deep contextualized word rep-
resentations. In NAACL.
Matthew Peters, Mark Neumann, Luke Zettlemoyer,
and Wen-tau Yih. 2018b.
Dissecting contextual
word embeddings: Architecture and representation.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1499–1509.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018.
Improving language under-
standing with unsupervised learning. Technical re-
port, OpenAI.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392.
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
ﬂow for machine comprehension. In ICLR.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013.
Recursive deep models
for semantic compositionality over a sentiment tree-
bank.
In Proceedings of the 2013 conference on
empirical methods in natural language processing,
pages 1631–1642.
Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.
2018.
U-net:
Machine reading comprehension
with unanswerable questions.
arXiv preprint
arXiv:1810.06638.
Wilson L Taylor. 1953.
Cloze procedure:
A new
tool for measuring readability. Journalism Bulletin,
30(4):415–433.
Erik F Tjong Kim Sang and Fien De Meulder.
2003.
Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
CoNLL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ’10, pages 384–394.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008.
Extracting and
composing robust features with denoising autoen-
coders.
In Proceedings of the 25th international
conference on Machine learning, pages 1096–1103.
ACM.
Alex Wang, Amanpreet Singh, Julian Michael, Fe-
lix Hill, Omer Levy, and Samuel Bowman. 2018a.
Glue: A multi-task benchmark and analysis platform

72.0 | 66.21150207519531 | 290.27056884765625 | 109.05105590820312 | Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In ACL.
 |  | 
71.99998474121094 | 118.16548156738281 | 290.2705383300781 | 171.96401977539062 | Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems,
pages 3294–3302.
 | 1 | 
71.99998474121094 | 181.0784454345703 | 290.2705078125 | 223.91702270507812 | Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Inter-
national Conference on Machine Learning, pages
1188–1196.
 | 2 | 
71.99996948242188 | 233.0314483642578 | 290.27056884765625 | 275.8710021972656 | Hector J Levesque, Ernest Davis, and Leora Morgen-
stern. 2011. The winograd schema challenge. In
Aaai spring symposium: Logical formalizations of
commonsense reasoning, volume 46, page 47.
 | 3 | 
71.99996948242188 | 284.98541259765625 | 290.2705383300781 | 327.8249816894531 | Lajanugen Logeswaran and Honglak Lee. 2018. An
efﬁcient framework for learning sentence represen-
tations.
In International Conference on Learning
Representations.
 | 4 | 
71.99996948242188 | 336.9393615722656 | 290.27056884765625 | 368.8199768066406 | Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In NIPS.
 | 5 | 
71.99996948242188 | 377.9343566894531 | 290.27056884765625 | 409.8149719238281 | Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016. context2vec: Learning generic context em-
bedding with bidirectional LSTM. In CoNLL.
 | 6 | 
71.99996948242188 | 418.9293518066406 | 290.2705078125 | 483.6859436035156 | Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119. Curran Associates,
Inc.
 | 7 | 
71.99996948242188 | 492.8003234863281 | 290.2705078125 | 557.557861328125 | Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model.
In
D. Koller, D. Schuurmans, Y. Bengio, and L. Bot-
tou, editors, Advances in Neural Information Pro-
cessing Systems 21, pages 1081–1088. Curran As-
sociates, Inc.
 | 8 | 
71.99996948242188 | 566.622314453125 | 290.27044677734375 | 598.5528564453125 | Ankur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In EMNLP.
 | 9 | 
71.99995422363281 | 607.6672973632812 | 290.2707824707031 | 661.4649047851562 | Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.
 | 10 | 
71.99993896484375 | 670.5792846679688 | 290.27056884765625 | 713.4188842773438 | Matthew Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In ACL.
 | 11 | 
71.99993896484375 | 722.5332641601562 | 290.2705383300781 | 765.3728637695312 | Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018a. Deep contextualized word rep-
resentations. In NAACL.
 | 12 | 
307.27593994140625 | 66.21131896972656 | 525.5467529296875 | 130.96884155273438 | Matthew Peters, Mark Neumann, Luke Zettlemoyer,
and Wen-tau Yih. 2018b.
Dissecting contextual
word embeddings: Architecture and representation.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1499–1509.
 | 13 | 
307.27593994140625 | 140.8212432861328 | 525.5465698242188 | 183.66079711914062 | Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018.
Improving language under-
standing with unsupervised learning. Technical re-
port, OpenAI.
 | 14 | 
307.27593994140625 | 193.51319885253906 | 525.5465087890625 | 247.31076049804688 | Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392.
 | 15 | 
307.27593994140625 | 257.16314697265625 | 525.5464477539062 | 289.0437316894531 | Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
ﬂow for machine comprehension. In ICLR.
 | 16 | 
307.27593994140625 | 298.8961181640625 | 525.546630859375 | 374.6126403808594 | Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013.
Recursive deep models
for semantic compositionality over a sentiment tree-
bank.
In Proceedings of the 2013 conference on
empirical methods in natural language processing,
pages 1631–1642.
 | 17 | 
307.27593994140625 | 384.46502685546875 | 525.5465698242188 | 427.3036193847656 | Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.
2018.
U-net:
Machine reading comprehension
with unanswerable questions.
arXiv preprint
arXiv:1810.06638.
 | 18 | 
307.27593994140625 | 437.156005859375 | 525.5464477539062 | 469.0365905761719 | Wilson L Taylor. 1953.
Cloze procedure:
A new
tool for measuring readability. Journalism Bulletin,
30(4):415–433.
 | 19 | 
307.2760009765625 | 478.88897705078125 | 525.546630859375 | 521.728515625 | Erik F Tjong Kim Sang and Fien De Meulder.
2003.
Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
CoNLL.
 | 20 | 
307.2760009765625 | 531.5809326171875 | 525.5465698242188 | 585.3794555664062 | Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, ACL ’10, pages 384–394.
 | 21 | 
307.2760009765625 | 595.2318725585938 | 525.5467529296875 | 649.0294799804688 | Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.
 | 22 | 
307.2760009765625 | 658.8818359375 | 525.5466918945312 | 723.6394653320312 | Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008.
Extracting and
composing robust features with denoising autoen-
coders.
In Proceedings of the 25th international
conference on Machine learning, pages 1096–1103.
ACM.
 | 23 | 
307.2760009765625 | 733.4918823242188 | 525.546630859375 | 765.3724365234375 | Alex Wang, Amanpreet Singh, Julian Michael, Fe-
lix Hill, Omer Levy, and Samuel Bowman. 2018a.
Glue: A multi-task benchmark and analysis platform
 | 24 | 
for natural language understanding. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: An-
alyzing and Interpreting Neural Networks for NLP,
pages 353–355.
Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi-
granularity hierarchical attention fusion networks
for reading comprehension and question answering.
In Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Lin-
guistics.
Alex Warstadt, Amanpreet Singh, and Samuel R Bow-
man. 2018.
Neural network acceptability judg-
ments. arXiv preprint arXiv:1805.12471.
Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2018.
A broad-coverage challenge corpus
for sentence understanding through inference.
In
NAACL.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le,
Mohammad Norouzi,
Wolfgang Macherey,
Maxim Krikun,
Yuan Cao,
Qin Gao,
Klaus
Macherey, et al. 2016.
Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation.
arXiv preprint
arXiv:1609.08144.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod
Lipson. 2014. How transferable are features in deep
neural networks? In Advances in neural information
processing systems, pages 3320–3328.
Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le. 2018.
QANet: Combining local convolution
with global self-attention for reading comprehen-
sion. In ICLR.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin
Choi. 2018. Swag: A large-scale adversarial dataset
for grounded commonsense inference. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books.
In Proceedings of the IEEE
international conference on computer vision, pages
19–27.
Appendix for “BERT: Pre-training of
Deep Bidirectional Transformers for
Language Understanding”
We organize the appendix into three sections:
• Additional implementation details for BERT
are presented in Appendix A;
• Additional details for our experiments are
presented in Appendix B; and
• Additional ablation studies are presented in
Appendix C.
We present additional ablation studies for
BERT including:
– Effect of Number of Training Steps; and
– Ablation for Different Masking Proce-
dures.
A
Additional Details for BERT
A.1
Illustration of the Pre-training Tasks
We provide examples of the pre-training tasks in
the following.
Masked LM and the Masking Procedure
As-
suming the unlabeled sentence is
my dog is
hairy, and during the random masking procedure
we chose the 4-th token (which corresponding to
hairy), our masking procedure can be further il-
lustrated by
• 80% of the time: Replace the word with the
[MASK] token, e.g., my dog is hairy →
my dog is [MASK]
• 10% of the time: Replace the word with a
random word, e.g., my dog is hairy →my
dog is apple
• 10% of the time:
Keep the word un-
changed, e.g., my dog is hairy →my dog
is hairy. The purpose of this is to bias the
representation towards the actual observed
word.
The advantage of this procedure is that the
Transformer encoder does not know which words
it will be asked to predict or which have been re-
placed by random words, so it is forced to keep
a distributional contextual representation of ev-
ery input token.
Additionally, because random
replacement only occurs for 1.5% of all tokens
(i.e., 10% of 15%), this does not seem to harm
the model’s language understanding capability. In
Section C.2, we evaluate the impact this proce-
dure.
Compared to standard langauge model training,
the masked LM only make predictions on 15% of
tokens in each batch, which suggests that more
pre-training steps may be required for the model

82.90898132324219 | 66.03762817382812 | 290.2705078125 | 109.05105590820312 | for natural language understanding. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: An-
alyzing and Interpreting Neural Networks for NLP,
pages 353–355.
 |  | 
71.99998474121094 | 120.79145812988281 | 290.2705078125 | 196.50796508789062 | Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi-
granularity hierarchical attention fusion networks
for reading comprehension and question answering.
In Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Lin-
guistics.
 | 1 | 
71.99998474121094 | 208.2483673095703 | 290.2705383300781 | 240.12796020507812 | Alex Warstadt, Amanpreet Singh, and Samuel R Bow-
man. 2018.
Neural network acceptability judg-
ments. arXiv preprint arXiv:1805.12471.
 | 2 | 
71.99996948242188 | 251.8683624267578 | 290.2705383300781 | 294.7079162597656 | Adina Williams, Nikita Nangia, and Samuel R Bow-
man. 2018.
A broad-coverage challenge corpus
for sentence understanding through inference.
In
NAACL.
 | 3 | 
71.99996948242188 | 306.44830322265625 | 290.2705383300781 | 382.1648254394531 | Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le,
Mohammad Norouzi,
Wolfgang Macherey,
Maxim Krikun,
Yuan Cao,
Qin Gao,
Klaus
Macherey, et al. 2016.
Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation.
arXiv preprint
arXiv:1609.08144.
 | 4 | 
71.9999771118164 | 393.90521240234375 | 290.27056884765625 | 436.7438049316406 | Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod
Lipson. 2014. How transferable are features in deep
neural networks? In Advances in neural information
processing systems, pages 3320–3328.
 | 5 | 
71.99998474121094 | 448.4851989746094 | 290.27056884765625 | 502.2828063964844 | Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le. 2018.
QANet: Combining local convolution
with global self-attention for reading comprehen-
sion. In ICLR.
 | 6 | 
71.99998474121094 | 514.023193359375 | 290.2705993652344 | 567.8217163085938 | Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin
Choi. 2018. Swag: A large-scale adversarial dataset
for grounded commonsense inference. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing (EMNLP).
 | 7 | 
71.99998474121094 | 579.5621337890625 | 290.2714538574219 | 655.2777099609375 | Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books.
In Proceedings of the IEEE
international conference on computer vision, pages
19–27.
 | 8 | 
85.91796875 | 669.2068481445312 | 276.35235595703125 | 708.260009765625 | Appendix for “BERT: Pre-training of
Deep Bidirectional Transformers for
Language Understanding”
 | 9 | 
82.90896606445312 | 716.849365234375 | 281.1382141113281 | 727.7584228515625 | We organize the appendix into three sections:
 | 10 | 
85.01896667480469 | 741.142333984375 | 290.26275634765625 | 765.6013793945312 | • Additional implementation details for BERT
are presented in Appendix A;
 | 11 | 
320.2939453125 | 65.49331665039062 | 525.5485229492188 | 89.95143127441406 | • Additional details for our experiments are
presented in Appendix B; and
 | 12 | 
320.2939453125 | 102.22335815429688 | 525.5484619140625 | 126.68147277832031 | • Additional ablation studies are presented in
Appendix C.
 | 13 | 
329.09393310546875 | 134.13735961914062 | 525.544921875 | 158.59547424316406 | We present additional ablation studies for
BERT including:
 | 14 | 
340.4759216308594 | 168.1097869873047 | 525.544677734375 | 208.37550354003906 | – Effect of Number of Training Steps; and
– Ablation for Different Masking Proce-
dures.
 | 15 | 
307.27593994140625 | 220.9058837890625 | 473.5488586425781 | 232.861083984375 | A
Additional Details for BERT
 | 16 | 
307.27593994140625 | 243.0327606201172 | 505.1669006347656 | 253.9418487548828 | A.1
Illustration of the Pre-training Tasks
 | 17 | 
307.27593994140625 | 260.9393615722656 | 525.5449829101562 | 285.3974609375 | We provide examples of the pre-training tasks in
the following.
 | 18 | 
307.27593994140625 | 295.8767395019531 | 525.543701171875 | 320.4344482421875 | Masked LM and the Masking Procedure
As-
suming the unlabeled sentence is
my dog is
 | 19 | 
307.27593994140625 | 323.0753479003906 | 525.545166015625 | 347.533447265625 | hairy, and during the random masking procedure
we chose the 4-th token (which corresponding to
 | 20 | 
307.27593994140625 | 350.1733703613281 | 525.5422973632812 | 374.6314697265625 | hairy), our masking procedure can be further il-
lustrated by
 | 21 | 
320.2939453125 | 386.9023742675781 | 525.5484619140625 | 411.3614501953125 | • 80% of the time: Replace the word with the
[MASK] token, e.g., my dog is hairy →
 | 22 | 
329.09393310546875 | 415.0156555175781 | 415.17156982421875 | 423.9820556640625 | my dog is [MASK]
 | 23 | 
320.2939453125 | 437.1813659667969 | 525.5484619140625 | 461.63946533203125 | • 10% of the time: Replace the word with a
random word, e.g., my dog is hairy →my
 | 24 | 
329.09393310546875 | 465.2936706542969 | 393.65216064453125 | 474.26007080078125 | dog is apple
 | 25 | 
320.2939453125 | 487.46038818359375 | 525.548583984375 | 511.9184875488281 | • 10% of the time:
Keep the word un-
changed, e.g., my dog is hairy →my dog
 | 26 | 
329.09393310546875 | 514.5584716796875 | 525.5465087890625 | 552.5655517578125 | is hairy. The purpose of this is to bias the
representation towards the actual observed
word.
 | 27 | 
307.27593994140625 | 564.8374633789062 | 525.5452880859375 | 765.6015014648438 | The advantage of this procedure is that the
Transformer encoder does not know which words
it will be asked to predict or which have been re-
placed by random words, so it is forced to keep
a distributional contextual representation of ev-
ery input token.
Additionally, because random
replacement only occurs for 1.5% of all tokens
(i.e., 10% of 15%), this does not seem to harm
the model’s language understanding capability. In
Section C.2, we evaluate the impact this proce-
dure.
Compared to standard langauge model training,
the masked LM only make predictions on 15% of
tokens in each batch, which suggests that more
pre-training steps may be required for the model
 | 28 | 
BERT (Ours)
Trm
Trm
Trm
Trm
Trm
Trm
...
...
Trm
Trm
Trm
Trm
Trm
Trm
...
...
OpenAI GPT
Lstm
ELMo
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
Lstm
 T1
T2
 TN
...
...
...
...
...
 E1
E2
 EN
...
 T1
T2
TN
...
 E1
E2
 EN
...
 T1
T2
 TN
...
 E1
E2
 EN
...
Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT
uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-
left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly
conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and
OpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach.
to converge. In Section C.1 we demonstrate that
MLM does converge marginally slower than a left-
to-right model (which predicts every token), but
the empirical improvements of the MLM model
far outweigh the increased training cost.
Next Sentence Prediction
The next sentence
prediction task can be illustrated in the following
examples.
Input = [CLS] the man went to [MASK] store [SEP]
he bought a gallon [MASK] milk [SEP]
Label = IsNext
Input = [CLS] the man [MASK] to the store [SEP]
penguin [MASK] are flight ##less birds [SEP]
Label = NotNext
A.2
Pre-training Procedure
To generate each training input sequence, we sam-
ple two spans of text from the corpus, which we
refer to as “sentences” even though they are typ-
ically much longer than single sentences (but can
be shorter also). The ﬁrst sentence receives the A
embedding and the second receives the B embed-
ding. 50% of the time B is the actual next sentence
that follows A and 50% of the time it is a random
sentence, which is done for the “next sentence pre-
diction” task. They are sampled such that the com-
bined length is ≤512 tokens. The LM masking is
applied after WordPiece tokenization with a uni-
form masking rate of 15%, and no special consid-
eration given to partial word pieces.
We train with batch size of 256 sequences (256
sequences * 512 tokens = 128,000 tokens/batch)
for 1,000,000 steps, which is approximately 40
epochs over the 3.3 billion word corpus.
We
use Adam with learning rate of 1e-4, β1 = 0.9,
β2 = 0.999, L2 weight decay of 0.01, learning
rate warmup over the ﬁrst 10,000 steps, and linear
decay of the learning rate. We use a dropout prob-
ability of 0.1 on all layers. We use a gelu acti-
vation (Hendrycks and Gimpel, 2016) rather than
the standard relu, following OpenAI GPT. The
training loss is the sum of the mean masked LM
likelihood and the mean next sentence prediction
likelihood.
Training of BERTBASE was performed on 4
Cloud TPUs in Pod conﬁguration (16 TPU chips
total).13 Training of BERTLARGE was performed
on 16 Cloud TPUs (64 TPU chips total). Each pre-
training took 4 days to complete.
Longer sequences are disproportionately expen-
sive because attention is quadratic to the sequence
length. To speed up pretraing in our experiments,
we pre-train the model with sequence length of
128 for 90% of the steps. Then, we train the rest
10% of the steps of sequence of 512 to learn the
positional embeddings.
A.3
Fine-tuning Procedure
For ﬁne-tuning, most model hyperparameters are
the same as in pre-training, with the exception of
the batch size, learning rate, and number of train-
ing epochs. The dropout probability was always
kept at 0.1. The optimal hyperparameter values
are task-speciﬁc, but we found the following range
of possible values to work well across all tasks:
• Batch size: 16, 32
13https://cloudplatform.googleblog.com/2018/06/Cloud-
TPU-now-offers-preemptible-pricing-and-global-
availability.html

111.63350677490234 | 67.04566192626953 | 164.4841766357422 | 76.2004623413086 | BERT (Ours)
 |  | 
95.89209747314453 | 101.69269561767578 | 172.67330932617188 | 106.27461242675781 | Trm
Trm
Trm
 | 1 | 
95.88285827636719 | 126.868408203125 | 172.67330932617188 | 131.44581604003906 | Trm
Trm
Trm
 | 2 | 
140.91517639160156 | 100.23548126220703 | 145.35296630859375 | 105.57577514648438 | ...
 | 3 | 
140.91517639160156 | 125.41118621826172 | 145.35296630859375 | 130.75148010253906 | ...
 | 4 | 
214.89524841308594 | 101.69269561767578 | 291.6856994628906 | 106.27009582519531 | Trm
Trm
Trm
 | 5 | 
214.89524841308594 | 126.868408203125 | 292.0022277832031 | 131.44581604003906 | Trm
Trm
Trm
 | 6 | 
259.9275817871094 | 100.23548126220703 | 264.3653564453125 | 105.57577514648438 | ...
 | 7 | 
259.9275817871094 | 125.41118621826172 | 264.3653564453125 | 130.75148010253906 | ...
 | 8 | 
223.041259765625 | 67.43537139892578 | 275.3883972167969 | 76.59017181396484 | OpenAI GPT
 | 9 | 
324.3134460449219 | 109.7369384765625 | 334.22808837890625 | 114.31433868408203 | Lstm
 | 10 | 
405.18243408203125 | 69.58389282226562 | 429.0856018066406 | 78.73869323730469 | ELMo
 | 11 | 
354.8880615234375 | 109.73727416992188 | 405.9992980957031 | 114.3146743774414 | Lstm
Lstm
 | 12 | 
324.1142883300781 | 130.3705291748047 | 405.9992980957031 | 134.94793701171875 | Lstm
Lstm
Lstm
 | 13 | 
425.8377380371094 | 109.73727416992188 | 508.99078369140625 | 114.3146743774414 | Lstm
Lstm
Lstm
 | 14 | 
425.8377380371094 | 130.3705291748047 | 508.99078369140625 | 134.94793701171875 | Lstm
Lstm
Lstm
 | 15 | 
217.0360870361328 | 84.21458435058594 | 291.6246032714844 | 90.86573791503906 |  T1
T2
 TN
...
 | 16 | 
377.7414855957031 | 110.64876556396484 | 382.17926025390625 | 115.98905944824219 | ...
 | 17 | 
377.7414855957031 | 131.24705505371094 | 382.17926025390625 | 136.5873565673828 | ...
 | 18 | 
478.44427490234375 | 112.93746185302734 | 482.8820495605469 | 118.27775573730469 | ...
 | 19 | 
480.73297119140625 | 131.24705505371094 | 485.1707458496094 | 136.5873565673828 | ...
 | 20 | 
217.0246124267578 | 149.44252014160156 | 292.2992858886719 | 156.09368896484375 |  E1
E2
 EN
...
 | 21 | 
98.16674041748047 | 84.21458435058594 | 171.97634887695312 | 90.86573791503906 |  T1
T2
TN
...
 | 22 | 
97.75213623046875 | 150.10305786132812 | 172.3115997314453 | 156.1984405517578 |  E1
E2
 EN
...
 | 23 | 
381.8224792480469 | 84.21458435058594 | 456.4109802246094 | 90.86573791503906 |  T1
T2
 TN
...
 | 24 | 
383.5535583496094 | 151.73123168945312 | 458.82818603515625 | 158.38238525390625 |  E1
E2
 EN
...
 | 25 | 
72.0 | 183.46546936035156 | 525.54736328125 | 241.24813842773438 | Figure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT
uses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-
left LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly
conditioned on both left and right context in all layers. In addition to the architecture differences, BERT and
OpenAI GPT are ﬁne-tuning approaches, while ELMo is a feature-based approach.
 | 26 | 
72.0 | 265.0387268066406 | 290.269287109375 | 330.14483642578125 | to converge. In Section C.1 we demonstrate that
MLM does converge marginally slower than a left-
to-right model (which predicts every token), but
the empirical improvements of the MLM model
far outweigh the increased training cost.
 | 27 | 
72.0 | 341.32012939453125 | 290.26922607421875 | 379.4278259277344 | Next Sentence Prediction
The next sentence
prediction task can be illustrated in the following
examples.
 | 28 | 
72.0 | 393.8752746582031 | 279.97515869140625 | 405.0368347167969 | Input = [CLS] the man went to [MASK] store [SEP]
 | 29 | 
111.61500549316406 | 413.293212890625 | 262.24908447265625 | 420.2669982910156 | he bought a gallon [MASK] milk [SEP]
 | 30 | 
73.21900939941406 | 426.9512634277344 | 137.70968627929688 | 438.1128234863281 | Label = IsNext
 | 31 | 
72.00001525878906 | 460.0272521972656 | 275.7908935546875 | 471.1888122558594 | Input = [CLS] the man [MASK] to the store [SEP]
 | 32 | 
111.61502075195312 | 479.4451904296875 | 295.72320556640625 | 486.4189758300781 | penguin [MASK] are flight ##less birds [SEP]
 | 33 | 
73.21902465820312 | 493.1032409667969 | 141.8939971923828 | 504.2648010253906 | Label = NotNext
 | 34 | 
72.00003051757812 | 519.233154296875 | 207.14198303222656 | 530.1422119140625 | A.2
Pre-training Procedure
 | 35 | 
72.00003051757812 | 537.5377197265625 | 290.2693176269531 | 765.6017456054688 | To generate each training input sequence, we sam-
ple two spans of text from the corpus, which we
refer to as “sentences” even though they are typ-
ically much longer than single sentences (but can
be shorter also). The ﬁrst sentence receives the A
embedding and the second receives the B embed-
ding. 50% of the time B is the actual next sentence
that follows A and 50% of the time it is a random
sentence, which is done for the “next sentence pre-
diction” task. They are sampled such that the com-
bined length is ≤512 tokens. The LM masking is
applied after WordPiece tokenization with a uni-
form masking rate of 15%, and no special consid-
eration given to partial word pieces.
We train with batch size of 256 sequences (256
sequences * 512 tokens = 128,000 tokens/batch)
for 1,000,000 steps, which is approximately 40
 | 36 | 
307.2760009765625 | 265.0386657714844 | 525.546630859375 | 575.10693359375 | epochs over the 3.3 billion word corpus.
We
use Adam with learning rate of 1e-4, β1 = 0.9,
β2 = 0.999, L2 weight decay of 0.01, learning
rate warmup over the ﬁrst 10,000 steps, and linear
decay of the learning rate. We use a dropout prob-
ability of 0.1 on all layers. We use a gelu acti-
vation (Hendrycks and Gimpel, 2016) rather than
the standard relu, following OpenAI GPT. The
training loss is the sum of the mean masked LM
likelihood and the mean next sentence prediction
likelihood.
Training of BERTBASE was performed on 4
Cloud TPUs in Pod conﬁguration (16 TPU chips
total).13 Training of BERTLARGE was performed
on 16 Cloud TPUs (64 TPU chips total). Each pre-
training took 4 days to complete.
Longer sequences are disproportionately expen-
sive because attention is quadratic to the sequence
length. To speed up pretraing in our experiments,
we pre-train the model with sequence length of
128 for 90% of the steps. Then, we train the rest
10% of the steps of sequence of 512 to learn the
positional embeddings.
 | 37 | 
307.27606201171875 | 588.3492431640625 | 439.6033630371094 | 599.25830078125 | A.3
Fine-tuning Procedure
 | 38 | 
307.27606201171875 | 606.993896484375 | 525.5452270507812 | 699.1979370117188 | For ﬁne-tuning, most model hyperparameters are
the same as in pre-training, with the exception of
the batch size, learning rate, and number of train-
ing epochs. The dropout probability was always
kept at 0.1. The optimal hyperparameter values
are task-speciﬁc, but we found the following range
of possible values to work well across all tasks:
 | 39 | 
320.2950744628906 | 714.124267578125 | 409.7444152832031 | 725.1329345703125 | • Batch size: 16, 32
 | 40 | 
307.2760009765625 | 734.6986694335938 | 518.6942138671875 | 765.1323852539062 | 13https://cloudplatform.googleblog.com/2018/06/Cloud-
TPU-now-offers-preemptible-pricing-and-global-
availability.html
 | 41 | 
• Learning rate (Adam): 5e-5, 3e-5, 2e-5
• Number of epochs: 2, 3, 4
We also observed that large data sets (e.g.,
100k+ labeled training examples) were far less
sensitive to hyperparameter choice than small data
sets. Fine-tuning is typically very fast, so it is rea-
sonable to simply run an exhaustive search over
the above parameters and choose the model that
performs best on the development set.
A.4
Comparison of BERT, ELMo ,and
OpenAI GPT
Here we studies the differences in recent popular
representation learning models including ELMo,
OpenAI GPT and BERT. The comparisons be-
tween the model architectures are shown visually
in Figure 3. Note that in addition to the architec-
ture differences, BERT and OpenAI GPT are ﬁne-
tuning approaches, while ELMo is a feature-based
approach.
The most comparable existing pre-training
method to BERT is OpenAI GPT, which trains a
left-to-right Transformer LM on a large text cor-
pus. In fact, many of the design decisions in BERT
were intentionally made to make it as close to
GPT as possible so that the two methods could be
minimally compared. The core argument of this
work is that the bi-directionality and the two pre-
training tasks presented in Section 3.1 account for
the majority of the empirical improvements, but
we do note that there are several other differences
between how BERT and GPT were trained:
• GPT is trained on the BooksCorpus (800M
words); BERT is trained on the BooksCor-
pus (800M words) and Wikipedia (2,500M
words).
• GPT uses a sentence separator ([SEP]) and
classiﬁer token ([CLS]) which are only in-
troduced at ﬁne-tuning time; BERT learns
[SEP], [CLS] and sentence A/B embed-
dings during pre-training.
• GPT was trained for 1M steps with a batch
size of 32,000 words; BERT was trained for
1M steps with a batch size of 128,000 words.
• GPT used the same learning rate of 5e-5 for
all ﬁne-tuning experiments; BERT chooses a
task-speciﬁc ﬁne-tuning learning rate which
performs the best on the development set.
To isolate the effect of these differences, we per-
form ablation experiments in Section 5.1 which
demonstrate that the majority of the improvements
are in fact coming from the two pre-training tasks
and the bidirectionality they enable.
A.5
Illustrations of Fine-tuning on Different
Tasks
The illustration of ﬁne-tuning BERT on different
tasks can be seen in Figure 4. Our task-speciﬁc
models are formed by incorporating BERT with
one additional output layer, so a minimal num-
ber of parameters need to be learned from scratch.
Among the tasks, (a) and (b) are sequence-level
tasks while (c) and (d) are token-level tasks. In
the ﬁgure, E represents the input embedding, Ti
represents the contextual representation of token i,
[CLS] is the special symbol for classiﬁcation out-
put, and [SEP] is the special symbol to separate
non-consecutive token sequences.
B
Detailed Experimental Setup
B.1
Detailed Descriptions for the GLUE
Benchmark Experiments.
Our
GLUE
results
in
Table1
are
obtained
from
https://gluebenchmark.com/
leaderboard
and
https://blog.
openai.com/language-unsupervised.
The GLUE benchmark includes the following
datasets, the descriptions of which were originally
summarized in Wang et al. (2018a):
MNLI
Multi-Genre Natural Language Inference
is a large-scale, crowdsourced entailment classiﬁ-
cation task (Williams et al., 2018). Given a pair of
sentences, the goal is to predict whether the sec-
ond sentence is an entailment, contradiction, or
neutral with respect to the ﬁrst one.
QQP
Quora Question Pairs is a binary classiﬁ-
cation task where the goal is to determine if two
questions asked on Quora are semantically equiv-
alent (Chen et al., 2018).
QNLI
Question Natural Language Inference is
a version of the Stanford Question Answering
Dataset (Rajpurkar et al., 2016) which has been
converted to a binary classiﬁcation task (Wang
et al., 2018a). The positive examples are (ques-
tion, sentence) pairs which do contain the correct
answer, and the negative examples are (question,
sentence) from the same paragraph which do not
contain the answer.

85.01899719238281 | 65.39408111572266 | 272.1480712890625 | 89.95277404785156 | • Learning rate (Adam): 5e-5, 3e-5, 2e-5
• Number of epochs: 2, 3, 4
 |  | 
71.99998474121094 | 103.59567260742188 | 290.2692565917969 | 195.79981994628906 | We also observed that large data sets (e.g.,
100k+ labeled training examples) were far less
sensitive to hyperparameter choice than small data
sets. Fine-tuning is typically very fast, so it is rea-
sonable to simply run an exhaustive search over
the above parameters and choose the model that
performs best on the development set.
 | 1 | 
71.99998474121094 | 207.97813415527344 | 258.3492431640625 | 232.4362335205078 | A.4
Comparison of BERT, ELMo ,and
OpenAI GPT
 | 2 | 
71.99998474121094 | 239.56173706054688 | 290.269287109375 | 508.1369323730469 | Here we studies the differences in recent popular
representation learning models including ELMo,
OpenAI GPT and BERT. The comparisons be-
tween the model architectures are shown visually
in Figure 3. Note that in addition to the architec-
ture differences, BERT and OpenAI GPT are ﬁne-
tuning approaches, while ELMo is a feature-based
approach.
The most comparable existing pre-training
method to BERT is OpenAI GPT, which trains a
left-to-right Transformer LM on a large text cor-
pus. In fact, many of the design decisions in BERT
were intentionally made to make it as close to
GPT as possible so that the two methods could be
minimally compared. The core argument of this
work is that the bi-directionality and the two pre-
training tasks presented in Section 3.1 account for
the majority of the empirical improvements, but
we do note that there are several other differences
between how BERT and GPT were trained:
 | 3 | 
85.01898193359375 | 521.7808837890625 | 290.2690734863281 | 573.3369750976562 | • GPT is trained on the BooksCorpus (800M
words); BERT is trained on the BooksCor-
pus (800M words) and Wikipedia (2,500M
words).
 | 4 | 
85.01898193359375 | 585.3103637695312 | 290.2690124511719 | 650.9749755859375 | • GPT uses a sentence separator ([SEP]) and
classiﬁer token ([CLS]) which are only in-
troduced at ﬁne-tuning time; BERT learns
[SEP], [CLS] and sentence A/B embed-
dings during pre-training.
 | 5 | 
85.01898193359375 | 663.5059204101562 | 290.26904296875 | 701.5139770507812 | • GPT was trained for 1M steps with a batch
size of 32,000 words; BERT was trained for
1M steps with a batch size of 128,000 words.
 | 6 | 
85.01898193359375 | 714.044921875 | 290.26904296875 | 765.6019287109375 | • GPT used the same learning rate of 5e-5 for
all ﬁne-tuning experiments; BERT chooses a
task-speciﬁc ﬁne-tuning learning rate which
performs the best on the development set.
 | 7 | 
307.2760009765625 | 65.49386596679688 | 525.5451049804688 | 130.59999084472656 | To isolate the effect of these differences, we per-
form ablation experiments in Section 5.1 which
demonstrate that the majority of the improvements
are in fact coming from the two pre-training tasks
and the bidirectionality they enable.
 | 8 | 
307.2760009765625 | 141.31431579589844 | 519.50146484375 | 165.7724151611328 | A.5
Illustrations of Fine-tuning on Different
Tasks
 | 9 | 
307.2759704589844 | 172.43991088867188 | 525.5452270507812 | 332.39007568359375 | The illustration of ﬁne-tuning BERT on different
tasks can be seen in Figure 4. Our task-speciﬁc
models are formed by incorporating BERT with
one additional output layer, so a minimal num-
ber of parameters need to be learned from scratch.
Among the tasks, (a) and (b) are sequence-level
tasks while (c) and (d) are token-level tasks. In
the ﬁgure, E represents the input embedding, Ti
represents the contextual representation of token i,
[CLS] is the special symbol for classiﬁcation out-
put, and [SEP] is the special symbol to separate
non-consecutive token sequences.
 | 10 | 
307.2759704589844 | 343.6814880371094 | 474.6487731933594 | 355.6366882324219 | B
Detailed Experimental Setup
 | 11 | 
307.2759704589844 | 364.734375 | 500.628662109375 | 389.1925048828125 | B.1
Detailed Descriptions for the GLUE
Benchmark Experiments.
 | 12 | 
307.2759704589844 | 395.8599853515625 | 525.5452270507812 | 488.0641174316406 | Our
GLUE
results
in
Table1
are
obtained
from
https://gluebenchmark.com/
leaderboard
and
https://blog.
openai.com/language-unsupervised.
The GLUE benchmark includes the following
datasets, the descriptions of which were originally
summarized in Wang et al. (2018a):
 | 13 | 
307.2759704589844 | 497.305419921875 | 525.5451049804688 | 576.0601806640625 | MNLI
Multi-Genre Natural Language Inference
is a large-scale, crowdsourced entailment classiﬁ-
cation task (Williams et al., 2018). Given a pair of
sentences, the goal is to predict whether the sec-
ond sentence is an entailment, contradiction, or
neutral with respect to the ﬁrst one.
 | 14 | 
307.2760009765625 | 585.301513671875 | 525.5452270507812 | 636.9581298828125 | QQP
Quora Question Pairs is a binary classiﬁ-
cation task where the goal is to determine if two
questions asked on Quora are semantically equiv-
alent (Chen et al., 2018).
 | 15 | 
307.2760009765625 | 646.199462890625 | 525.545166015625 | 765.6021118164062 | QNLI
Question Natural Language Inference is
a version of the Stanford Question Answering
Dataset (Rajpurkar et al., 2016) which has been
converted to a binary classiﬁcation task (Wang
et al., 2018a). The positive examples are (ques-
tion, sentence) pairs which do contain the correct
answer, and the negative examples are (question,
sentence) from the same paragraph which do not
contain the answer.
 | 16 | 
BERT
E[CLS]
E1
 E[SEP]
...
EN
E1’
...
EM’
C
T1
T[SEP]
...
TN
T1’
...
TM’
[CLS]
Tok 
1
 [SEP]
...
Tok 
N
Tok 
1
...
Tok
M
Question
Paragraph
BERT
E[CLS]
E1
 E2
 EN
C
T1
 T2
 TN
Single Sentence 
...
...
BERT
Tok 1
 Tok 2
 Tok N
...
[CLS]
E[CLS]
E1
 E2
 EN
C
T1
 T2
 TN
Single Sentence 
B-PER
O
O
...
...
E[CLS]
E1
 E[SEP]
Class 
Label
...
EN
E1’
...
EM’
C
T1
T[SEP]
...
TN
T1’
...
TM’
Start/End Span
Class 
Label
BERT
Tok 1
 Tok 2
 Tok N
...
[CLS]
Tok 1
[CLS]
[CLS]
Tok 
1
 [SEP]
...
Tok 
N
Tok 
1
...
Tok
M
Sentence 1
...
Sentence 2
Figure 4: Illustrations of Fine-tuning BERT on Different Tasks.
SST-2
The Stanford Sentiment Treebank is a
binary single-sentence classiﬁcation task consist-
ing of sentences extracted from movie reviews
with human annotations of their sentiment (Socher
et al., 2013).
CoLA
The Corpus of Linguistic Acceptability is
a binary single-sentence classiﬁcation task, where
the goal is to predict whether an English sentence
is linguistically “acceptable” or not (Warstadt
et al., 2018).
STS-B
The Semantic Textual Similarity Bench-
mark is a collection of sentence pairs drawn from
news headlines and other sources (Cer et al.,
2017). They were annotated with a score from 1
to 5 denoting how similar the two sentences are in
terms of semantic meaning.
MRPC
Microsoft Research Paraphrase Corpus
consists of sentence pairs automatically extracted
from online news sources, with human annotations
for whether the sentences in the pair are semanti-
cally equivalent (Dolan and Brockett, 2005).
RTE
Recognizing Textual Entailment is a bi-
nary entailment task similar to MNLI, but with
much less training data (Bentivogli et al., 2009).14
WNLI
Winograd NLI is a small natural lan-
guage inference dataset (Levesque et al., 2011).
The GLUE webpage notes that there are issues
with the construction of this dataset, 15 and every
trained system that’s been submitted to GLUE has
performed worse than the 65.1 baseline accuracy
of predicting the majority class. We therefore ex-
clude this set to be fair to OpenAI GPT. For our
GLUE submission, we always predicted the ma-
14Note that we only report single-task ﬁne-tuning results
in this paper. A multitask ﬁne-tuning approach could poten-
tially push the performance even further. For example, we
did observe substantial improvements on RTE from multi-
task training with MNLI.
15https://gluebenchmark.com/faq

182.57586669921875 | 313.81683349609375 | 212.12901306152344 | 324.91033935546875 | BERT
 |  | 
122.7163314819336 | 341.7343444824219 | 267.7628173828125 | 349.8827209472656 | E[CLS]
E1
 E[SEP]
...
EN
E1’
...
EM’
 | 1 | 
125.74142456054688 | 286.2667236328125 | 267.60711669921875 | 294.37188720703125 | C
T1
T[SEP]
...
TN
T1’
...
TM’
 | 2 | 
123.70043182373047 | 369.3148498535156 | 151.38294982910156 | 375.0655212402344 | [CLS]
Tok 
 | 3 | 
146.2498779296875 | 369.3148498535156 | 213.58718872070312 | 377.5194396972656 | 1
 [SEP]
...
Tok 
 | 4 | 
184.7705535888672 | 373.82159423828125 | 187.44039916992188 | 377.5194396972656 | N
 | 5 | 
227.39942932128906 | 369.3148498535156 | 234.5843505859375 | 373.0126953125 | Tok 
 | 6 | 
229.45127868652344 | 369.3148498535156 | 266.840576171875 | 377.5194396972656 | 1
...
Tok
 | 7 | 
262.220458984375 | 373.82159423828125 | 265.3007507324219 | 377.5194396972656 | M
 | 8 | 
150.94972229003906 | 396.7442626953125 | 259.4268798828125 | 403.5621643066406 | Question
Paragraph
 | 9 | 
179.80247497558594 | 125.22698974609375 | 209.35560607910156 | 136.3205108642578 | BERT
 | 10 | 
325.9317321777344 | 343.2680969238281 | 464.0362243652344 | 351.3390197753906 | E[CLS]
E1
 E2
 EN
 | 11 | 
330.3983154296875 | 285.0271301269531 | 464.9919128417969 | 293.0980529785156 | C
T1
 T2
 TN
 | 12 | 
373.3971252441406 | 397.7842712402344 | 422.2872009277344 | 404.2554931640625 | Single Sentence 
 | 13 | 
416.385986328125 | 283.49334716796875 | 421.7635803222656 | 289.9645690917969 | ...
 | 14 | 
416.385986328125 | 341.7343444824219 | 421.7635803222656 | 348.20556640625 | ...
 | 15 | 
382.2592468261719 | 313.81683349609375 | 411.8124084472656 | 324.91033935546875 | BERT
 | 16 | 
326.6232604980469 | 368.5191345214844 | 468.36395263671875 | 375.9393615722656 | Tok 1
 Tok 2
 Tok N
...
[CLS]
 | 17 | 
328.7051086425781 | 154.6782989501953 | 466.80963134765625 | 162.7491912841797 | E[CLS]
E1
 E2
 EN
 | 18 | 
333.17169189453125 | 96.43730163574219 | 467.7652893066406 | 104.5082015991211 | C
T1
 T2
 TN
 | 19 | 
376.1714172363281 | 209.19442749023438 | 425.0614929199219 | 215.6656494140625 | Single Sentence 
 | 20 | 
352.5982360839844 | 261.30633544921875 | 463.01483154296875 | 267.7775573730469 | B-PER
O
O
 | 21 | 
419.15936279296875 | 94.90350341796875 | 424.5369567871094 | 101.37472534179688 | ...
 | 22 | 
119.94295501708984 | 153.1444854736328 | 424.5369567871094 | 161.2928466796875 | ...
E[CLS]
E1
 E[SEP]
 | 23 | 
114.08756256103516 | 66.97134399414062 | 132.05813598632812 | 81.0693588256836 | Class 
Label
 | 24 | 
156.5207977294922 | 153.1444854736328 | 264.98944091796875 | 161.24240112304688 | ...
EN
E1’
...
EM’
 | 25 | 
122.9680404663086 | 97.6768798828125 | 264.8337097167969 | 105.78203582763672 | C
T1
T[SEP]
...
TN
T1’
...
TM’
 | 26 | 
226.00636291503906 | 261.30633544921875 | 269.848876953125 | 267.7775573730469 | Start/End Span
 | 27 | 
325.07464599609375 | 68.3580322265625 | 343.04522705078125 | 82.4560546875 | Class 
Label
 | 28 | 
385.0326232910156 | 125.22698974609375 | 414.5857849121094 | 136.3205108642578 | BERT
 | 29 | 
120.92705535888672 | 179.92926025390625 | 471.1373291015625 | 187.34950256347656 | Tok 1
 Tok 2
 Tok N
...
[CLS]
Tok 1
[CLS]
[CLS]
Tok 
 | 30 | 
143.47650146484375 | 180.72500610351562 | 210.8137969970703 | 188.92959594726562 | 1
 [SEP]
...
Tok 
 | 31 | 
181.99717712402344 | 185.23175048828125 | 184.66702270507812 | 188.92959594726562 | N
 | 32 | 
224.6260528564453 | 180.72500610351562 | 231.81097412109375 | 184.4228515625 | Tok 
 | 33 | 
226.6779022216797 | 180.72500610351562 | 264.06719970703125 | 188.92959594726562 | 1
...
Tok
 | 34 | 
259.44708251953125 | 185.23175048828125 | 262.5273742675781 | 188.92959594726562 | M
 | 35 | 
144.75779724121094 | 208.15440368652344 | 177.47628784179688 | 214.62562561035156 | Sentence 1
 | 36 | 
416.385986328125 | 258.5329284667969 | 421.7635803222656 | 265.004150390625 | ...
 | 37 | 
225.18580627441406 | 208.50108337402344 | 257.904296875 | 214.97230529785156 | Sentence 2
 | 38 | 
172.41700744628906 | 453.43646240234375 | 425.1283264160156 | 463.3990783691406 | Figure 4: Illustrations of Fine-tuning BERT on Different Tasks.
 | 39 | 
72.00000762939453 | 487.0900573730469 | 290.2692565917969 | 552.2957763671875 | SST-2
The Stanford Sentiment Treebank is a
binary single-sentence classiﬁcation task consist-
ing of sentences extracted from movie reviews
with human annotations of their sentiment (Socher
et al., 2013).
 | 40 | 
72.0 | 562.7081298828125 | 290.26947021484375 | 627.9137573242188 | CoLA
The Corpus of Linguistic Acceptability is
a binary single-sentence classiﬁcation task, where
the goal is to predict whether an English sentence
is linguistically “acceptable” or not (Warstadt
et al., 2018).
 | 41 | 
72.0 | 638.3270874023438 | 290.269287109375 | 717.0817260742188 | STS-B
The Semantic Textual Similarity Bench-
mark is a collection of sentence pairs drawn from
news headlines and other sources (Cer et al.,
2017). They were annotated with a score from 1
to 5 denoting how similar the two sentences are in
terms of semantic meaning.
 | 42 | 
72.0 | 727.4940795898438 | 290.26922607421875 | 765.6017456054688 | MRPC
Microsoft Research Paraphrase Corpus
consists of sentence pairs automatically extracted
from online news sources, with human annotations
 | 43 | 
307.2760009765625 | 487.1896667480469 | 525.545166015625 | 511.64776611328125 | for whether the sentences in the pair are semanti-
cally equivalent (Dolan and Brockett, 2005).
 | 44 | 
307.2760009765625 | 523.6751098632812 | 525.5449829101562 | 561.7817993164062 | RTE
Recognizing Textual Entailment is a bi-
nary entailment task similar to MNLI, but with
much less training data (Bentivogli et al., 2009).14
 | 45 | 
307.2759704589844 | 573.80908203125 | 525.5452270507812 | 693.2107543945312 | WNLI
Winograd NLI is a small natural lan-
guage inference dataset (Levesque et al., 2011).
The GLUE webpage notes that there are issues
with the construction of this dataset, 15 and every
trained system that’s been submitted to GLUE has
performed worse than the 65.1 baseline accuracy
of predicting the majority class. We therefore ex-
clude this set to be fair to OpenAI GPT. For our
GLUE submission, we always predicted the ma-
 | 46 | 
307.2760009765625 | 703.898681640625 | 525.5451049804688 | 764.67333984375 | 14Note that we only report single-task ﬁne-tuning results
in this paper. A multitask ﬁne-tuning approach could poten-
tially push the performance even further. For example, we
did observe substantial improvements on RTE from multi-
task training with MNLI.
15https://gluebenchmark.com/faq
 | 47 | 
jority class.
C
Additional Ablation Studies
C.1
Effect of Number of Training Steps
Figure 5 presents MNLI Dev accuracy after ﬁne-
tuning from a checkpoint that has been pre-trained
for k steps. This allows us to answer the following
questions:
1. Question:
Does BERT really need such
a large amount of pre-training (128,000
words/batch * 1,000,000 steps) to achieve
high ﬁne-tuning accuracy?
Answer: Yes, BERTBASE achieves almost
1.0% additional accuracy on MNLI when
trained on 1M steps compared to 500k steps.
2. Question: Does MLM pre-training converge
slower than LTR pre-training, since only 15%
of words are predicted in each batch rather
than every word?
Answer: The MLM model does converge
slightly slower than the LTR model. How-
ever, in terms of absolute accuracy the MLM
model begins to outperform the LTR model
almost immediately.
C.2
Ablation for Different Masking
Procedures
In Section 3.1, we mention that BERT uses a
mixed strategy for masking the target tokens when
pre-training with the masked language model
(MLM) objective. The following is an ablation
study to evaluate the effect of different masking
strategies.
200
400
600
800
1,000
76
78
80
82
84
Pre-training Steps (Thousands)
MNLI Dev Accuracy
BERTBASE (Masked LM)
BERTBASE (Left-to-Right)
Figure 5: Ablation over number of training steps. This
shows the MNLI accuracy after ﬁne-tuning, starting
from model parameters that have been pre-trained for
k steps. The x-axis is the value of k.
Note that the purpose of the masking strategies
is to reduce the mismatch between pre-training
and ﬁne-tuning, as the [MASK] symbol never ap-
pears during the ﬁne-tuning stage. We report the
Dev results for both MNLI and NER. For NER,
we report both ﬁne-tuning and feature-based ap-
proaches, as we expect the mismatch will be am-
pliﬁed for the feature-based approach as the model
will not have the chance to adjust the representa-
tions.
Masking Rates
Dev Set Results
MASK SAME
RND
MNLI
NER
Fine-tune Fine-tune Feature-based
80%
10%
10%
84.2
95.4
94.9
100%
0%
0%
84.3
94.9
94.0
80%
0%
20%
84.1
95.2
94.6
80%
20%
0%
84.4
95.2
94.7
0%
20%
80%
83.7
94.8
94.6
0%
0% 100%
83.6
94.9
94.6
Table 8: Ablation over different masking strategies.
The results are presented in Table 8. In the table,
MASK means that we replace the target token with
the [MASK] symbol for MLM; SAME means that
we keep the target token as is; RND means that
we replace the target token with another random
token.
The numbers in the left part of the table repre-
sent the probabilities of the speciﬁc strategies used
during MLM pre-training (BERT uses 80%, 10%,
10%). The right part of the paper represents the
Dev set results. For the feature-based approach,
we concatenate the last 4 layers of BERT as the
features, which was shown to be the best approach
in Section 5.3.
From the table it can be seen that ﬁne-tuning is
surprisingly robust to different masking strategies.
However, as expected, using only the MASK strat-
egy was problematic when applying the feature-
based approach to NER. Interestingly, using only
the RND strategy performs much worse than our
strategy as well.

72.0 | 65.49368286132812 | 122.3018569946289 | 76.40278625488281 | jority class.
 |  | 
72.0 | 90.52214813232422 | 234.75811767578125 | 102.47734832763672 | C
Additional Ablation Studies
 | 1 | 
72.0 | 113.78604888916016 | 262.69110107421875 | 124.69515228271484 | C.1
Effect of Number of Training Steps
 | 2 | 
72.0 | 132.60366821289062 | 290.2693176269531 | 184.15980529785156 | Figure 5 presents MNLI Dev accuracy after ﬁne-
tuning from a checkpoint that has been pre-trained
for k steps. This allows us to answer the following
questions:
 | 3 | 
80.65499877929688 | 199.57467651367188 | 290.2734069824219 | 291.77880859375 | 1. Question:
Does BERT really need such
a large amount of pre-training (128,000
words/batch * 1,000,000 steps) to achieve
high ﬁne-tuning accuracy?
Answer: Yes, BERTBASE achieves almost
1.0% additional accuracy on MNLI when
trained on 1M steps compared to 500k steps.
 | 4 | 
80.65499877929688 | 305.8857116699219 | 290.27337646484375 | 425.1878662109375 | 2. Question: Does MLM pre-training converge
slower than LTR pre-training, since only 15%
of words are predicted in each batch rather
than every word?
Answer: The MLM model does converge
slightly slower than the LTR model. How-
ever, in terms of absolute accuracy the MLM
model begins to outperform the LTR model
almost immediately.
 | 5 | 
72.0 | 440.503173828125 | 244.2438201904297 | 464.9613037109375 | C.2
Ablation for Different Masking
Procedures
 | 6 | 
72.0 | 472.8688049316406 | 290.26934814453125 | 551.52392578125 | In Section 3.1, we mention that BERT uses a
mixed strategy for masking the target tokens when
pre-training with the masked language model
(MLM) objective. The following is an ablation
study to evaluate the effect of different masking
strategies.
 | 7 | 
125.14599609375 | 683.4577026367188 | 270.2561950683594 | 690.4315185546875 | 200
400
600
800
1,000
 | 8 | 
86.82099914550781 | 666.5797119140625 | 94.77113342285156 | 673.5535278320312 | 76
 | 9 | 
86.82099914550781 | 644.98974609375 | 94.77113342285156 | 651.9635620117188 | 78
 | 10 | 
86.82099914550781 | 623.3987426757812 | 94.77113342285156 | 630.37255859375 | 80
 | 11 | 
86.82099914550781 | 601.8077392578125 | 94.77113342285156 | 608.7815551757812 | 82
 | 12 | 
86.82099914550781 | 580.2167358398438 | 94.77113342285156 | 587.1905517578125 | 84
 | 13 | 
136.4720001220703 | 694.7481079101562 | 223.24700927734375 | 701.721923828125 | Pre-training Steps (Thousands)
 | 14 | 
75.00615692138672 | 596.9209594726562 | 81.9799575805664 | 656.281982421875 | MNLI Dev Accuracy
 | 15 | 
180.9010009765625 | 659.0921020507812 | 255.93885803222656 | 677.43896484375 | BERTBASE (Masked LM)
BERTBASE (Left-to-Right)
 | 16 | 
72.0 | 715.95849609375 | 290.2705993652344 | 761.7860717773438 | Figure 5: Ablation over number of training steps. This
shows the MNLI accuracy after ﬁne-tuning, starting
from model parameters that have been pre-trained for
k steps. The x-axis is the value of k.
 | 17 | 
307.2760009765625 | 65.49368286132812 | 525.5452270507812 | 198.34584045410156 | Note that the purpose of the masking strategies
is to reduce the mismatch between pre-training
and ﬁne-tuning, as the [MASK] symbol never ap-
pears during the ﬁne-tuning stage. We report the
Dev results for both MNLI and NER. For NER,
we report both ﬁne-tuning and feature-based ap-
proaches, as we expect the mismatch will be am-
pliﬁed for the feature-based approach as the model
will not have the chance to adjust the representa-
tions.
 | 18 | 
322.5409851074219 | 212.88601684570312 | 487.57647705078125 | 221.8524169921875 | Masking Rates
Dev Set Results
 | 19 | 
307.5 | 228.10800170898438 | 526.7459106445312 | 247.03741455078125 | MASK SAME
RND
MNLI
NER
Fine-tune Fine-tune Feature-based
 | 20 | 
310.4490051269531 | 253.29202270507812 | 509.44122314453125 | 312.07244873046875 | 80%
10%
10%
84.2
95.4
94.9
100%
0%
0%
84.3
94.9
94.0
80%
0%
20%
84.1
95.2
94.6
80%
20%
0%
84.4
95.2
94.7
0%
20%
80%
83.7
94.8
94.6
0%
0% 100%
83.6
94.9
94.6
 | 21 | 
312.5450134277344 | 331.4024658203125 | 520.2750854492188 | 341.3650817871094 | Table 8: Ablation over different masking strategies.
 | 22 | 
307.2760009765625 | 359.6506652832031 | 525.5453491210938 | 641.5438232421875 | The results are presented in Table 8. In the table,
MASK means that we replace the target token with
the [MASK] symbol for MLM; SAME means that
we keep the target token as is; RND means that
we replace the target token with another random
token.
The numbers in the left part of the table repre-
sent the probabilities of the speciﬁc strategies used
during MLM pre-training (BERT uses 80%, 10%,
10%). The right part of the paper represents the
Dev set results. For the feature-based approach,
we concatenate the last 4 layers of BERT as the
features, which was shown to be the best approach
in Section 5.3.
From the table it can be seen that ﬁne-tuning is
surprisingly robust to different masking strategies.
However, as expected, using only the MASK strat-
egy was problematic when applying the feature-
based approach to NER. Interestingly, using only
the RND strategy performs much worse than our
strategy as well.
 | 23 | 




































### Extracted from Billion-scale similarity search with GPUs.pdf ###

Billion-scale similarity search with GPUs
Jeff Johnson
Facebook AI Research
New York
Matthijs Douze
Facebook AI Research
Paris
Herv´e J´egou
Facebook AI Research
Paris
ABSTRACT
Similarity search ﬁnds application in specialized database
systems handling complex data such as images or videos,
which are typically represented by high-dimensional features
and require speciﬁc indexing structures. This paper tackles
the problem of better utilizing GPUs for this task. While
GPUs excel at data-parallel tasks, prior approaches are bot-
tlenecked by algorithms that expose less parallelism, such as
k-min selection, or make poor use of the memory hierarchy.
We propose a design for k-selection that operates at up
to 55% of theoretical peak performance, enabling a nearest
neighbor implementation that is 8.5× faster than prior GPU
state of the art. We apply it in diﬀerent similarity search
scenarios, by proposing optimized design for brute-force, ap-
proximate and compressed-domain search based on product
quantization. In all these setups, we outperform the state of
the art by large margins. Our implementation enables the
construction of a high accuracy k-NN graph on 95 million
images from the Yfcc100M dataset in 35 minutes, and of
a graph connecting 1 billion vectors in less than 12 hours
on 4 Maxwell Titan X GPUs. We have open-sourced our
approach1 for the sake of comparison and reproducibility.
1.
INTRODUCTION
Images and videos constitute a new massive source of data
for indexing and search. Extensive metadata for this con-
tent is often not available. Search and interpretation of this
and other human-generated content, like text, is diﬃcult and
important. A variety of machine learning and deep learn-
ing algorithms are being used to interpret and classify these
complex, real-world entities. Popular examples include the
text representation known as word2vec [32], representations
of images by convolutional neural networks [39, 19], and im-
age descriptors for instance search [20]. Such representations
or embeddings are usually real-valued, high-dimensional vec-
tors of 50 to 1000+ dimensions. Many of these vector repre-
sentations can only eﬀectively be produced on GPU systems,
1https://github.com/facebookresearch/faiss
as the underlying processes either have high arithmetic com-
plexity and/or high data bandwidth demands [28], or cannot
be eﬀectively partitioned without failing due to communi-
cation overhead or representation quality [38].
Once pro-
duced, their manipulation is itself arithmetically intensive.
However, how to utilize GPU assets is not straightforward.
More generally, how to exploit new heterogeneous architec-
tures is a key subject for the database community [9].
In this context, searching by numerical similarity rather
than via structured relations is more suitable. This could be
to ﬁnd the most similar content to a picture, or to ﬁnd the
vectors that have the highest response to a linear classiﬁer
on all vectors of a collection.
One of the most expensive operations to be performed on
large collections is to compute a k-NN graph. It is a directed
graph where each vector of the database is a node and each
edge connects a node to its k nearest neighbors.
This is
our ﬂagship application. Note, state of the art methods like
NN-Descent [15] have a large memory overhead on top of
the dataset itself and cannot readily scale to the billion-sized
databases we consider.
Such applications must deal with the curse of dimension-
ality [46], rendering both exhaustive search or exact index-
ing for non-exhaustive search impractical on billion-scale
databases.
This is why there is a large body of work on
approximate search and/or graph construction. To handle
huge datasets that do not ﬁt in RAM, several approaches
employ an internal compressed representation of the vec-
tors using an encoding.
This is especially convenient for
memory-limited devices like GPUs. It turns out that accept-
ing a minimal accuracy loss results in orders of magnitude
of compression [21]. The most popular vector compression
methods can be classiﬁed into either binary codes [18, 22],
or quantization methods [25, 37]. Both have the desirable
property that searching neighbors does not require recon-
structing the vectors.
Our paper focuses on methods based on product quanti-
zation (PQ) codes, as these were shown to be more eﬀective
than binary codes [34]. In addition, binary codes incur im-
portant overheads for non-exhaustive search methods [35].
Several improvements were proposed after the original prod-
uct quantization proposal known as IVFADC [25]; most are
diﬃcult to implement eﬃciently on GPU. For instance, the
inverted multi-index [4], useful for high-speed/low-quality
operating points, depends on a complicated “multi-sequence”
algorithm. The optimized product quantization or OPQ [17]
is a linear transformation on the input vectors that improves
the accuracy of the product quantization; it can be applied
1
arXiv:1702.08734v1  [cs.CV]  28 Feb 2017

131.70700073242188 | 71.52238464355469 | 478.0074157714844 | 89.45519256591797 | Billion-scale similarity search with GPUs
 |  | 
108.18899536132812 | 117.3917236328125 | 209.02041625976562 | 149.80911254882812 | Jeff Johnson
Facebook AI Research
New York
 | 1 | 
254.4409942626953 | 117.3917236328125 | 355.2724609375 | 149.80911254882812 | Matthijs Douze
Facebook AI Research
Paris
 | 2 | 
400.6919860839844 | 117.3917236328125 | 501.5234680175781 | 149.80911254882812 | Herv´e J´egou
Facebook AI Research
Paris
 | 3 | 
53.7979736328125 | 207.01318359375 | 118.23650360107422 | 218.9683837890625 | ABSTRACT
 | 4 | 
53.79795837402344 | 223.6274871826172 | 292.9498291015625 | 441.8098449707031 | Similarity search ﬁnds application in specialized database
systems handling complex data such as images or videos,
which are typically represented by high-dimensional features
and require speciﬁc indexing structures. This paper tackles
the problem of better utilizing GPUs for this task. While
GPUs excel at data-parallel tasks, prior approaches are bot-
tlenecked by algorithms that expose less parallelism, such as
k-min selection, or make poor use of the memory hierarchy.
We propose a design for k-selection that operates at up
to 55% of theoretical peak performance, enabling a nearest
neighbor implementation that is 8.5× faster than prior GPU
state of the art. We apply it in diﬀerent similarity search
scenarios, by proposing optimized design for brute-force, ap-
proximate and compressed-domain search based on product
quantization. In all these setups, we outperform the state of
the art by large margins. Our implementation enables the
construction of a high accuracy k-NN graph on 95 million
images from the Yfcc100M dataset in 35 minutes, and of
a graph connecting 1 billion vectors in less than 12 hours
on 4 Maxwell Titan X GPUs. We have open-sourced our
approach1 for the sake of comparison and reproducibility.
 | 5 | 
53.79798126220703 | 453.6041259765625 | 292.9498596191406 | 603.2188110351562 | 1.
INTRODUCTION
Images and videos constitute a new massive source of data
for indexing and search. Extensive metadata for this con-
tent is often not available. Search and interpretation of this
and other human-generated content, like text, is diﬃcult and
important. A variety of machine learning and deep learn-
ing algorithms are being used to interpret and classify these
complex, real-world entities. Popular examples include the
text representation known as word2vec [32], representations
of images by convolutional neural networks [39, 19], and im-
age descriptors for instance search [20]. Such representations
or embeddings are usually real-valued, high-dimensional vec-
tors of 50 to 1000+ dimensions. Many of these vector repre-
sentations can only eﬀectively be produced on GPU systems,
 | 6 | 
54.255001068115234 | 609.2722778320312 | 251.40794372558594 | 620.142578125 | 1https://github.com/facebookresearch/faiss
 | 7 | 
316.81201171875 | 209.18150329589844 | 558.3938598632812 | 709.8048095703125 | as the underlying processes either have high arithmetic com-
plexity and/or high data bandwidth demands [28], or cannot
be eﬀectively partitioned without failing due to communi-
cation overhead or representation quality [38].
Once pro-
duced, their manipulation is itself arithmetically intensive.
However, how to utilize GPU assets is not straightforward.
More generally, how to exploit new heterogeneous architec-
tures is a key subject for the database community [9].
In this context, searching by numerical similarity rather
than via structured relations is more suitable. This could be
to ﬁnd the most similar content to a picture, or to ﬁnd the
vectors that have the highest response to a linear classiﬁer
on all vectors of a collection.
One of the most expensive operations to be performed on
large collections is to compute a k-NN graph. It is a directed
graph where each vector of the database is a node and each
edge connects a node to its k nearest neighbors.
This is
our ﬂagship application. Note, state of the art methods like
NN-Descent [15] have a large memory overhead on top of
the dataset itself and cannot readily scale to the billion-sized
databases we consider.
Such applications must deal with the curse of dimension-
ality [46], rendering both exhaustive search or exact index-
ing for non-exhaustive search impractical on billion-scale
databases.
This is why there is a large body of work on
approximate search and/or graph construction. To handle
huge datasets that do not ﬁt in RAM, several approaches
employ an internal compressed representation of the vec-
tors using an encoding.
This is especially convenient for
memory-limited devices like GPUs. It turns out that accept-
ing a minimal accuracy loss results in orders of magnitude
of compression [21]. The most popular vector compression
methods can be classiﬁed into either binary codes [18, 22],
or quantization methods [25, 37]. Both have the desirable
property that searching neighbors does not require recon-
structing the vectors.
Our paper focuses on methods based on product quanti-
zation (PQ) codes, as these were shown to be more eﬀective
than binary codes [34]. In addition, binary codes incur im-
portant overheads for non-exhaustive search methods [35].
Several improvements were proposed after the original prod-
uct quantization proposal known as IVFADC [25]; most are
diﬃcult to implement eﬃciently on GPU. For instance, the
inverted multi-index [4], useful for high-speed/low-quality
operating points, depends on a complicated “multi-sequence”
algorithm. The optimized product quantization or OPQ [17]
is a linear transformation on the input vectors that improves
the accuracy of the product quantization; it can be applied
 | 8 | 
302.5530090332031 | 740.1904907226562 | 307.1617431640625 | 749.1568603515625 | 1
 | 9 | 
10.940000534057617 | 211.13995361328125 | 37.619998931884766 | 560.0 | arXiv:1702.08734v1  [cs.CV]  28 Feb 2017
 | 10 | 
as a pre-processing. The SIMD-optimized IVFADC imple-
mentation from [2] operates only with sub-optimal parame-
ters (few coarse quantization centroids). Many other meth-
ods, like LOPQ and the Polysemous codes [27, 16] are too
complex to be implemented eﬃciently on GPUs.
There are many implementations of similarity search on
GPUs, but mostly with binary codes [36], small datasets [44],
or exhaustive search [14, 40, 41]. To the best of our knowl-
edge, only the work by Wieschollek et al. [47] appears suit-
able for billion-scale datasets with quantization codes. This
is the prior state of the art on GPUs, which we compare
against in Section 6.4.
This paper makes the following contributions:
• a GPU k-selection algorithm, operating in fast register
memory and ﬂexible enough to be fusable with other
kernels, for which we provide a complexity analysis;
• a near-optimal algorithmic layout for exact and ap-
proximate k-nearest neighbor search on GPU;
• a range of experiments that show that these improve-
ments outperform previous art by a large margin on
mid- to large-scale nearest-neighbor search tasks, in
single or multi-GPU conﬁgurations.
The paper is organized as follows. Section 2 introduces
the context and notation.
Section 3 reviews GPU archi-
tecture and discusses problems appearing when using it for
similarity search. Section 4 introduces one of our main con-
tributions, i.e., our k-selection method for GPUs, while Sec-
tion 5 provides details regarding the algorithm computation
layout. Finally, Section 6 provides extensive experiments for
our approach, compares it to the state of the art, and shows
concrete use cases for image collections.
2.
PROBLEM STATEMENT
We are concerned with similarity search in vector collec-
tions. Given the query vector x ∈Rd and the collection2
[yi]i=0:ℓ(yi ∈Rd), we search:
L = k-argmini=0:ℓ∥x −yi∥2,
(1)
i.e., we search the k nearest neighbors of x in terms of L2
distance. The L2 distance is used most often, as it is op-
timized by design when learning several embeddings (e.g.,
[20]), due to its attractive linear algebra properties.
The lowest distances are collected by k-selection. For an
array [ai]i=0:ℓ, k-selection ﬁnds the k lowest valued elements
[asi]i=0:k, asi ≤asi+1, along with the indices [si]i=0:k, 0 ≤
si < ℓ, of those elements from the input array. The ai will be
32-bit ﬂoating point values; the si are 32- or 64-bit integers.
Other comparators are sometimes desired; e.g., for cosine
similarity we search for highest values. The order between
equivalent keys asi = asj is not speciﬁed.
Batching.
Typically, searches are performed in batches
of nq query vectors [xj]j=0:nq (xj ∈Rd) in parallel, which
allows for more ﬂexibility when executing on multiple CPU
threads or on GPU. Batching for k-selection entails selecting
nq × k elements and indices from nq separate arrays, where
each array is of a potentially diﬀerent length ℓi ≥k.
2To avoid clutter in 0-based indexing, we use the array no-
tation 0 : ℓto denote the range {0, ..., ℓ−1} inclusive.
Exact search. The exact solution computes the full pair-
wise distance matrix D = [∥xj −yi∥2
2]j=0:nq,i=0:ℓ∈Rnq×ℓ.
In practice, we use the decomposition
∥xj −yi∥2
2 = ∥xj∥2 + ∥yi∥2 −2⟨xj, yi⟩.
(2)
The two ﬁrst terms can be precomputed in one pass over
the matrices X and Y whose rows are the [xj] and [yi]. The
bottleneck is to evaluate ⟨xj, yi⟩, equivalent to the matrix
multiplication XY ⊤. The k-nearest neighbors for each of
the nq queries are k-selected along each row of D.
Compressed-domain search. From now on, we focus on
approximate nearest-neighbor search. We consider, in par-
ticular, the IVFADC indexing structure [25]. The IVFADC
index relies on two levels of quantization, and the database
vectors are encoded. The database vector y is approximated
as:
y ≈q(y) = q1(y) + q2(y −q1(y))
(3)
where q1 : Rd →C1 ⊂Rd and q2 : Rd →C2 ⊂Rd are quan-
tizers; i.e., functions that output an element from a ﬁnite
set. Since the sets are ﬁnite, q(y) is encoded as the index of
q1(y) and that of q2(y −q1(y)). The ﬁrst-level quantizer is a
coarse quantizer and the second level ﬁne quantizer encodes
the residual vector after the ﬁrst level.
The Asymmetric Distance Computation (ADC) search
method returns an approximate result:
LADC = k-argmini=0:ℓ∥x −q(yi)∥2.
(4)
For IVFADC the search is not exhaustive.
Vectors for
which the distance is computed are pre-selected depending
on the ﬁrst-level quantizer q1:
LIVF = τ-argminc∈C1∥x −c∥2.
(5)
The multi-probe parameter τ is the number of coarse-level
centroids we consider.
The quantizer operates a nearest-
neighbor search with exact distances, in the set of reproduc-
tion values. Then, the IVFADC search computes
LIVFADC =
k-argmin
i=0:ℓs.t. q1(yi)∈LIVF
∥x −q(yi)∥2.
(6)
Hence, IVFADC relies on the same distance estimations as
the two-step quantization of ADC, but computes them only
on a subset of vectors.
The corresponding data structure, the inverted ﬁle, groups
the vectors yi into |C1| inverted lists I1, ..., I|C1| with homo-
geneous q1(yi). Therefore, the most memory-intensive op-
eration is computing LIVFADC, and boils down to linearly
scanning τ inverted lists.
The quantizers. The quantizers q1 and q2 have diﬀerent
properties. q1 needs to have a relatively low number of repro-
duction values so that the number of inverted lists does not
explode. We typically use |C1| ≈
√
ℓ, trained via k-means.
For q2, we can aﬀord to spend more memory for a more ex-
tensive representation. The ID of the vector (a 4- or 8-byte
integer) is also stored in the inverted lists, so it makes no
sense to have shorter codes than that; i.e., log2 |C2| > 4 × 8.
Product quantizer. We use a product quantizer [25] for q2,
which provides a large number of reproduction values with-
out increasing the processing cost. It interprets the vector y
as b sub-vectors y = [y0...yb−1], where b is an even divisor of
2

53.79800033569336 | 56.75346374511719 | 293.3623046875 | 180.7879180908203 | as a pre-processing. The SIMD-optimized IVFADC imple-
mentation from [2] operates only with sub-optimal parame-
ters (few coarse quantization centroids). Many other meth-
ods, like LOPQ and the Polysemous codes [27, 16] are too
complex to be implemented eﬃciently on GPUs.
There are many implementations of similarity search on
GPUs, but mostly with binary codes [36], small datasets [44],
or exhaustive search [14, 40, 41]. To the best of our knowl-
edge, only the work by Wieschollek et al. [47] appears suit-
able for billion-scale datasets with quantization codes. This
is the prior state of the art on GPUs, which we compare
against in Section 6.4.
 |  | 
62.76499938964844 | 188.3874969482422 | 247.81356811523438 | 197.35389709472656 | This paper makes the following contributions:
 | 1 | 
67.12300109863281 | 208.08726501464844 | 292.9460754394531 | 238.0918731689453 | • a GPU k-selection algorithm, operating in fast register
memory and ﬂexible enough to be fusable with other
kernels, for which we provide a complexity analysis;
 | 2 | 
67.12300109863281 | 246.8972625732422 | 292.95880126953125 | 266.43988037109375 | • a near-optimal algorithmic layout for exact and ap-
proximate k-nearest neighbor search on GPU;
 | 3 | 
67.12300109863281 | 275.24530029296875 | 292.9318542480469 | 315.7098693847656 | • a range of experiments that show that these improve-
ments outperform previous art by a large margin on
mid- to large-scale nearest-neighbor search tasks, in
single or multi-GPU conﬁgurations.
 | 4 | 
53.79800033569336 | 326.5604553222656 | 292.9587707519531 | 419.21282958984375 | The paper is organized as follows. Section 2 introduces
the context and notation.
Section 3 reviews GPU archi-
tecture and discusses problems appearing when using it for
similarity search. Section 4 introduces one of our main con-
tributions, i.e., our k-selection method for GPUs, while Sec-
tion 5 provides details regarding the algorithm computation
layout. Finally, Section 6 provides extensive experiments for
our approach, compares it to the state of the art, and shows
concrete use cases for image collections.
 | 5 | 
53.79800033569336 | 432.24212646484375 | 292.94146728515625 | 466.788818359375 | 2.
PROBLEM STATEMENT
We are concerned with similarity search in vector collec-
tions. Given the query vector x ∈Rd and the collection2
 | 6 | 
53.798004150390625 | 466.3245544433594 | 170.91567993164062 | 478.129638671875 | [yi]i=0:ℓ(yi ∈Rd), we search:
 | 7 | 
117.05500793457031 | 484.73223876953125 | 292.9078369140625 | 495.40386962890625 | L = k-argmini=0:ℓ∥x −yi∥2,
(1)
 | 8 | 
53.798004150390625 | 501.4134216308594 | 292.9272766113281 | 627.705810546875 | i.e., we search the k nearest neighbors of x in terms of L2
distance. The L2 distance is used most often, as it is op-
timized by design when learning several embeddings (e.g.,
[20]), due to its attractive linear algebra properties.
The lowest distances are collected by k-selection. For an
array [ai]i=0:ℓ, k-selection ﬁnds the k lowest valued elements
[asi]i=0:k, asi ≤asi+1, along with the indices [si]i=0:k, 0 ≤
si < ℓ, of those elements from the input array. The ai will be
32-bit ﬂoating point values; the si are 32- or 64-bit integers.
Other comparators are sometimes desired; e.g., for cosine
similarity we search for highest values. The order between
equivalent keys asi = asj is not speciﬁed.
 | 9 | 
53.79811096191406 | 633.0474243164062 | 292.9410095214844 | 695.1976318359375 | Batching.
Typically, searches are performed in batches
of nq query vectors [xj]j=0:nq (xj ∈Rd) in parallel, which
allows for more ﬂexibility when executing on multiple CPU
threads or on GPU. Batching for k-selection entails selecting
nq × k elements and indices from nq separate arrays, where
each array is of a potentially diﬀerent length ℓi ≥k.
 | 10 | 
53.798004150390625 | 699.8633422851562 | 292.9221496582031 | 719.2688598632812 | 2To avoid clutter in 0-based indexing, we use the array no-
tation 0 : ℓto denote the range {0, ..., ℓ−1} inclusive.
 | 11 | 
316.8119812011719 | 56.75346374511719 | 555.93994140625 | 86.64179992675781 | Exact search. The exact solution computes the full pair-
wise distance matrix D = [∥xj −yi∥2
2]j=0:nq,i=0:ℓ∈Rnq×ℓ.
In practice, we use the decomposition
 | 12 | 
358.8210144042969 | 99.45714569091797 | 555.9218139648438 | 114.42378234863281 | ∥xj −yi∥2
2 = ∥xj∥2 + ∥yi∥2 −2⟨xj, yi⟩.
(2)
 | 13 | 
316.81201171875 | 119.62437438964844 | 555.9375 | 171.4307403564453 | The two ﬁrst terms can be precomputed in one pass over
the matrices X and Y whose rows are the [xj] and [yi]. The
bottleneck is to evaluate ⟨xj, yi⟩, equivalent to the matrix
multiplication XY ⊤. The k-nearest neighbors for each of
the nq queries are k-selected along each row of D.
 | 14 | 
316.81207275390625 | 177.44334411621094 | 555.9459838867188 | 238.71376037597656 | Compressed-domain search. From now on, we focus on
approximate nearest-neighbor search. We consider, in par-
ticular, the IVFADC indexing structure [25]. The IVFADC
index relies on two levels of quantization, and the database
vectors are encoded. The database vector y is approximated
as:
 | 15 | 
371.705078125 | 245.3761444091797 | 555.929443359375 | 254.80177307128906 | y ≈q(y) = q1(y) + q2(y −q1(y))
(3)
 | 16 | 
316.8121032714844 | 257.3111572265625 | 555.9553833007812 | 343.4287414550781 | where q1 : Rd →C1 ⊂Rd and q2 : Rd →C2 ⊂Rd are quan-
tizers; i.e., functions that output an element from a ﬁnite
set. Since the sets are ﬁnite, q(y) is encoded as the index of
q1(y) and that of q2(y −q1(y)). The ﬁrst-level quantizer is a
coarse quantizer and the second level ﬁne quantizer encodes
the residual vector after the ﬁrst level.
The Asymmetric Distance Computation (ADC) search
method returns an approximate result:
 | 17 | 
365.99920654296875 | 350.55316162109375 | 555.9219970703125 | 361.2237854003906 | LADC = k-argmini=0:ℓ∥x −q(yi)∥2.
(4)
 | 18 | 
316.81219482421875 | 366.8763427734375 | 555.9370727539062 | 397.1077880859375 | For IVFADC the search is not exhaustive.
Vectors for
which the distance is computed are pre-selected depending
on the ﬁrst-level quantizer q1:
 | 19 | 
375.73321533203125 | 403.88916015625 | 555.9220581054688 | 415.4931335449219 | LIVF = τ-argminc∈C1∥x −c∥2.
(5)
 | 20 | 
316.812255859375 | 420.21234130859375 | 555.937255859375 | 460.56072998046875 | The multi-probe parameter τ is the number of coarse-level
centroids we consider.
The quantizer operates a nearest-
neighbor search with exact distances, in the set of reproduc-
tion values. Then, the IVFADC search computes
 | 21 | 
349.46527099609375 | 477.6831359863281 | 555.9220581054688 | 494.943115234375 | LIVFADC =
k-argmin
i=0:ℓs.t. q1(yi)∈LIVF
∥x −q(yi)∥2.
(6)
 | 22 | 
316.81219482421875 | 500.518310546875 | 555.955078125 | 582.709716796875 | Hence, IVFADC relies on the same distance estimations as
the two-step quantization of ADC, but computes them only
on a subset of vectors.
The corresponding data structure, the inverted ﬁle, groups
the vectors yi into |C1| inverted lists I1, ..., I|C1| with homo-
geneous q1(yi). Therefore, the most memory-intensive op-
eration is computing LIVFADC, and boils down to linearly
scanning τ inverted lists.
 | 23 | 
316.81219482421875 | 589.7192993164062 | 555.9371337890625 | 630.4117431640625 | The quantizers. The quantizers q1 and q2 have diﬀerent
properties. q1 needs to have a relatively low number of repro-
duction values so that the number of inverted lists does not
explode. We typically use |C1| ≈
√
 | 24 | 
316.81195068359375 | 621.1015014648438 | 555.9368286132812 | 674.03662109375 | ℓ, trained via k-means.
For q2, we can aﬀord to spend more memory for a more ex-
tensive representation. The ID of the vector (a 4- or 8-byte
integer) is also stored in the inverted lists, so it makes no
sense to have shorter codes than that; i.e., log2 |C2| > 4 × 8.
 | 25 | 
316.8119201660156 | 678.9204711914062 | 555.9370727539062 | 719.2688598632812 | Product quantizer. We use a product quantizer [25] for q2,
which provides a large number of reproduction values with-
out increasing the processing cost. It interprets the vector y
as b sub-vectors y = [y0...yb−1], where b is an even divisor of
 | 26 | 
302.55303955078125 | 740.1904907226562 | 307.1617736816406 | 749.1568603515625 | 2
 | 27 | 
the dimension d. Each sub-vector is quantized with its own
quantizer, yielding the tuple (q0(y0), ..., qb−1(yb−1)). The
sub-quantizers typically have 256 reproduction values, to ﬁt
in one byte. The quantization value of the product quantizer
is then q2(y) = q0(y0) + 256 × q1(y1) + ... + 256b−1 × qb−1,
which from a storage point of view is just the concatena-
tion of the bytes produced by each sub-quantizer. Thus, the
product quantizer generates b-byte codes with |C2| = 256b
reproduction values. The k-means dictionaries of the quan-
tizers are small and quantization is computationally cheap.
3.
GPU: OVERVIEW AND K-SELECTION
This section reviews salient details of Nvidia’s general-
purpose GPU architecture and programming model [30]. We
then focus on one of the less GPU-compliant parts involved
in similarity search, namely the k-selection, and discuss the
literature and challenges.
3.1
Architecture
GPU lanes and warps. The Nvidia GPU is a general-
purpose computer that executes instruction streams using
a 32-wide vector of CUDA threads (the warp); individual
threads in the warp are referred to as lanes, with a lane
ID from 0 – 31. Despite the “thread” terminology, the best
analogy to modern vectorized multicore CPUs is that each
warp is a separate CPU hardware thread, as the warp shares
an instruction counter. Warp lanes taking diﬀerent execu-
tion paths results in warp divergence, reducing performance.
Each lane has up to 255 32-bit registers in a shared register
ﬁle. The CPU analogy is that there are up to 255 vector
registers of width 32, with warp lanes as SIMD vector lanes.
Collections of warps. A user-conﬁgurable collection of 1
to 32 warps comprises a block or a co-operative thread ar-
ray (CTA). Each block has a high speed shared memory, up
to 48 KiB in size. Individual CUDA threads have a block-
relative ID, called a thread id, which can be used to parti-
tion and assign work. Each block is run on a single core of
the GPU called a streaming multiprocessor (SM). Each SM
has functional units, including ALUs, memory load/store
units, and various special instruction units. A GPU hides
execution latencies by having many operations in ﬂight on
warps across all SMs. Each individual warp lane instruction
throughput is low and latency is high, but the aggregate
arithmetic throughput of all SMs together is 5 – 10× higher
than typical CPUs.
Grids and kernels. Blocks are organized in a grid of blocks
in a kernel. Each block is assigned a grid relative ID. The
kernel is the unit of work (instruction stream with argu-
ments) scheduled by the host CPU for the GPU to execute.
After a block runs through to completion, new blocks can
be scheduled. Blocks from diﬀerent kernels can run concur-
rently. Ordering between kernels is controllable via ordering
primitives such as streams and events.
Resources and occupancy. The number of blocks execut-
ing concurrently depends upon shared memory and register
resources used by each block. Per-CUDA thread register us-
age is determined at compilation time, while shared memory
usage can be chosen at runtime. This usage aﬀects occu-
pancy on the GPU. If a block demands all 48 KiB of shared
memory for its private usage, or 128 registers per thread as
opposed to 32, then only 1 – 2 other blocks can run concur-
rently on the same SM, resulting in low occupancy. Under
high occupancy more blocks will be present across all SMs,
allowing more work to be in ﬂight at once.
Memory types. Diﬀerent blocks and kernels communicate
through global memory, typically 4 – 32 GB in size, with 5 –
10× higher bandwidth than CPU main memory.
Shared
memory is analogous to CPU L1 cache in terms of speed.
GPU register ﬁle memory is the highest bandwidth memory.
In order to maintain the high number of instructions in ﬂight
on a GPU, a vast register ﬁle is also required: 14 MB in the
latest Pascal P100, in contrast with a few tens of KB on
CPU. A ratio of 250 : 6.25 : 1 for register to shared to global
memory aggregate cross-sectional bandwidth is typical on
GPU, yielding 10 – 100s of TB/s for the register ﬁle [10].
3.2
GPU register ﬁle usage
Structured register data. Shared and register memory
usage involves eﬃciency tradeoﬀs; they lower occupancy but
can increase overall performance by retaining a larger work-
ing set in a faster memory. Making heavy use of register-
resident data at the expense of occupancy or instead of
shared memory is often proﬁtable [43].
As the GPU register ﬁle is very large, storing structured
data (not just temporary operands) is useful. A single lane
can use its (scalar) registers to solve a local task, but with
limited parallelism and storage.
Instead, lanes in a GPU
warp can instead exchange register data using the warp shuf-
ﬂe instruction, enabling warp-wide parallelism and storage.
Lane-stride register array. A common pattern to achieve
this is a lane-stride register array. That is, given elements
[ai]i=0:ℓ, each successive value is held in a register by neigh-
boring lanes. The array is stored in ℓ/32 registers per lane,
with ℓa multiple of 32. Lane j stores {aj, a32+j, ..., aℓ−32+j},
while register r holds {a32r, a32r+1, ..., a32r+31}.
For manipulating the [ai], the register in which ai is stored
(i.e., ⌊i/32⌋) and ℓmust be known at assembly time, while
the lane (i.e., i mod 32) can be runtime knowledge. A wide
variety of access patterns (shift, any-to-any) are provided;
we use the butterﬂy permutation [29] extensively.
3.3
k-selection on CPU versus GPU
k-selection algorithms, often for arbitrarily large ℓand
k, can be translated to a GPU, including radix selection
and bucket selection [1], probabilistic selection [33], quick-
select [14], and truncated sorts [40]. Their performance is
dominated by multiple passes over the input in global mem-
ory. Sometimes for similarity search, the input distances are
computed on-the-ﬂy or stored only in small blocks, not in
their entirety. The full, explicit array might be too large to
ﬁt into any memory, and its size could be unknown at the
start of the processing, rendering algorithms that require
multiple passes impractical. They suﬀer from other issues
as well.
Quickselect requires partitioning on a storage of
size O(ℓ), a data-dependent memory movement. This can
result in excessive memory transactions, or requiring parallel
preﬁx sums to determine write oﬀsets, with synchronization
overhead. Radix selection has no partitioning but multiple
passes are still required.
Heap parallelism. In similarity search applications, one
is usually interested only in a small number of results, k <
3

53.7979736328125 | 56.75346374511719 | 292.9319152832031 | 139.28895568847656 | the dimension d. Each sub-vector is quantized with its own
quantizer, yielding the tuple (q0(y0), ..., qb−1(yb−1)). The
sub-quantizers typically have 256 reproduction values, to ﬁt
in one byte. The quantization value of the product quantizer
is then q2(y) = q0(y0) + 256 × q1(y1) + ... + 256b−1 × qb−1,
which from a storage point of view is just the concatena-
tion of the bytes produced by each sub-quantizer. Thus, the
product quantizer generates b-byte codes with |C2| = 256b
 |  | 
53.7979736328125 | 140.4394989013672 | 292.9254150390625 | 159.8668975830078 | reproduction values. The k-means dictionaries of the quan-
tizers are small and quantization is computationally cheap.
 | 1 | 
53.79798889160156 | 174.63519287109375 | 292.94976806640625 | 240.5639190673828 | 3.
GPU: OVERVIEW AND K-SELECTION
This section reviews salient details of Nvidia’s general-
purpose GPU architecture and programming model [30]. We
then focus on one of the less GPU-compliant parts involved
in similarity search, namely the k-selection, and discuss the
literature and challenges.
 | 2 | 
53.798004150390625 | 248.856201171875 | 145.32699584960938 | 260.8114013671875 | 3.1
Architecture
 | 3 | 
53.79798889160156 | 270.9505310058594 | 292.9375305175781 | 394.9848937988281 | GPU lanes and warps. The Nvidia GPU is a general-
purpose computer that executes instruction streams using
a 32-wide vector of CUDA threads (the warp); individual
threads in the warp are referred to as lanes, with a lane
ID from 0 – 31. Despite the “thread” terminology, the best
analogy to modern vectorized multicore CPUs is that each
warp is a separate CPU hardware thread, as the warp shares
an instruction counter. Warp lanes taking diﬀerent execu-
tion paths results in warp divergence, reducing performance.
Each lane has up to 255 32-bit registers in a shared register
ﬁle. The CPU analogy is that there are up to 255 vector
registers of width 32, with warp lanes as SIMD vector lanes.
 | 4 | 
53.79795837402344 | 403.4534912109375 | 292.9408264160156 | 548.4098510742188 | Collections of warps. A user-conﬁgurable collection of 1
to 32 warps comprises a block or a co-operative thread ar-
ray (CTA). Each block has a high speed shared memory, up
to 48 KiB in size. Individual CUDA threads have a block-
relative ID, called a thread id, which can be used to parti-
tion and assign work. Each block is run on a single core of
the GPU called a streaming multiprocessor (SM). Each SM
has functional units, including ALUs, memory load/store
units, and various special instruction units. A GPU hides
execution latencies by having many operations in ﬂight on
warps across all SMs. Each individual warp lane instruction
throughput is low and latency is high, but the aggregate
arithmetic throughput of all SMs together is 5 – 10× higher
than typical CPUs.
 | 5 | 
53.797943115234375 | 556.8784790039062 | 292.9318542480469 | 639.06982421875 | Grids and kernels. Blocks are organized in a grid of blocks
in a kernel. Each block is assigned a grid relative ID. The
kernel is the unit of work (instruction stream with argu-
ments) scheduled by the host CPU for the GPU to execute.
After a block runs through to completion, new blocks can
be scheduled. Blocks from diﬀerent kernels can run concur-
rently. Ordering between kernels is controllable via ordering
primitives such as streams and events.
 | 6 | 
53.797943115234375 | 647.5385131835938 | 292.9587707519531 | 719.2688598632812 | Resources and occupancy. The number of blocks execut-
ing concurrently depends upon shared memory and register
resources used by each block. Per-CUDA thread register us-
age is determined at compilation time, while shared memory
usage can be chosen at runtime. This usage aﬀects occu-
pancy on the GPU. If a block demands all 48 KiB of shared
memory for its private usage, or 128 registers per thread as
 | 7 | 
316.81195068359375 | 56.75346374511719 | 555.9459838867188 | 97.10188293457031 | opposed to 32, then only 1 – 2 other blocks can run concur-
rently on the same SM, resulting in low occupancy. Under
high occupancy more blocks will be present across all SMs,
allowing more work to be in ﬂight at once.
 | 8 | 
316.81195068359375 | 104.90647888183594 | 555.9727783203125 | 218.4809112548828 | Memory types. Diﬀerent blocks and kernels communicate
through global memory, typically 4 – 32 GB in size, with 5 –
10× higher bandwidth than CPU main memory.
Shared
memory is analogous to CPU L1 cache in terms of speed.
GPU register ﬁle memory is the highest bandwidth memory.
In order to maintain the high number of instructions in ﬂight
on a GPU, a vast register ﬁle is also required: 14 MB in the
latest Pascal P100, in contrast with a few tens of KB on
CPU. A ratio of 250 : 6.25 : 1 for register to shared to global
memory aggregate cross-sectional bandwidth is typical on
GPU, yielding 10 – 100s of TB/s for the register ﬁle [10].
 | 9 | 
316.81195068359375 | 226.10919189453125 | 460.7047424316406 | 238.06439208984375 | 3.2
GPU register ﬁle usage
 | 10 | 
316.81195068359375 | 247.5384979248047 | 555.9727172851562 | 371.5728759765625 | Structured register data. Shared and register memory
usage involves eﬃciency tradeoﬀs; they lower occupancy but
can increase overall performance by retaining a larger work-
ing set in a faster memory. Making heavy use of register-
resident data at the expense of occupancy or instead of
shared memory is often proﬁtable [43].
As the GPU register ﬁle is very large, storing structured
data (not just temporary operands) is useful. A single lane
can use its (scalar) registers to solve a local task, but with
limited parallelism and storage.
Instead, lanes in a GPU
warp can instead exchange register data using the warp shuf-
ﬂe instruction, enabling warp-wide parallelism and storage.
 | 11 | 
316.8118896484375 | 379.3774719238281 | 555.938720703125 | 492.95184326171875 | Lane-stride register array. A common pattern to achieve
this is a lane-stride register array. That is, given elements
[ai]i=0:ℓ, each successive value is held in a register by neigh-
boring lanes. The array is stored in ℓ/32 registers per lane,
with ℓa multiple of 32. Lane j stores {aj, a32+j, ..., aℓ−32+j},
while register r holds {a32r, a32r+1, ..., a32r+31}.
For manipulating the [ai], the register in which ai is stored
(i.e., ⌊i/32⌋) and ℓmust be known at assembly time, while
the lane (i.e., i mod 32) can be runtime knowledge. A wide
variety of access patterns (shift, any-to-any) are provided;
we use the butterﬂy permutation [29] extensively.
 | 12 | 
316.8118896484375 | 500.58013916015625 | 555.9727783203125 | 692.037841796875 | 3.3
k-selection on CPU versus GPU
k-selection algorithms, often for arbitrarily large ℓand
k, can be translated to a GPU, including radix selection
and bucket selection [1], probabilistic selection [33], quick-
select [14], and truncated sorts [40]. Their performance is
dominated by multiple passes over the input in global mem-
ory. Sometimes for similarity search, the input distances are
computed on-the-ﬂy or stored only in small blocks, not in
their entirety. The full, explicit array might be too large to
ﬁt into any memory, and its size could be unknown at the
start of the processing, rendering algorithms that require
multiple passes impractical. They suﬀer from other issues
as well.
Quickselect requires partitioning on a storage of
size O(ℓ), a data-dependent memory movement. This can
result in excessive memory transactions, or requiring parallel
preﬁx sums to determine write oﬀsets, with synchronization
overhead. Radix selection has no partitioning but multiple
passes are still required.
 | 13 | 
316.81195068359375 | 699.8424682617188 | 555.9523315429688 | 719.2698364257812 | Heap parallelism. In similarity search applications, one
is usually interested only in a small number of results, k <
 | 14 | 
302.552978515625 | 740.1914672851562 | 307.1617126464844 | 749.1578369140625 | 3
 | 15 | 
1000 or so. In this regime, selection via max-heap is a typi-
cal choice on the CPU, but heaps do not expose much data
parallelism (due to serial tree update) and cannot saturate
SIMD execution units. The ad-heap [31] takes better advan-
tage of parallelism available in heterogeneous systems, but
still attempts to partition serial and parallel work between
appropriate execution units.
Despite the serial nature of
heap update, for small k the CPU can maintain all of its
state in the L1 cache with little eﬀort, and L1 cache latency
and bandwidth remains a limiting factor. Other similarity
search components, like PQ code manipulation, tend to have
greater impact on CPU performance [2].
GPU heaps.
Heaps can be similarly implemented on a
GPU [7]. However, a straightforward GPU heap implemen-
tation suﬀers from high warp divergence and irregular, data-
dependent memory movement, since the path taken for each
inserted element depends upon other values in the heap.
GPU parallel priority queues [24] improve over the serial
heap update by allowing multiple concurrent updates, but
they require a potential number of small sorts for each insert
and data-dependent memory movement. Moreover, it uses
multiple synchronization barriers through kernel launches in
diﬀerent streams, plus the additional latency of successive
kernel launches and coordination with the CPU host.
Other more novel GPU algorithms are available for small
k, namely the selection algorithm in the fgknn library [41].
This is a complex algorithm that may suﬀer from too many
synchronization points, greater kernel launch overhead, us-
age of slower memories, excessive use of hierarchy, partition-
ing and buﬀering. However, we take inspiration from this
particular algorithm through the use of parallel merges as
seen in their merge queue structure.
4.
FAST K-SELECTION ON THE GPU
For any CPU or GPU algorithm, either memory or arith-
metic throughput should be the limiting factor as per the
rooﬂine performance model [48]. For input from global mem-
ory, k-selection cannot run faster than the time required to
scan the input once at peak memory bandwidth. We aim to
get as close to this limit as possible. Thus, we wish to per-
form a single pass over the input data (from global memory
or produced on-the-ﬂy, perhaps fused with a kernel that is
generating the data).
We want to keep intermediate state in the fastest memory:
the register ﬁle. The major disadvantage of register memory
is that the indexing into the register ﬁle must be known at
assembly time, which is a strong constraint on the algorithm.
4.1
In-register sorting
We use an in-register sorting primitive as a building block.
Sorting networks are commonly used on SIMD architec-
tures [13], as they exploit vector parallelism. They are eas-
ily implemented on the GPU, and we build sorting networks
with lane-stride register arrays.
We use a variant of Batcher’s bitonic sorting network [8],
which is a set of parallel merges on an array of size 2k. Each
merge takes s arrays of length t (s and t a power of 2) to s/2
arrays of length 2t, using log2(t) parallel steps. A bitonic
sort applies this merge recursively: to sort an array of length
ℓ, merge ℓarrays of length 1 to ℓ/2 arrays of length 2, to ℓ/4
arrays of length 4, successively to 1 sorted array of length ℓ,
leading to 1
2(log2(ℓ)2 + log2(ℓ)) parallel merge steps.
Algorithm 1 Odd-size merging network
function merge-odd([Li]i=0:ℓL, [Ri]i=0:ℓR)
parallel for i ←0 : min(ℓL, ℓR) do
▷inverted 1st stage; inputs are already sorted
compare-swap(LℓL−i−1, Ri)
end for
parallel do
▷If ℓL = ℓR and a power-of-2, these are equivalent
merge-odd-continue([Li]i=0:ℓL, left)
merge-odd-continue([Ri]i=0:ℓR, right)
end do
end function
function merge-odd-continue([xi]i=0:ℓ, p)
if ℓ> 1 then
h ←2⌈log2 ℓ⌉−1
▷largest power-of-2 < ℓ
parallel for i ←0 : ℓ−h do
▷Implemented with warp shuﬄe butterﬂy
compare-swap(xi, xi+h)
end for
parallel do
if p = left then
▷left side recursion
merge-odd-continue([xi]i=0:ℓ−h, left)
merge-odd-continue([xi]i=ℓ−h:ℓ, right)
else
▷right side recursion
merge-odd-continue([xi]i=0:h, left)
merge-odd-continue([xi]i=h:ℓ, right)
end if
end do
end if
end function
Odd-size merging and sorting networks. If some input
data is already sorted, we can modify the network to avoid
merging steps. We may also not have a full power-of-2 set of
data, in which case we can eﬃciently shortcut to deal with
the smaller size.
Algorithm 1 is an odd-sized merging network that merges
already sorted left and right arrays, each of arbitrary length.
While the bitonic network merges bitonic sequences, we start
with monotonic sequences: sequences sorted monotonically.
A bitonic merge is made monotonic by reversing the ﬁrst
comparator stage.
The odd size algorithm is derived by considering arrays to
be padded to the next highest power-of-2 size with dummy
1
3
4
8
9
0
3
7
1
3
4
3
0
9
8
7
0
3
4
3
1
7
8
9
0
3
1
3
4
7
8
9
0
1
3
3
4
7
8
9
step 1
step 2
step 3
step 4
Figure 1:
Odd-size network merging arrays of sizes
5 and 3.
Bullets indicate parallel compare/swap.
Dashed lines are elided elements or comparisons.
4

53.79800033569336 | 56.75346374511719 | 292.9677734375 | 180.7879180908203 | 1000 or so. In this regime, selection via max-heap is a typi-
cal choice on the CPU, but heaps do not expose much data
parallelism (due to serial tree update) and cannot saturate
SIMD execution units. The ad-heap [31] takes better advan-
tage of parallelism available in heterogeneous systems, but
still attempts to partition serial and parallel work between
appropriate execution units.
Despite the serial nature of
heap update, for small k the CPU can maintain all of its
state in the L1 cache with little eﬀort, and L1 cache latency
and bandwidth remains a limiting factor. Other similarity
search components, like PQ code manipulation, tend to have
greater impact on CPU performance [2].
 |  | 
53.79801940917969 | 189.2565155029297 | 292.94989013671875 | 396.9779052734375 | GPU heaps.
Heaps can be similarly implemented on a
GPU [7]. However, a straightforward GPU heap implemen-
tation suﬀers from high warp divergence and irregular, data-
dependent memory movement, since the path taken for each
inserted element depends upon other values in the heap.
GPU parallel priority queues [24] improve over the serial
heap update by allowing multiple concurrent updates, but
they require a potential number of small sorts for each insert
and data-dependent memory movement. Moreover, it uses
multiple synchronization barriers through kernel launches in
diﬀerent streams, plus the additional latency of successive
kernel launches and coordination with the CPU host.
Other more novel GPU algorithms are available for small
k, namely the selection algorithm in the fgknn library [41].
This is a complex algorithm that may suﬀer from too many
synchronization points, greater kernel launch overhead, us-
age of slower memories, excessive use of hierarchy, partition-
ing and buﬀering. However, we take inspiration from this
particular algorithm through the use of parallel merges as
seen in their merge queue structure.
 | 1 | 
53.79801559448242 | 411.7462158203125 | 292.95880126953125 | 561.3618774414062 | 4.
FAST K-SELECTION ON THE GPU
For any CPU or GPU algorithm, either memory or arith-
metic throughput should be the limiting factor as per the
rooﬂine performance model [48]. For input from global mem-
ory, k-selection cannot run faster than the time required to
scan the input once at peak memory bandwidth. We aim to
get as close to this limit as possible. Thus, we wish to per-
form a single pass over the input data (from global memory
or produced on-the-ﬂy, perhaps fused with a kernel that is
generating the data).
We want to keep intermediate state in the fastest memory:
the register ﬁle. The major disadvantage of register memory
is that the indexing into the register ﬁle must be known at
assembly time, which is a strong constraint on the algorithm.
 | 2 | 
53.7979736328125 | 569.6541748046875 | 292.9415283203125 | 719.2688598632812 | 4.1
In-register sorting
We use an in-register sorting primitive as a building block.
Sorting networks are commonly used on SIMD architec-
tures [13], as they exploit vector parallelism. They are eas-
ily implemented on the GPU, and we build sorting networks
with lane-stride register arrays.
We use a variant of Batcher’s bitonic sorting network [8],
which is a set of parallel merges on an array of size 2k. Each
merge takes s arrays of length t (s and t a power of 2) to s/2
arrays of length 2t, using log2(t) parallel steps. A bitonic
sort applies this merge recursively: to sort an array of length
ℓ, merge ℓarrays of length 1 to ℓ/2 arrays of length 2, to ℓ/4
arrays of length 4, successively to 1 sorted array of length ℓ,
leading to 1
 | 3 | 
98.00199890136719 | 706.4934692382812 | 265.1570739746094 | 721.658935546875 | 2(log2(ℓ)2 + log2(ℓ)) parallel merge steps.
 | 4 | 
316.81201171875 | 55.80650329589844 | 481.7943420410156 | 64.77290344238281 | Algorithm 1 Odd-size merging network
 | 5 | 
326.0270080566406 | 66.69961547851562 | 502.2305908203125 | 79.1622314453125 | function merge-odd([Li]i=0:ℓL, [Ri]i=0:ℓR)
 | 6 | 
339.85003662109375 | 79.00316619873047 | 484.0121154785156 | 88.42877960205078 | parallel for i ←0 : min(ℓL, ℓR) do
 | 7 | 
339.8500671386719 | 89.58036804199219 | 555.9210205078125 | 129.9287872314453 | ▷inverted 1st stage; inputs are already sorted
compare-swap(LℓL−i−1, Ri)
end for
parallel do
 | 8 | 
326.027099609375 | 131.4233856201172 | 555.9219360351562 | 193.10279846191406 | ▷If ℓL = ℓR and a power-of-2, these are equivalent
merge-odd-continue([Li]i=0:ℓL, left)
merge-odd-continue([Ri]i=0:ℓR, right)
end do
end function
function merge-odd-continue([xi]i=0:ℓ, p)
 | 9 | 
339.85009765625 | 194.18833923339844 | 394.6732177734375 | 203.1547393798828 | if ℓ> 1 then
 | 10 | 
353.67303466796875 | 203.09771728515625 | 555.9044189453125 | 224.0757598876953 | h ←2⌈log2 ℓ⌉−1
▷largest power-of-2 < ℓ
parallel for i ←0 : ℓ−h do
 | 11 | 
353.67303466796875 | 225.5703582763672 | 555.9264526367188 | 265.91876220703125 | ▷Implemented with warp shuﬄe butterﬂy
compare-swap(xi, xi+h)
end for
parallel do
 | 12 | 
326.02703857421875 | 267.4133605957031 | 555.9217529296875 | 370.5267333984375 | if p = left then
▷left side recursion
merge-odd-continue([xi]i=0:ℓ−h, left)
merge-odd-continue([xi]i=ℓ−h:ℓ, right)
else
▷right side recursion
merge-odd-continue([xi]i=0:h, left)
merge-odd-continue([xi]i=h:ℓ, right)
end if
end do
end if
end function
 | 13 | 
316.81201171875 | 398.095458984375 | 555.954833984375 | 532.5908203125 | Odd-size merging and sorting networks. If some input
data is already sorted, we can modify the network to avoid
merging steps. We may also not have a full power-of-2 set of
data, in which case we can eﬃciently shortcut to deal with
the smaller size.
Algorithm 1 is an odd-sized merging network that merges
already sorted left and right arrays, each of arbitrary length.
While the bitonic network merges bitonic sequences, we start
with monotonic sequences: sequences sorted monotonically.
A bitonic merge is made monotonic by reversing the ﬁrst
comparator stage.
The odd size algorithm is derived by considering arrays to
be padded to the next highest power-of-2 size with dummy
 | 14 | 
407.9740295410156 | 554.7133178710938 | 524.1863403320312 | 566.1434936523438 | 1
3
4
8
9
0
3
7
 | 15 | 
408.2294616699219 | 582.5543823242188 | 524.4417724609375 | 593.9845581054688 | 1
3
4
3
0
9
8
7
 | 16 | 
407.7186279296875 | 609.3226928710938 | 523.930908203125 | 620.7528686523438 | 0
3
4
3
1
7
8
9
 | 17 | 
408.2294921875 | 635.273681640625 | 524.4417724609375 | 646.703857421875 | 0
3
1
3
4
7
8
9
 | 18 | 
407.97406005859375 | 653.40869140625 | 524.1863403320312 | 664.8388671875 | 0
1
3
3
4
7
8
9
 | 19 | 
329.0483703613281 | 572.4443359375 | 348.5514221191406 | 579.59619140625 | step 1
 | 20 | 
328.53753662109375 | 598.7018432617188 | 348.0262756347656 | 605.8536987304688 | step 2
 | 21 | 
328.53753662109375 | 625.112548828125 | 348.0262756347656 | 632.264404296875 | step 3
 | 22 | 
328.53753662109375 | 647.0789184570312 | 348.0262756347656 | 654.2307739257812 | step 4
 | 23 | 
316.81201171875 | 687.6375122070312 | 555.9459228515625 | 717.52587890625 | Figure 1:
Odd-size network merging arrays of sizes
5 and 3.
Bullets indicate parallel compare/swap.
Dashed lines are elided elements or comparisons.
 | 24 | 
302.5530090332031 | 740.1904907226562 | 307.1617431640625 | 749.1568603515625 | 4
 | 25 | 
input 
insertion
thread queue
merging  network
warp queue
lane 0
lane 1
lane 31
coalesced 
read
. . . . .
. . . . .
. . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . . . . . 
. . . . . . . . . 
. . . . . . . . . . . .
Figure 2:
Overview of WarpSelect. The input val-
ues stream in on the left, and the warp queue on the
right holds the output result.
elements that are never swapped (the merge is monotonic)
and are already properly positioned; any comparisons with
dummy elements are elided. A left array is considered to
be padded with dummy elements at the start; a right ar-
ray has them at the end.
A merge of two sorted arrays
of length ℓL and ℓR to a sorted array of ℓL + ℓR requires
⌈log2(max(ℓL, ℓR))⌉+1 parallel steps. Figure 1 shows Algo-
rithm 1’s merging network for arrays of size 5 and 3, with 4
parallel steps.
The compare-swap is implemented using warp shuﬄes on
a lane-stride register array. Swaps with a stride a multiple
of 32 occur directly within a lane as the lane holds both
elements locally. Swaps of stride ≤16 or a non-multiple of
32 occur with warp shuﬄes. In practice, used array lengths
are multiples of 32 as they are held in lane-stride arrays.
Algorithm 2 Odd-size sorting network
function sort-odd([xi]i=0:ℓ)
if ℓ> 1 then
parallel do
sort-odd([xi]i=0:⌊ℓ/2⌋)
sort-odd([xi]i=⌊ℓ/2⌋:ℓ)
end do
merge-odd([xi]i=0:⌊ℓ/2⌋, [xi]i=⌊ℓ/2⌋:ℓ)
end if
end function
Algorithm 2 extends the merge to a full sort. Assuming no
structure present in the input data, 1
2(⌈log2(ℓ)⌉2+⌈log2(ℓ)⌉)
parallel steps are required for sorting data of length ℓ.
4.2
WarpSelect
Our k-selection implementation, WarpSelect, maintains
state entirely in registers, requires only a single pass over
data and avoids cross-warp synchronization. It uses merge-
odd and sort-odd as primitives. Since the register ﬁle pro-
vides much more storage than shared memory, it supports
k ≤1024. Each warp is dedicated to k-selection to a single
one of the n arrays [ai]. If n is large enough, a single warp
per each [ai] will result in full GPU occupancy. Large ℓper
warp is handled by recursive decomposition, if ℓis known in
advance.
Overview. Our approach (Algorithm 3 and Figure 2) oper-
ates on values, with associated indices carried along (omit-
ted from the description for simplicity). It selects the k least
values that come from global memory, or from intermediate
value registers if fused into another kernel providing the val-
ues. Let [ai]i=0:ℓbe the sequence provided for selection.
The elements (on the left of Figure 2) are processed in
groups of 32, the warp size. Lane j is responsible for pro-
cessing {aj, a32+j, ...}; thus, if the elements come from global
memory, the reads are contiguous and coalesced into a min-
imal number of memory transactions.
Data structures.
Each lane j maintains a small queue
of t elements in registers, called the thread queues [T j
i ]i=0:t,
ordered from largest to smallest (T j
i ≥T j
i+1). The choice of
t is made relative to k, see Section 4.3. The thread queue is
a ﬁrst-level ﬁlter for new values coming in. If a new a32i+j
is greater than the largest key currently in the queue, T j
0 , it
is guaranteed that it won’t be in the k smallest ﬁnal results.
The warp shares a lane-stride register array of k smallest
seen elements, [Wi]i=0:k, called the warp queue. It is ordered
from smallest to largest (Wi ≤Wi+1); if the requested k is
not a multiple of 32, we round it up. This is a second level
data structure that will be used to maintain all of the k
smallest warp-wide seen values. The thread and warp queues
are initialized to maximum sentinel values, e.g., +∞.
Update. The three invariants maintained are:
• all per-lane T j
0 are not in the min-k
• all per-lane T j
0 are greater than all warp queue keys
Wi
• all ai seen so far in the min-k are contained in either
some lane’s thread queue ([T j
i ]i=0:t,j=0:32), or in the
warp queue.
Lane j receives a new a32i+j and attempts to insert it into
its thread queue. If a32i+j > T j
0 , then the new pair is by
deﬁnition not in the k minimum, and can be rejected.
Otherwise, it is inserted into its proper sorted position
in the thread queue, thus ejecting the old T j
0 .
All lanes
complete doing this with their new received pair and their
thread queue, but it is now possible that the second invariant
have been violated. Using the warp ballot instruction, we
determine if any lane has violated the second invariant. If
not, we are free to continue processing new elements.
Restoring the invariants. If any lane has its invariant
violated, then the warp uses odd-merge to merge and sort
the thread and warp queues together. The new warp queue
Algorithm 3 WarpSelect pseudocode for lane j
function WarpSelect(a)
if a < T j
0 then
insert a into our [T j
i ]i=0:t
end if
if warp-ballot(T j
0 < Wk−1) then
▷Reinterpret thread queues as lane-stride array
[αi]i=0:32t ←cast([T j
i ]i=0:t,j=0:32)
▷concatenate and sort thread queues
sort-odd([αi]i=0:32t)
merge-odd([Wi]i=0:k, [αi]i=0:32t)
▷Reinterpret lane-stride array as thread queues
[T j
i ]i=0:t,j=0:32 ←cast([αi]i=0:32t)
reverse-array([Ti]i=0:t)
▷Back in thread queue order, invariant restored
end if
end function
5

53.43732452392578 | 53.51304626464844 | 67.1303482055664 | 59.10888671875 | input 
 |  | 
106.28330993652344 | 73.34776306152344 | 127.45796966552734 | 78.943603515625 | insertion
 | 1 | 
134.27960205078125 | 53.51304626464844 | 167.2726593017578 | 59.10888671875 | thread queue
 | 2 | 
191.01925659179688 | 77.02704620361328 | 196.93455505371094 | 119.98760986328125 | merging  network
 | 3 | 
225.19003295898438 | 53.51304626464844 | 254.43948364257812 | 59.10888671875 | warp queue
 | 4 | 
274.633056640625 | 66.94306182861328 | 289.87054443359375 | 72.53890228271484 | lane 0
 | 5 | 
275.4806823730469 | 80.37307739257812 | 290.7181701660156 | 85.96891784667969 | lane 1
 | 6 | 
275.65374755859375 | 126.38331604003906 | 294.00250244140625 | 131.97915649414062 | lane 31
 | 7 | 
70.75787353515625 | 96.19961547851562 | 97.52278137207031 | 108.7902603149414 | coalesced 
read
 | 8 | 
143.89710998535156 | 123.49549865722656 | 157.0431365966797 | 130.89532470703125 | . . . . .
 | 9 | 
142.40489196777344 | 77.85832214355469 | 155.5509033203125 | 85.25814819335938 | . . . . .
 | 10 | 
141.4100799560547 | 63.806541442871094 | 154.5561065673828 | 71.20636749267578 | . . . . .
 | 11 | 
217.8843536376953 | 89.58125305175781 | 225.3960418701172 | 119.89083099365234 | . . . . . . . . . . .
 | 12 | 
231.81178283691406 | 74.53466033935547 | 239.42921447753906 | 133.45370483398438 | . . . . . . . . . . . . . . . . . . . .  
 | 13 | 
257.4282531738281 | 66.57613372802734 | 265.0391845703125 | 123.72663116455078 | . . . . . . . . . . . . . . . . . . . . 
 | 14 | 
57.76166534423828 | 67.22440338134766 | 65.4847640991211 | 142.724853515625 | . . . . . . . . . 
. . . . . . . . . . . .
 | 15 | 
53.79800033569336 | 161.8774871826172 | 292.9140319824219 | 191.76490783691406 | Figure 2:
Overview of WarpSelect. The input val-
ues stream in on the left, and the warp queue on the
right holds the output result.
 | 16 | 
53.79795837402344 | 210.27549743652344 | 292.94085693359375 | 365.69287109375 | elements that are never swapped (the merge is monotonic)
and are already properly positioned; any comparisons with
dummy elements are elided. A left array is considered to
be padded with dummy elements at the start; a right ar-
ray has them at the end.
A merge of two sorted arrays
of length ℓL and ℓR to a sorted array of ℓL + ℓR requires
⌈log2(max(ℓL, ℓR))⌉+1 parallel steps. Figure 1 shows Algo-
rithm 1’s merging network for arrays of size 5 and 3, with 4
parallel steps.
The compare-swap is implemented using warp shuﬄes on
a lane-stride register array. Swaps with a stride a multiple
of 32 occur directly within a lane as the lane holds both
elements locally. Swaps of stride ≤16 or a non-multiple of
32 occur with warp shuﬄes. In practice, used array lengths
are multiples of 32 as they are held in lane-stride arrays.
 | 17 | 
53.79800033569336 | 377.36346435546875 | 214.2164306640625 | 386.3298645019531 | Algorithm 2 Odd-size sorting network
 | 18 | 
63.013999938964844 | 388.256591796875 | 182.57655334472656 | 399.5918884277344 | function sort-odd([xi]i=0:ℓ)
 | 19 | 
76.83699035644531 | 400.67645263671875 | 131.6591339111328 | 409.6428527832031 | if ℓ> 1 then
 | 20 | 
90.65998840332031 | 411.137451171875 | 139.68826293945312 | 420.1038513183594 | parallel do
 | 21 | 
63.01397705078125 | 419.6385803222656 | 240.19754028320312 | 482.8678283691406 | sort-odd([xi]i=0:⌊ℓ/2⌋)
sort-odd([xi]i=⌊ℓ/2⌋:ℓ)
end do
merge-odd([xi]i=0:⌊ℓ/2⌋, [xi]i=⌊ℓ/2⌋:ℓ)
end if
end function
 | 22 | 
53.79800033569336 | 496.7314758300781 | 292.9414978027344 | 516.1578369140625 | Algorithm 2 extends the merge to a full sort. Assuming no
structure present in the input data, 1
 | 23 | 
53.79795837402344 | 505.2663269042969 | 292.9255065917969 | 651.6078491210938 | 2(⌈log2(ℓ)⌉2+⌈log2(ℓ)⌉)
parallel steps are required for sorting data of length ℓ.
4.2
WarpSelect
Our k-selection implementation, WarpSelect, maintains
state entirely in registers, requires only a single pass over
data and avoids cross-warp synchronization. It uses merge-
odd and sort-odd as primitives. Since the register ﬁle pro-
vides much more storage than shared memory, it supports
k ≤1024. Each warp is dedicated to k-selection to a single
one of the n arrays [ai]. If n is large enough, a single warp
per each [ai] will result in full GPU occupancy. Large ℓper
warp is handled by recursive decomposition, if ℓis known in
advance.
 | 24 | 
53.79801940917969 | 657.9984741210938 | 292.9588928222656 | 719.6788940429688 | Overview. Our approach (Algorithm 3 and Figure 2) oper-
ates on values, with associated indices carried along (omit-
ted from the description for simplicity). It selects the k least
values that come from global memory, or from intermediate
value registers if fused into another kernel providing the val-
ues. Let [ai]i=0:ℓbe the sequence provided for selection.
 | 25 | 
316.81201171875 | 56.75346374511719 | 555.9548950195312 | 107.56288146972656 | The elements (on the left of Figure 2) are processed in
groups of 32, the warp size. Lane j is responsible for pro-
cessing {aj, a32+j, ...}; thus, if the elements come from global
memory, the reads are contiguous and coalesced into a min-
imal number of memory transactions.
 | 26 | 
316.81201171875 | 114.99845886230469 | 555.955078125 | 261.08795166015625 | Data structures.
Each lane j maintains a small queue
of t elements in registers, called the thread queues [T j
i ]i=0:t,
ordered from largest to smallest (T j
i ≥T j
i+1). The choice of
t is made relative to k, see Section 4.3. The thread queue is
a ﬁrst-level ﬁlter for new values coming in. If a new a32i+j
is greater than the largest key currently in the queue, T j
0 , it
is guaranteed that it won’t be in the k smallest ﬁnal results.
The warp shares a lane-stride register array of k smallest
seen elements, [Wi]i=0:k, called the warp queue. It is ordered
from smallest to largest (Wi ≤Wi+1); if the requested k is
not a multiple of 32, we round it up. This is a second level
data structure that will be used to maintain all of the k
smallest warp-wide seen values. The thread and warp queues
are initialized to maximum sentinel values, e.g., +∞.
 | 27 | 
316.812255859375 | 268.5225524902344 | 505.8341369628906 | 277.48895263671875 | Update. The three invariants maintained are:
 | 28 | 
330.1372375488281 | 286.14239501953125 | 482.8862609863281 | 299.30096435546875 | • all per-lane T j
0 are not in the min-k
 | 29 | 
330.1372375488281 | 304.7633972167969 | 555.906005859375 | 326.448974609375 | • all per-lane T j
0 are greater than all warp queue keys
Wi
 | 30 | 
330.13726806640625 | 334.620361328125 | 555.9341430664062 | 364.6239318847656 | • all ai seen so far in the min-k are contained in either
some lane’s thread queue ([T j
i ]i=0:t,j=0:32), or in the
warp queue.
 | 31 | 
316.812255859375 | 375.1925354003906 | 555.9552001953125 | 478.3058776855469 | Lane j receives a new a32i+j and attempts to insert it into
its thread queue. If a32i+j > T j
0 , then the new pair is by
deﬁnition not in the k minimum, and can be rejected.
Otherwise, it is inserted into its proper sorted position
in the thread queue, thus ejecting the old T j
0 .
All lanes
complete doing this with their new received pair and their
thread queue, but it is now possible that the second invariant
have been violated. Using the warp ballot instruction, we
determine if any lane has violated the second invariant. If
not, we are free to continue processing new elements.
 | 32 | 
316.81231689453125 | 485.740478515625 | 555.9299926757812 | 515.62890625 | Restoring the invariants. If any lane has its invariant
violated, then the warp uses odd-merge to merge and sort
the thread and warp queues together. The new warp queue
 | 33 | 
316.81201171875 | 533.321533203125 | 521.5288696289062 | 544.246826171875 | Algorithm 3 WarpSelect pseudocode for lane j
 | 34 | 
326.0270080566406 | 546.173583984375 | 434.62255859375 | 557.098876953125 | function WarpSelect(a)
 | 35 | 
339.8500061035156 | 556.6782836914062 | 555.9169921875 | 652.6798706054688 | if a < T j
0 then
insert a into our [T j
i ]i=0:t
end if
if warp-ballot(T j
0 < Wk−1) then
▷Reinterpret thread queues as lane-stride array
[αi]i=0:32t ←cast([T j
i ]i=0:t,j=0:32)
▷concatenate and sort thread queues
sort-odd([αi]i=0:32t)
merge-odd([Wi]i=0:k, [αi]i=0:32t)
 | 36 | 
353.673095703125 | 653.7644653320312 | 555.9169921875 | 683.995849609375 | ▷Reinterpret lane-stride array as thread queues
[T j
i ]i=0:t,j=0:32 ←cast([αi]i=0:32t)
reverse-array([Ti]i=0:t)
 | 37 | 
326.0271301269531 | 685.1474609375 | 555.920654296875 | 715.0348510742188 | ▷Back in thread queue order, invariant restored
end if
end function
 | 38 | 
302.5530090332031 | 740.1904907226562 | 307.1617431640625 | 749.1568603515625 | 5
 | 39 | 
will be the min-k elements across the merged, sorted queues,
and the new thread queues will be the remainder, from min-
(k +1) to min-(k +32t+1). This restores the invariants and
we are free to continue processing subsequent elements.
Since the thread and warp queues are already sorted, we
merge the sorted warp queue of length k with 32 sorted
arrays of length t. Supporting odd-sized merges is important
because Batcher’s formulation would require that 32t = k
and is a power-of-2; thus if k = 1024, t must be 32. We
found that the optimal t is way smaller (see below).
Using odd-merge to merge the 32 already sorted thread
queues would require a struct-of-arrays to array-of-structs
transposition in registers across the warp, since the t succes-
sive sorted values are held in diﬀerent registers in the same
lane rather than a lane-stride array. This is possible [12],
but would use a comparable number of warp shuﬄes, so we
just reinterpret the thread queue registers as an (unsorted)
lane-stride array and sort from scratch. Signiﬁcant speedup
is realizable by using odd-merge for the merge of the ag-
gregate sorted thread queues with the warp queue.
Handling the remainder. If there are remainder elements
because ℓis not a multiple of 32, those are inserted into the
thread queues for the lanes that have them, after which we
proceed to the output stage.
Output. A ﬁnal sort and merge is made of the thread and
warp queues, after which the warp queue holds all min-k
values.
4.3
Complexity and parameter selection
For each incoming group of 32 elements, WarpSelect
can perform 1, 2 or 3 constant-time operations, all happen-
ing in warp-wide parallel time:
1. read 32 elements, compare to all thread queue heads
T j
0 , cost C1, happens N1 times;
2. if ∃j ∈{0, ..., 31}, a32n+j < T j
0 , perform insertion sort
on those speciﬁc thread queues, cost C2 = O(t), hap-
pens N2 times;
3. if ∃j, T j
0 < Wk−1, sort and merge queues, cost C3 =
O(t log(32t)2 + k log(max(k, 32t))), happens N3 times.
Thus, the total cost is N1C1 + N2C2 + N3C3. N1 = ℓ/32,
and on random data drawn independently, N2 = O(k log(ℓ))
and N3 = O(k log(ℓ)/t), see the Appendix for a full deriva-
tion. Hence, the trade-oﬀis to balance a cost in N2C2 and
one in N3C3. The practical choice for t given k and ℓwas
made by experiment on a variety of k-NN data. For k ≤32,
we use t = 2, k ≤128 uses t = 3, k ≤256 uses t = 4, and
k ≤1024 uses t = 8, all irrespective of ℓ.
5.
COMPUTATION LAYOUT
This section explains how IVFADC, one of the indexing
methods originally built upon product quantization [25], is
implemented eﬃciently. Details on distance computations
and articulation with k-selection are the key to understand-
ing why this method can outperform more recent GPU-
compliant approximate nearest neighbor strategies [47].
5.1
Exact search
We brieﬂy come back to the exhaustive search method,
often referred to as exact brute-force. It is interesting on its
own for exact nearest neighbor search in small datasets. It
is also a component of many indexes in the literature. In
our case, we use it for the IVFADC coarse quantizer q1.
As stated in Section 2, the distance computation boils
down to a matrix multiplication. We use optimized GEMM
routines in the cuBLAS library to calculate the −2⟨xj, yi⟩
term for L2 distance, resulting in a partial distance matrix
D′. To complete the distance calculation, we use a fused
k-selection kernel that adds the ∥yi∥2 term to each entry of
the distance matrix and immediately submits the value to
k-selection in registers. The ∥xj∥2 term need not be taken
into account before k-selection. Kernel fusion thus allows
for only 2 passes (GEMM write, k-select read) over D′, com-
pared to other implementations that may require 3 or more.
Row-wise k-selection is likely not fusable with a well-tuned
GEMM kernel, or would result in lower overall eﬃciency.
As D′ does not ﬁt in GPU memory for realistic problem
sizes, the problem is tiled over the batch of queries, with
tq ≤nq queries being run in a single tile. Each of the ⌈nq/tq⌉
tiles are independent problems, but we run two in parallel
on diﬀerent streams to better occupy the GPU, so the eﬀec-
tive memory requirement of D is O(2ℓtq). The computation
can similarly be tiled over ℓ. For very large input coming
from the CPU, we support buﬀering with pinned memory
to overlap CPU to GPU copy with GPU compute.
5.2
IVFADC indexing
PQ lookup tables. At its core, the IVFADC requires com-
puting the distance from a vector to a set of product quanti-
zation reproduction values. By developing Equation (6) for
a database vector y, we obtain:
∥x −q(y)∥2
2 = ∥x −q1(y) −q2(y −q1(y))∥2
2.
(7)
If we decompose the residual vectors left after q1 as:
y −q1(y)
=
[ ey1 · · · eyb] and
(8)
x −q1(y)
=
[f
x1 · · · exb]
(9)
then the distance is rewritten as:
∥x −q(y)∥2
2 = ∥f
x1 −q1( ey1)∥2
2 + ... + ∥exb −qb( eyb)∥2
2.
(10)
Each quantizer q1, ..., qb has 256 reproduction values, so
when x and q1(y) are known all distances can be precom-
puted and stored in tables T1, ..., Tb each of size 256 [25].
Computing the sum (10) consists of b look-ups and addi-
tions. Comparing the cost to compute n distances:
• Explicit computation: n × d mutiply-adds;
• With lookup tables: 256 × d multiply-adds and n × b
lookup-adds.
This is the key to the eﬃciency of the product quantizer.
In our GPU implementation, b is any multiple of 4 up to
64. The codes are stored as sequential groups of b bytes per
vector within lists.
IVFADC lookup tables.
When scanning over the ele-
ments of the inverted list IL (where by deﬁnition q1(y) is
constant), the look-up table method can be applied, as the
query x and q1(y) are known.
6

53.79798889160156 | 56.75346374511719 | 292.94091796875 | 264.47491455078125 | will be the min-k elements across the merged, sorted queues,
and the new thread queues will be the remainder, from min-
(k +1) to min-(k +32t+1). This restores the invariants and
we are free to continue processing subsequent elements.
Since the thread and warp queues are already sorted, we
merge the sorted warp queue of length k with 32 sorted
arrays of length t. Supporting odd-sized merges is important
because Batcher’s formulation would require that 32t = k
and is a power-of-2; thus if k = 1024, t must be 32. We
found that the optimal t is way smaller (see below).
Using odd-merge to merge the 32 already sorted thread
queues would require a struct-of-arrays to array-of-structs
transposition in registers across the warp, since the t succes-
sive sorted values are held in diﬀerent registers in the same
lane rather than a lane-stride array. This is possible [12],
but would use a comparable number of warp shuﬄes, so we
just reinterpret the thread queue registers as an (unsorted)
lane-stride array and sort from scratch. Signiﬁcant speedup
is realizable by using odd-merge for the merge of the ag-
gregate sorted thread queues with the warp queue.
 |  | 
53.798004150390625 | 271.9135437011719 | 292.9388427734375 | 312.262939453125 | Handling the remainder. If there are remainder elements
because ℓis not a multiple of 32, those are inserted into the
thread queues for the lanes that have them, after which we
proceed to the output stage.
 | 1 | 
53.798004150390625 | 319.7015380859375 | 292.9173889160156 | 349.5899353027344 | Output. A ﬁnal sort and merge is made of the thread and
warp queues, after which the warp queue holds all min-k
values.
 | 2 | 
53.798004150390625 | 356.8692321777344 | 292.95880126953125 | 401.8769226074219 | 4.3
Complexity and parameter selection
For each incoming group of 32 elements, WarpSelect
can perform 1, 2 or 3 constant-time operations, all happen-
ing in warp-wide parallel time:
 | 3 | 
64.56300354003906 | 410.4795227050781 | 292.9282531738281 | 432.1849060058594 | 1. read 32 elements, compare to all thread queue heads
T j
0 , cost C1, happens N1 times;
 | 4 | 
64.56302642822266 | 437.6513366699219 | 292.9180908203125 | 470.44989013671875 | 2. if ∃j ∈{0, ..., 31}, a32n+j < T j
0 , perform insertion sort
on those speciﬁc thread queues, cost C2 = O(t), hap-
pens N2 times;
 | 5 | 
64.56303405761719 | 476.17333984375 | 292.9125671386719 | 498.5118713378906 | 3. if ∃j, T j
0 < Wk−1, sort and merge queues, cost C3 =
O(t log(32t)2 + k log(max(k, 32t))), happens N3 times.
 | 6 | 
53.7979736328125 | 506.1174621582031 | 292.9180908203125 | 588.3098754882812 | Thus, the total cost is N1C1 + N2C2 + N3C3. N1 = ℓ/32,
and on random data drawn independently, N2 = O(k log(ℓ))
and N3 = O(k log(ℓ)/t), see the Appendix for a full deriva-
tion. Hence, the trade-oﬀis to balance a cost in N2C2 and
one in N3C3. The practical choice for t given k and ℓwas
made by experiment on a variety of k-NN data. For k ≤32,
we use t = 2, k ≤128 uses t = 3, k ≤256 uses t = 4, and
k ≤1024 uses t = 8, all irrespective of ℓ.
 | 7 | 
53.79798889160156 | 601.0521850585938 | 292.96771240234375 | 677.44287109375 | 5.
COMPUTATION LAYOUT
This section explains how IVFADC, one of the indexing
methods originally built upon product quantization [25], is
implemented eﬃciently. Details on distance computations
and articulation with k-selection are the key to understand-
ing why this method can outperform more recent GPU-
compliant approximate nearest neighbor strategies [47].
 | 8 | 
53.79798889160156 | 684.72216796875 | 292.92291259765625 | 719.2688598632812 | 5.1
Exact search
We brieﬂy come back to the exhaustive search method,
often referred to as exact brute-force. It is interesting on its
 | 9 | 
316.8119812011719 | 56.75346374511719 | 555.9728393554688 | 316.7789001464844 | own for exact nearest neighbor search in small datasets. It
is also a component of many indexes in the literature. In
our case, we use it for the IVFADC coarse quantizer q1.
As stated in Section 2, the distance computation boils
down to a matrix multiplication. We use optimized GEMM
routines in the cuBLAS library to calculate the −2⟨xj, yi⟩
term for L2 distance, resulting in a partial distance matrix
D′. To complete the distance calculation, we use a fused
k-selection kernel that adds the ∥yi∥2 term to each entry of
the distance matrix and immediately submits the value to
k-selection in registers. The ∥xj∥2 term need not be taken
into account before k-selection. Kernel fusion thus allows
for only 2 passes (GEMM write, k-select read) over D′, com-
pared to other implementations that may require 3 or more.
Row-wise k-selection is likely not fusable with a well-tuned
GEMM kernel, or would result in lower overall eﬃciency.
As D′ does not ﬁt in GPU memory for realistic problem
sizes, the problem is tiled over the batch of queries, with
tq ≤nq queries being run in a single tile. Each of the ⌈nq/tq⌉
tiles are independent problems, but we run two in parallel
on diﬀerent streams to better occupy the GPU, so the eﬀec-
tive memory requirement of D is O(2ℓtq). The computation
can similarly be tiled over ℓ. For very large input coming
from the CPU, we support buﬀering with pinned memory
to overlap CPU to GPU copy with GPU compute.
 | 10 | 
316.81201171875 | 326.6531982421875 | 435.9575500488281 | 338.6083984375 | 5.2
IVFADC indexing
 | 11 | 
316.81201171875 | 350.3294982910156 | 555.9549560546875 | 390.67889404296875 | PQ lookup tables. At its core, the IVFADC requires com-
puting the distance from a vector to a set of product quanti-
zation reproduction values. By developing Equation (6) for
a database vector y, we obtain:
 | 12 | 
349.14599609375 | 398.7583312988281 | 555.9218139648438 | 411.27288818359375 | ∥x −q(y)∥2
2 = ∥x −q1(y) −q2(y −q1(y))∥2
2.
(7)
 | 13 | 
316.81201171875 | 419.7464904785156 | 527.6025390625 | 429.7088928222656 | If we decompose the residual vectors left after q1 as:
 | 14 | 
376.9970397949219 | 437.6922912597656 | 555.9304809570312 | 455.76983642578125 | y −q1(y)
=
[ ey1 · · · eyb] and
(8)
 | 15 | 
376.6340026855469 | 453.25927734375 | 555.9222412109375 | 471.3368225097656 | x −q1(y)
=
[f
x1 · · · exb]
(9)
 | 16 | 
316.81201171875 | 474.9664611816406 | 449.64019775390625 | 483.932861328125 | then the distance is rewritten as:
 | 17 | 
324.281005859375 | 491.29327392578125 | 555.9224243164062 | 509.3708190917969 | ∥x −q(y)∥2
2 = ∥f
x1 −q1( ey1)∥2
2 + ... + ∥exb −qb( eyb)∥2
2.
(10)
 | 18 | 
316.8119201660156 | 509.1914367675781 | 555.93896484375 | 563.8098754882812 | Each quantizer q1, ..., qb has 256 reproduction values, so
when x and q1(y) are known all distances can be precom-
puted and stored in tables T1, ..., Tb each of size 256 [25].
Computing the sum (10) consists of b look-ups and addi-
tions. Comparing the cost to compute n distances:
 | 19 | 
330.13702392578125 | 574.9402465820312 | 511.280517578125 | 584.0228271484375 | • Explicit computation: n × d mutiply-adds;
 | 20 | 
330.1369934082031 | 597.7312622070312 | 555.9112548828125 | 617.2748413085938 | • With lookup tables: 256 × d multiply-adds and n × b
lookup-adds.
 | 21 | 
316.8120422363281 | 628.521484375 | 555.9401245117188 | 668.869873046875 | This is the key to the eﬃciency of the product quantizer.
In our GPU implementation, b is any multiple of 4 up to
64. The codes are stored as sequential groups of b bytes per
vector within lists.
 | 22 | 
316.81201171875 | 678.9204711914062 | 555.9638671875 | 719.6129150390625 | IVFADC lookup tables.
When scanning over the ele-
ments of the inverted list IL (where by deﬁnition q1(y) is
constant), the look-up table method can be applied, as the
query x and q1(y) are known.
 | 23 | 
302.55303955078125 | 740.1914672851562 | 307.1617736816406 | 749.1578369140625 | 6
 | 24 | 
Moreover, the computation of the tables T1 . . . Tb is fur-
ther optimized [5]. The expression of ∥x−q(y)∥2
2 in Equation
(7) can be decomposed as:
∥q2(...)∥2
2 + 2⟨q1(y), q2(...)⟩
|
{z
}
term 1
+ ∥x −q1(y)∥2
2
|
{z
}
term 2
−2 ⟨x, q2(...)⟩
|
{z
}
term 3
.
(11)
The objective is to minimize inner loop computations.
The computations we can do in advance and store in lookup
tables are as follows:
• Term 1 is independent of the query. It can be precom-
puted from the quantizers, and stored in a table T of
size |C1| × 256 × b;
• Term 2 is the distance to q1’s reproduction value. It is
thus a by-product of the ﬁrst-level quantizer q1;
• Term 3 can be computed independently of the inverted
list. Its computation costs d × 256 multiply-adds.
This decomposition is used to produce the lookup tables
T1 . . . Tb used during the scan of the inverted list.
For a
single query, computing the τ × b tables from scratch costs
τ × d × 256 multiply-adds, while this decomposition costs
256×d multiply-adds and τ×b×256 additions. On the GPU,
the memory usage of T can be prohibitive, so we enable the
decomposition only when memory is a not a concern.
5.3
GPU implementation
Algorithm 4 summarizes the process as one would im-
plement it on a CPU. The inverted lists are stored as two
separate arrays, for PQ codes and associated IDs. IDs are
resolved only if k-selection determines k-nearest member-
ship. This lookup yields a few sparse memory reads in a
large array, thus the IDs can optionally be stored on CPU
for tiny performance cost.
List scanning. A kernel is responsible for scanning the τ
closest inverted lists for each query, and calculating the per-
vector pair distances using the lookup tables Ti. The Ti are
stored in shared memory: up to nq×τ ×maxi |Ii|×b lookups
are required for a query set (trillions of accesses in practice),
and are random access.
This limits b to at most 48 (32-
bit ﬂoating point) or 96 (16-bit ﬂoating point) with current
architectures. In case we do not use the decomposition of
Equation (11), the Ti are calculated by a separate kernel
before scanning.
Multi-pass kernels. Each nq × τ pairs of query against
inverted list can be processed independently.
At one ex-
treme, a block is dedicated to each of these, resulting in up
to nq × τ × maxi |Ii| partial results being written back to
global memory, which is then k-selected to nq × k ﬁnal re-
sults. This yields high parallelism but can exceed available
GPU global memory; as with exact search, we choose a tile
size tq ≤nq to reduce memory consumption, bounding its
complexity by O(2tqτ maxi |Ii|) with multi-streaming.
A single warp could be dedicated to k-selection of each
tq set of lists, which could result in low parallelism.
We
introduce a two-pass k-selection, reducing tq × τ × maxi |Ii|
to tq × f × k partial results for some subdivision factor f.
This is reduced again via k-selection to the ﬁnal tq×k results.
Fused kernel. As with exact search, we experimented with
a kernel that dedicates a single block to scanning all τ lists
for a single query, with k-selection fused with distance com-
putation. This is possible as WarpSelect does not ﬁght for
the shared memory resource which is severely limited. This
reduces global memory write-back, since almost all interme-
diate results can be eliminated. However, unlike k-selection
overhead for exact computation, a signiﬁcant portion of the
runtime is the gather from the Ti in shared memory and lin-
ear scanning of the Ii from global memory; the write-back is
not a dominant contributor. Timing for the fused kernel is
improved by at most 15%, and for some problem sizes would
be subject to lower parallelism and worse performance with-
out subsequent decomposition. Therefore, and for reasons
of implementation simplicity, we do not use this layout.
Algorithm 4 IVFPQ batch search routine
function ivfpq-search([x1, ..., xnq], I1, ..., I|C1|)
for i ←0 : nq do ▷batch quantization of Section 5.1
Li
IVF ←τ-argminc∈C1∥x −c∥2
end for
for i ←0 : nq do
L ←[]
▷distance table
Compute term 3 (see Section 5.2)
for L in Li
IVF do
▷τ loops
Compute distance tables T1, ..., Tb
for j in IL do
▷distance estimation, Equation (10)
d ←∥xi −q(yj)∥2
2
Append (d, L, j) to L
end for
end for
Ri ←k-select smallest distances d from L
end for
return R
end function
5.4
Multi-GPU parallelism
Modern servers can support several GPUs. We employ
this capability for both compute power and memory.
Replication. If an index instance ﬁts in the memory of a
single GPU, it can be replicated across R diﬀerent GPUs. To
query nq vectors, each replica handles a fraction nq/R of the
queries, joining the results back together on a single GPU
or in CPU memory. Replication has near linear speedup,
except for a potential loss in eﬃciency for small nq.
Sharding. If an index instance does not ﬁt in the memory
of a single GPU, an index can be sharded across S diﬀer-
ent GPUs. For adding ℓvectors, each shard receives ℓ/S of
the vectors, and for query, each shard handles the full query
set nq, joining the partial results (an additional round of k-
selection is still required) on a single GPU or in CPU mem-
ory. For a given index size ℓ, sharding will yield a speedup
(sharding has a query of nq against ℓ/S versus replication
with a query of nq/R against ℓ), but is usually less than
pure replication due to ﬁxed overhead and cost of subse-
quent k-selection.
Replication and sharding can be used together (S shards,
each with R replicas for S × R GPUs in total). Sharding or
replication are both fairly trivial, and the same principle can
be used to distribute an index across multiple machines.
7

53.79798889160156 | 56.75346374511719 | 292.90924072265625 | 86.64082336425781 | Moreover, the computation of the tables T1 . . . Tb is fur-
ther optimized [5]. The expression of ∥x−q(y)∥2
2 in Equation
(7) can be decomposed as:
 |  | 
61.13697814941406 | 92.33325958251953 | 168.16754150390625 | 117.39691925048828 | ∥q2(...)∥2
2 + 2⟨q1(y), q2(...)⟩
|
{z
}
term 1
 | 1 | 
169.7010040283203 | 92.33332061767578 | 228.5538787841797 | 117.39691925048828 | + ∥x −q1(y)∥2
2
|
{z
}
term 2
 | 2 | 
230.08999633789062 | 94.18828582763672 | 283.00555419921875 | 117.39691925048828 | −2 ⟨x, q2(...)⟩
|
{z
}
term 3
 | 3 | 
284.5389709472656 | 94.30448913574219 | 287.1033630371094 | 103.27088928222656 | .
 | 4 | 
53.7979850769043 | 117.80149841308594 | 292.9593505859375 | 158.14991760253906 | (11)
The objective is to minimize inner loop computations.
The computations we can do in advance and store in lookup
tables are as follows:
 | 5 | 
67.12298583984375 | 166.89231872558594 | 292.9139404296875 | 197.2399444580078 | • Term 1 is independent of the query. It can be precom-
puted from the quantizers, and stored in a table T of
size |C1| × 256 × b;
 | 6 | 
67.12297058105469 | 205.8313446044922 | 292.9188232421875 | 225.7179718017578 | • Term 2 is the distance to q1’s reproduction value. It is
thus a by-product of the ﬁrst-level quantizer q1;
 | 7 | 
67.12300109863281 | 234.3093719482422 | 292.9139099121094 | 253.8529815673828 | • Term 3 can be computed independently of the inverted
list. Its computation costs d × 256 multiply-adds.
 | 8 | 
53.7979736328125 | 262.7115783691406 | 292.9523620605469 | 334.4429626464844 | This decomposition is used to produce the lookup tables
T1 . . . Tb used during the scan of the inverted list.
For a
single query, computing the τ × b tables from scratch costs
τ × d × 256 multiply-adds, while this decomposition costs
256×d multiply-adds and τ×b×256 additions. On the GPU,
the memory usage of T can be prohibitive, so we enable the
decomposition only when memory is a not a concern.
 | 9 | 
53.7979736328125 | 341.9302673339844 | 292.93243408203125 | 428.78094482421875 | 5.3
GPU implementation
Algorithm 4 summarizes the process as one would im-
plement it on a CPU. The inverted lists are stored as two
separate arrays, for PQ codes and associated IDs. IDs are
resolved only if k-selection determines k-nearest member-
ship. This lookup yields a few sparse memory reads in a
large array, thus the IDs can optionally be stored on CPU
for tiny performance cost.
 | 10 | 
53.7979736328125 | 436.4445495605469 | 292.97674560546875 | 539.5579223632812 | List scanning. A kernel is responsible for scanning the τ
closest inverted lists for each query, and calculating the per-
vector pair distances using the lookup tables Ti. The Ti are
stored in shared memory: up to nq×τ ×maxi |Ii|×b lookups
are required for a query set (trillions of accesses in practice),
and are random access.
This limits b to at most 48 (32-
bit ﬂoating point) or 96 (16-bit ﬂoating point) with current
architectures. In case we do not use the decomposition of
Equation (11), the Ti are calculated by a separate kernel
before scanning.
 | 11 | 
53.79798889160156 | 547.1053466796875 | 292.94586181640625 | 692.52197265625 | Multi-pass kernels. Each nq × τ pairs of query against
inverted list can be processed independently.
At one ex-
treme, a block is dedicated to each of these, resulting in up
to nq × τ × maxi |Ii| partial results being written back to
global memory, which is then k-selected to nq × k ﬁnal re-
sults. This yields high parallelism but can exceed available
GPU global memory; as with exact search, we choose a tile
size tq ≤nq to reduce memory consumption, bounding its
complexity by O(2tqτ maxi |Ii|) with multi-streaming.
A single warp could be dedicated to k-selection of each
tq set of lists, which could result in low parallelism.
We
introduce a two-pass k-selection, reducing tq × τ × maxi |Ii|
to tq × f × k partial results for some subdivision factor f.
This is reduced again via k-selection to the ﬁnal tq×k results.
 | 12 | 
53.798004150390625 | 699.8425903320312 | 292.90887451171875 | 719.2689208984375 | Fused kernel. As with exact search, we experimented with
a kernel that dedicates a single block to scanning all τ lists
 | 13 | 
316.81195068359375 | 56.75352478027344 | 555.972900390625 | 191.2489776611328 | for a single query, with k-selection fused with distance com-
putation. This is possible as WarpSelect does not ﬁght for
the shared memory resource which is severely limited. This
reduces global memory write-back, since almost all interme-
diate results can be eliminated. However, unlike k-selection
overhead for exact computation, a signiﬁcant portion of the
runtime is the gather from the Ti in shared memory and lin-
ear scanning of the Ii from global memory; the write-back is
not a dominant contributor. Timing for the fused kernel is
improved by at most 15%, and for some problem sizes would
be subject to lower parallelism and worse performance with-
out subsequent decomposition. Therefore, and for reasons
of implementation simplicity, we do not use this layout.
 | 14 | 
316.81201171875 | 204.00144958496094 | 491.25384521484375 | 212.9678497314453 | Algorithm 4 IVFPQ batch search routine
 | 15 | 
326.0270080566406 | 214.89462280273438 | 525.465576171875 | 227.49517822265625 | function ivfpq-search([x1, ..., xnq], I1, ..., I|C1|)
 | 16 | 
339.85003662109375 | 227.19725036621094 | 555.9519653320312 | 237.27687072753906 | for i ←0 : nq do ▷batch quantization of Section 5.1
 | 17 | 
339.85003662109375 | 236.30128479003906 | 473.92333984375 | 268.6588134765625 | Li
IVF ←τ-argminc∈C1∥x −c∥2
end for
for i ←0 : nq do
 | 18 | 
353.6729736328125 | 269.040283203125 | 555.922607421875 | 320.9628601074219 | L ←[]
▷distance table
Compute term 3 (see Section 5.2)
for L in Li
IVF do
▷τ loops
Compute distance tables T1, ..., Tb
for j in IL do
 | 19 | 
326.02703857421875 | 321.46044921875 | 555.9451293945312 | 414.1128234863281 | ▷distance estimation, Equation (10)
d ←∥xi −q(yj)∥2
2
Append (d, L, j) to L
end for
end for
Ri ←k-select smallest distances d from L
end for
return R
end function
 | 20 | 
316.81201171875 | 434.8591613769531 | 555.900634765625 | 469.4058532714844 | 5.4
Multi-GPU parallelism
Modern servers can support several GPUs. We employ
this capability for both compute power and memory.
 | 21 | 
316.8119812011719 | 476.87744140625 | 555.9549560546875 | 538.4918823242188 | Replication. If an index instance ﬁts in the memory of a
single GPU, it can be replicated across R diﬀerent GPUs. To
query nq vectors, each replica handles a fraction nq/R of the
queries, joining the results back together on a single GPU
or in CPU memory. Replication has near linear speedup,
except for a potential loss in eﬃciency for small nq.
 | 22 | 
316.8118896484375 | 545.6204223632812 | 555.9816284179688 | 701.0368041992188 | Sharding. If an index instance does not ﬁt in the memory
of a single GPU, an index can be sharded across S diﬀer-
ent GPUs. For adding ℓvectors, each shard receives ℓ/S of
the vectors, and for query, each shard handles the full query
set nq, joining the partial results (an additional round of k-
selection is still required) on a single GPU or in CPU mem-
ory. For a given index size ℓ, sharding will yield a speedup
(sharding has a query of nq against ℓ/S versus replication
with a query of nq/R against ℓ), but is usually less than
pure replication due to ﬁxed overhead and cost of subse-
quent k-selection.
Replication and sharding can be used together (S shards,
each with R replicas for S × R GPUs in total). Sharding or
replication are both fairly trivial, and the same principle can
be used to distribute an index across multiple machines.
 | 23 | 
302.5528869628906 | 740.1904296875 | 307.16162109375 | 749.1567993164062 | 7
 | 24 | 






	



	




  ! 
Figure 3:
Runtimes for diﬀerent k-selection meth-
ods, as a function of array length ℓ. Simultaneous
arrays processed are nq = 10000. k = 100 for full lines,
k = 1000 for dashed lines.
6.
EXPERIMENTS & APPLICATIONS
This section compares our GPU k-selection and nearest-
neighbor approach to existing libraries. Unless stated other-
wise, experiments are carried out on a 2×2.8GHz Intel Xeon
E5-2680v2 with 4 Maxwell Titan X GPUs on CUDA 8.0.
6.1
k-selection performance
We compare against two other GPU small k-selection im-
plementations: the row-based Merge Queue with Buﬀered
Search and Hierarchical Partition extracted from the fgknn
library of Tang et al. [41] and Truncated Bitonic Sort (TBiS)
from Sismanis et al. [40]. Both were extracted from their re-
spective exact search libraries.
We evaluate k-selection for k = 100 and 1000 of each row
from a row-major matrix nq × ℓof random 32-bit ﬂoating
point values on a single Titan X. The batch size nq is ﬁxed
at 10000, and the array lengths ℓvary from 1000 to 128000.
Inputs and outputs to the problem remain resident in GPU
memory, with the output being of size nq × k, with corre-
sponding indices. Thus, the input problem sizes range from
40 MB (ℓ= 1000) to 5.12 GB (ℓ= 128k). TBiS requires large
auxiliary storage, and is limited to ℓ≤48000 in our tests.
Figure 3 shows our relative performance against TBiS and
fgknn. It also includes the peak possible performance given
by the memory bandwidth limit of the Titan X. The rela-
tive performance of WarpSelect over fgknn increases for
larger k; even TBiS starts to outperform fgknn for larger ℓ
at k = 1000. We look especially at the largest ℓ= 128000.
WarpSelect is 1.62× faster at k = 100, 2.01× at k = 1000.
Performance against peak possible drops oﬀfor all imple-
mentations at larger k. WarpSelect operates at 55% of
peak at k = 100 but only 16% of peak at k = 1000. This
is due to additional overhead assocated with bigger thread
queues and merge/sort networks for large k.
Diﬀerences from fgknn. WarpSelect is inﬂuenced by
fgknn, but has several improvements: all state is maintained
in registers (no shared memory), no inter-warp synchroniza-
tion or buﬀering is used, no “hierarchical partition”, the k-
selection can be fused into other kernels, and it uses odd-size
networks for eﬃcient merging and sorting.
# centroids
method
# GPUs
256
4096
BIDMach [11]
1
320 s
735 s
Ours
1
140 s
316 s
Ours
4
84 s
100 s
Table 1:
MNIST8m k-means performance
6.2
k-means clustering
The exact search method with k = 1 can be used by a k-
means clustering method in the assignment stage, to assign
nq training vectors to |C1| centroids. Despite the fact that
it does not use the IVFADC and k = 1 selection is trivial (a
parallel reduction is used for the k = 1 case, not WarpSe-
lect), k-means is a good benchmark for the clustering used
to train the quantizer q1.
We apply the algorithm on MNIST8m images. The 8.1M
images are graylevel digits in 28x28 pixels, linearized to vec-
tors of 784-d. We compare this k-means implementation to
the GPU k-means of BIDMach [11], which was shown to be
more eﬃcient than several distributed k-means implemen-
tations that require dozens of machines3. Both algorithms
were run for 20 iterations. Table 1 shows that our imple-
mentation is more than 2× faster, although both are built
upon cuBLAS. Our implementation receives some beneﬁt
from the k-selection fusion into L2 distance computation.
For multi-GPU execution via replicas, the speedup is close
to linear for large enough problems (3.16× for 4 GPUs with
4096 centroids). Note that this benchmark is somewhat un-
realistic, as one would typically sub-sample the dataset ran-
domly when so few centroids are requested.
Large scale. We can also compare to [3], an approximate
CPU method that clusters 108 128-d vectors to 85k cen-
troids. Their clustering method runs in 46 minutes, but re-
quires 56 minutes (at least) of pre-processing to encode the
vectors. Our method performs exact k-means on 4 GPUs in
52 minutes without any pre-processing.
6.3
Exact nearest neighbor search
We consider a classical dataset used to evaluate nearest
neighbor search: Sift1M [25]. Its characteristic sizes are
ℓ= 106, d = 128, nq = 104. Computing the partial distance
matrix D′ costs nq × ℓ× d = 1.28 Tﬂop, which runs in less
than one second on current GPUs. Figure 4 shows the cost
of the distance computations against the cost of our tiling
of the GEMM for the −2 ⟨xj, yi⟩term of Equation 2 and
the peak possible k-selection performance on the distance
matrix of size nq×ℓ, which additionally accounts for reading
the tiled result matrix D′ at peak memory bandwidth.
In addition to our method from Section 5, we include
times from the two GPU libraries evaluated for k-selection
performance in Section 6.1. We make several observations:
• for k-selection, the naive algorithm that sorts the full
result array for each query using thrust::sort_by_key
is more than 10× slower than the comparison methods;
• L2 distance and k-selection cost is dominant for all but
our method, which has 85 % of the peak possible
performance, assuming GEMM usage and our tiling
3BIDMach numbers from https://github.com/BIDData/
BIDMach/wiki/Benchmarks#KMeans
8

71.08743286132812 | 171.2527618408203 | 82.89071655273438 | 179.2531280517578 | 
 |  | 
76.99759674072266 | 137.0474853515625 | 82.89923858642578 | 145.0478515625 | 
 | 1 | 
73.05662536621094 | 102.84223175048828 | 82.89741516113281 | 110.84259796142578 | 
 | 2 | 
69.11565399169922 | 68.636962890625 | 82.89559173583984 | 76.6373291015625 | 
 | 3 | 
78.95641326904297 | 178.3252410888672 | 265.3208923339844 | 186.3256072998047 | 

	



 | 4 | 
55.881473541259766 | 98.64485168457031 | 63.881839752197266 | 138.38351440429688 | 	
 | 5 | 
165.3880157470703 | 188.9183807373047 | 202.7816162109375 | 196.9187469482422 | 
 | 6 | 
185.07733154296875 | 143.6892852783203 | 250.79592895507812 | 151.6896514892578 | 
 | 7 | 
213.03515625 | 150.7617645263672 | 250.8184051513672 | 158.7621307373047 | 
 | 8 | 
176.4300079345703 | 158.74981689453125 | 250.8190155029297 | 172.90968322753906 | 
  ! 
 | 9 | 
53.79798889160156 | 210.67747497558594 | 292.9217529296875 | 251.02687072753906 | Figure 3:
Runtimes for diﬀerent k-selection meth-
ods, as a function of array length ℓ. Simultaneous
arrays processed are nq = 10000. k = 100 for full lines,
k = 1000 for dashed lines.
 | 10 | 
53.79798126220703 | 269.82916259765625 | 292.9318542480469 | 325.2978515625 | 6.
EXPERIMENTS & APPLICATIONS
This section compares our GPU k-selection and nearest-
neighbor approach to existing libraries. Unless stated other-
wise, experiments are carried out on a 2×2.8GHz Intel Xeon
E5-2680v2 with 4 Maxwell Titan X GPUs on CUDA 8.0.
 | 11 | 
53.79798889160156 | 334.6361389160156 | 292.94989013671875 | 630.7017822265625 | 6.1
k-selection performance
We compare against two other GPU small k-selection im-
plementations: the row-based Merge Queue with Buﬀered
Search and Hierarchical Partition extracted from the fgknn
library of Tang et al. [41] and Truncated Bitonic Sort (TBiS)
from Sismanis et al. [40]. Both were extracted from their re-
spective exact search libraries.
We evaluate k-selection for k = 100 and 1000 of each row
from a row-major matrix nq × ℓof random 32-bit ﬂoating
point values on a single Titan X. The batch size nq is ﬁxed
at 10000, and the array lengths ℓvary from 1000 to 128000.
Inputs and outputs to the problem remain resident in GPU
memory, with the output being of size nq × k, with corre-
sponding indices. Thus, the input problem sizes range from
40 MB (ℓ= 1000) to 5.12 GB (ℓ= 128k). TBiS requires large
auxiliary storage, and is limited to ℓ≤48000 in our tests.
Figure 3 shows our relative performance against TBiS and
fgknn. It also includes the peak possible performance given
by the memory bandwidth limit of the Titan X. The rela-
tive performance of WarpSelect over fgknn increases for
larger k; even TBiS starts to outperform fgknn for larger ℓ
at k = 1000. We look especially at the largest ℓ= 128000.
WarpSelect is 1.62× faster at k = 100, 2.01× at k = 1000.
Performance against peak possible drops oﬀfor all imple-
mentations at larger k. WarpSelect operates at 55% of
peak at k = 100 but only 16% of peak at k = 1000. This
is due to additional overhead assocated with bigger thread
queues and merge/sort networks for large k.
 | 12 | 
53.79804992675781 | 638.257568359375 | 292.94091796875 | 701.4867553710938 | Diﬀerences from fgknn. WarpSelect is inﬂuenced by
fgknn, but has several improvements: all state is maintained
in registers (no shared memory), no inter-warp synchroniza-
tion or buﬀering is used, no “hierarchical partition”, the k-
selection can be fused into other kernels, and it uses odd-size
networks for eﬃcient merging and sorting.
 | 13 | 
355.18798828125 | 54.51145935058594 | 517.5516357421875 | 105.71986389160156 | # centroids
method
# GPUs
256
4096
BIDMach [11]
1
320 s
735 s
Ours
1
140 s
316 s
Ours
4
84 s
100 s
 | 14 | 
335.1919860839844 | 117.47544860839844 | 533.9844360351562 | 126.44184875488281 | Table 1:
MNIST8m k-means performance
 | 15 | 
316.81201171875 | 145.3331298828125 | 555.964111328125 | 389.09588623046875 | 6.2
k-means clustering
The exact search method with k = 1 can be used by a k-
means clustering method in the assignment stage, to assign
nq training vectors to |C1| centroids. Despite the fact that
it does not use the IVFADC and k = 1 selection is trivial (a
parallel reduction is used for the k = 1 case, not WarpSe-
lect), k-means is a good benchmark for the clustering used
to train the quantizer q1.
We apply the algorithm on MNIST8m images. The 8.1M
images are graylevel digits in 28x28 pixels, linearized to vec-
tors of 784-d. We compare this k-means implementation to
the GPU k-means of BIDMach [11], which was shown to be
more eﬃcient than several distributed k-means implemen-
tations that require dozens of machines3. Both algorithms
were run for 20 iterations. Table 1 shows that our imple-
mentation is more than 2× faster, although both are built
upon cuBLAS. Our implementation receives some beneﬁt
from the k-selection fusion into L2 distance computation.
For multi-GPU execution via replicas, the speedup is close
to linear for large enough problems (3.16× for 4 GPUs with
4096 centroids). Note that this benchmark is somewhat un-
realistic, as one would typically sub-sample the dataset ran-
domly when so few centroids are requested.
 | 16 | 
316.81207275390625 | 396.9554748535156 | 555.9548950195312 | 458.2258605957031 | Large scale. We can also compare to [3], an approximate
CPU method that clusters 108 128-d vectors to 85k cen-
troids. Their clustering method runs in 46 minutes, but re-
quires 56 minutes (at least) of pre-processing to encode the
vectors. Our method performs exact k-means on 4 GPUs in
52 minutes without any pre-processing.
 | 17 | 
316.8120422363281 | 465.9091491699219 | 555.9638671875 | 615.5248413085938 | 6.3
Exact nearest neighbor search
We consider a classical dataset used to evaluate nearest
neighbor search: Sift1M [25]. Its characteristic sizes are
ℓ= 106, d = 128, nq = 104. Computing the partial distance
matrix D′ costs nq × ℓ× d = 1.28 Tﬂop, which runs in less
than one second on current GPUs. Figure 4 shows the cost
of the distance computations against the cost of our tiling
of the GEMM for the −2 ⟨xj, yi⟩term of Equation 2 and
the peak possible k-selection performance on the distance
matrix of size nq×ℓ, which additionally accounts for reading
the tiled result matrix D′ at peak memory bandwidth.
In addition to our method from Section 5, we include
times from the two GPU libraries evaluated for k-selection
performance in Section 6.1. We make several observations:
 | 18 | 
330.13714599609375 | 624.4632568359375 | 556.8758544921875 | 654.466796875 | • for k-selection, the naive algorithm that sorts the full
result array for each query using thrust::sort_by_key
is more than 10× slower than the comparison methods;
 | 19 | 
330.13714599609375 | 663.7932739257812 | 555.9599609375 | 693.7968139648438 | • L2 distance and k-selection cost is dominant for all but
our method, which has 85 % of the peak possible
performance, assuming GEMM usage and our tiling
 | 20 | 
316.81201171875 | 699.8633422851562 | 555.9166259765625 | 719.6995849609375 | 3BIDMach numbers from https://github.com/BIDData/
BIDMach/wiki/Benchmarks#KMeans
 | 21 | 
302.5530090332031 | 740.1904907226562 | 307.1617431640625 | 749.1568603515625 | 8
 | 22 | 















	
	


		

 !
""
#$	""
Figure 4:
Exact search k-NN time for the SIFT1M
dataset with varying k on 1 Titan X GPU.
of the partial distance matrix D′ on top of GEMM is
close to optimal. The cuBLAS GEMM itself has low
eﬃciency for small reduction sizes (d = 128);
• Our fused L2/k-selection kernel is important.
Our
same exact algorithm without fusion (requiring an ad-
ditional pass through D′) is at least 25% slower.
Eﬃcient k-selection is even more important in situations
where approximate methods are used to compute distances,
because the relative cost of k-selection with respect to dis-
tance computation increases.
6.4
Billion-scale approximate search
There are few studies on GPU-based approximate nearest-
neighbor search on large datasets (ℓ≫106). We report a
few comparison points here on index search, using standard
datasets and evaluation protocol in this ﬁeld.
SIFT1M. For the sake of completeness, we ﬁrst compare
our GPU search speed on Sift1M with the implementation
of Wieschollek et al. [47]. They obtain a nearest neighbor re-
call at 1 (fraction of queries where the true nearest neighbor
is in the top 1 result) of R@1 = 0.51, and R@100 = 0.86 in
0.02 ms per query on a Titan X. For the same time budget,
our implementation obtains R@1 = 0.80 and R@100 = 0.95.
SIFT1B. We compare again with Wieschollek et al., on the
Sift1B dataset [26] of 1 billion SIFT image features at nq =
104. We compare the search performance in terms of same
memory usage for similar accuracy (more accurate methods
may involve greater search time or memory usage). On a
single GPU, with m = 8 bytes per vector, R@10 = 0.376 in
17.7 µs per query vector, versus their reported R@10 = 0.35
in 150 µs per query vector.
Thus, our implementation is
more accurate at a speed 8.5× faster.
DEEP1B. We also experimented on the Deep1B dataset [6]
of ℓ=1 billion CNN representations for images at nq = 104.
The paper that introduces the dataset reports CPU results
(1 thread): R@1 = 0.45 in 20 ms search time per vector. We
use a PQ encoding of m = 20, with d = 80 via OPQ [17],
and |C1| = 218, which uses a comparable dataset storage as
the original paper (20 GB). This requires multiple GPUs as
it is too large for a single GPU’s global memory, so we con-
sider 4 GPUs with S = 2, R = 2. We obtain a R@1 = 0.4517
in 0.0133 ms per vector. While the hardware platforms are











	




	

















	




	








Figure 5:
Speed/accuracy trade-oﬀof brute-force
10-NN graph construction for the YFCC100M and
DEEP1B datasets.
diﬀerent, it shows that making searches on GPUs is a game-
changer in terms of speed achievable on a single machine.
6.5
The k-NN graph
An example usage of our similarity search method is to
construct a k-nearest neighbor graph of a dataset via brute
force (all vectors queried against the entire index).
Experimental setup. We evaluate the trade-oﬀbetween
speed, precision and memory on two datasets: 95 million
images from the Yfcc100M dataset [42] and Deep1B. For
Yfcc100M, we compute CNN descriptors as the one-before-
last layer of a ResNet [23], reduced to d = 128 with PCA.
The evaluation measures the trade-oﬀbetween:
• Speed: How much time it takes to build the IVFADC
index from scratch and construct the whole k-NN graph
(k = 10) by searching nearest neighbors for all vectors
in the dataset. Thus, this is an end-to-end test that
includes indexing as well as search time;
• Quality: We sample 10,000 images for which we com-
pute the exact nearest neighbors. Our accuracy mea-
sure is the fraction of 10 found nearest neighbors that
are within the ground-truth 10 nearest neighbors.
For Yfcc100M, we use a coarse quantizer (216 centroids),
and consider m = 16, 32 and 64 byte PQ encodings for each
vector. For Deep1B, we pre-process the vectors to d = 120
via OPQ, use |C1| = 218 and consider m = 20, 40. For a
given encoding, we vary τ from 1 to 256, to obtain trade-
oﬀs between eﬃciency and quality, as seen in Figure 5.
9

76.99759674072266 | 171.2527618408203 | 82.89923858642578 | 179.2531280517578 | 
 |  | 
71.08743286132812 | 157.13893127441406 | 82.89071655273438 | 165.13929748535156 | 
 | 1 | 
76.99759674072266 | 143.0251007080078 | 82.89923858642578 | 151.0254669189453 | 
 | 2 | 
71.08743286132812 | 128.91128540039062 | 82.89071655273438 | 136.91165161132812 | 
 | 3 | 
76.99759674072266 | 114.7974624633789 | 82.89923858642578 | 122.7978286743164 | 
 | 4 | 
71.08743286132812 | 100.68364715576172 | 82.89071655273438 | 108.68401336669922 | 
 | 5 | 
76.99759674072266 | 86.56982421875 | 82.89923858642578 | 94.5701904296875 | 
 | 6 | 
71.08743286132812 | 72.45599365234375 | 82.89071655273438 | 80.45635986328125 | 
 | 7 | 
76.99759674072266 | 58.342166900634766 | 82.89923858642578 | 66.34253692626953 | 
 | 8 | 
83.90403747558594 | 178.3252410888672 | 290.1827697753906 | 186.3256072998047 | 





 | 9 | 
55.881473541259766 | 101.59659576416016 | 63.881839752197266 | 135.43360900878906 | 	
 | 10 | 
181.33871459960938 | 188.9183807373047 | 186.84361267089844 | 196.9187469482422 | 	
 | 11 | 
96.26919555664062 | 67.53950500488281 | 167.47146606445312 | 75.53987121582031 | 

 | 12 | 
97.81808471679688 | 74.61457824707031 | 167.4900360107422 | 82.61494445800781 | 		

 | 13 | 
101.7564697265625 | 81.68705749511719 | 167.50424194335938 | 96.75990295410156 |  !
""
 | 14 | 
150.18971252441406 | 96.75016784667969 | 167.51205444335938 | 103.83497619628906 | #$	""
 | 15 | 
53.79798889160156 | 210.67747497558594 | 292.8888854980469 | 230.10487365722656 | Figure 4:
Exact search k-NN time for the SIFT1M
dataset with varying k on 1 Titan X GPU.
 | 16 | 
76.21397399902344 | 246.5814666748047 | 292.931884765625 | 280.27789306640625 | of the partial distance matrix D′ on top of GEMM is
close to optimal. The cuBLAS GEMM itself has low
eﬃciency for small reduction sizes (d = 128);
 | 17 | 
67.12298583984375 | 288.6363220214844 | 292.94085693359375 | 318.639892578125 | • Our fused L2/k-selection kernel is important.
Our
same exact algorithm without fusion (requiring an ad-
ditional pass through D′) is at least 25% slower.
 | 18 | 
53.79796600341797 | 326.9205017089844 | 292.9400329589844 | 367.2688903808594 | Eﬃcient k-selection is even more important in situations
where approximate methods are used to compute distances,
because the relative cost of k-selection with respect to dis-
tance computation increases.
 | 19 | 
53.79796600341797 | 374.46820068359375 | 292.91387939453125 | 429.9368896484375 | 6.4
Billion-scale approximate search
There are few studies on GPU-based approximate nearest-
neighbor search on large datasets (ℓ≫106). We report a
few comparison points here on index search, using standard
datasets and evaluation protocol in this ﬁeld.
 | 20 | 
53.7979736328125 | 437.2154846191406 | 292.9408874511719 | 508.94586181640625 | SIFT1M. For the sake of completeness, we ﬁrst compare
our GPU search speed on Sift1M with the implementation
of Wieschollek et al. [47]. They obtain a nearest neighbor re-
call at 1 (fraction of queries where the true nearest neighbor
is in the top 1 result) of R@1 = 0.51, and R@100 = 0.86 in
0.02 ms per query on a Titan X. For the same time budget,
our implementation obtains R@1 = 0.80 and R@100 = 0.95.
 | 21 | 
53.79793930053711 | 516.2244873046875 | 292.9407958984375 | 608.8768310546875 | SIFT1B. We compare again with Wieschollek et al., on the
Sift1B dataset [26] of 1 billion SIFT image features at nq =
104. We compare the search performance in terms of same
memory usage for similar accuracy (more accurate methods
may involve greater search time or memory usage). On a
single GPU, with m = 8 bytes per vector, R@10 = 0.376 in
17.7 µs per query vector, versus their reported R@10 = 0.35
in 150 µs per query vector.
Thus, our implementation is
more accurate at a speed 8.5× faster.
 | 22 | 
53.79791259765625 | 614.196533203125 | 293.3392028808594 | 719.2688598632812 | DEEP1B. We also experimented on the Deep1B dataset [6]
of ℓ=1 billion CNN representations for images at nq = 104.
The paper that introduces the dataset reports CPU results
(1 thread): R@1 = 0.45 in 20 ms search time per vector. We
use a PQ encoding of m = 20, with d = 80 via OPQ [17],
and |C1| = 218, which uses a comparable dataset storage as
the original paper (20 GB). This requires multiple GPUs as
it is too large for a single GPU’s global memory, so we con-
sider 4 GPUs with S = 2, R = 2. We obtain a R@1 = 0.4517
in 0.0133 ms per vector. While the hardware platforms are
 | 23 | 
336.0602722167969 | 171.2527618408203 | 341.9618835449219 | 179.2531280517578 | 
 | 24 | 
332.1192932128906 | 152.42222595214844 | 341.9600830078125 | 160.42259216308594 | 
 | 25 | 
332.1192932128906 | 133.6254119873047 | 341.9600830078125 | 141.6257781982422 | 
 | 26 | 
332.1192932128906 | 114.7974624633789 | 341.9600830078125 | 122.7978286743164 | 
 | 27 | 
332.1192932128906 | 95.96692657470703 | 341.9600830078125 | 103.96729278564453 | 
 | 28 | 
328.1783142089844 | 77.17012023925781 | 341.958251953125 | 85.17048645019531 | 
 | 29 | 
328.1783142089844 | 58.342166900634766 | 341.958251953125 | 66.34253692626953 | 
 | 30 | 
340.0115966796875 | 178.3252410888672 | 550.2386474609375 | 186.3256072998047 | 



	




 | 31 | 
318.8955078125 | 75.24480438232422 | 326.8958740234375 | 161.8140106201172 | 	

 | 32 | 
413.01690673828125 | 188.9183807373047 | 477.1697998046875 | 196.9187469482422 | 
 | 33 | 
480.5894775390625 | 158.9806671142578 | 536.0723876953125 | 169.6075439453125 | 
 | 34 | 
353.1966247558594 | 68.47091674804688 | 437.17279052734375 | 90.61624145507812 | 


 | 35 | 
336.0602722167969 | 315.7127990722656 | 341.9618835449219 | 323.7131652832031 | 
 | 36 | 
336.0602722167969 | 296.88226318359375 | 341.9618835449219 | 304.88262939453125 | 
 | 37 | 
336.0602722167969 | 278.08544921875 | 341.9618835449219 | 286.0858154296875 | 
 | 38 | 
332.1192932128906 | 259.2574768066406 | 341.9600830078125 | 267.2578430175781 | 
 | 39 | 
332.1192932128906 | 240.42694091796875 | 341.9600830078125 | 248.42730712890625 | 
 | 40 | 
332.1192932128906 | 221.63014221191406 | 341.9600830078125 | 229.63050842285156 | 
 | 41 | 
332.1192932128906 | 202.80218505859375 | 341.9600830078125 | 210.80255126953125 | 
 | 42 | 
340.0115966796875 | 322.7852783203125 | 550.2386474609375 | 330.78564453125 | 



	




 | 43 | 
318.8955078125 | 216.55421447753906 | 326.8958740234375 | 309.42889404296875 | 	


 | 44 | 
413.01690673828125 | 333.37841796875 | 477.1697998046875 | 341.3787841796875 | 
 | 45 | 
495.27923583984375 | 303.4407043457031 | 537.17041015625 | 314.0675964355469 | 
 | 46 | 
353.1966247558594 | 211.07070922851562 | 437.17279052734375 | 226.1435546875 | 

 | 47 | 
361.8594970703125 | 225.21826171875 | 437.19927978515625 | 240.29110717773438 | 

 | 48 | 
316.81201171875 | 355.13751220703125 | 555.9100341796875 | 385.0259094238281 | Figure 5:
Speed/accuracy trade-oﬀof brute-force
10-NN graph construction for the YFCC100M and
DEEP1B datasets.
 | 49 | 
316.81201171875 | 403.1094970703125 | 555.9458618164062 | 422.5368957519531 | diﬀerent, it shows that making searches on GPUs is a game-
changer in terms of speed achievable on a single machine.
 | 50 | 
316.81201171875 | 429.6221923828125 | 555.9454956054688 | 474.6298828125 | 6.5
The k-NN graph
An example usage of our similarity search method is to
construct a k-nearest neighbor graph of a dataset via brute
force (all vectors queried against the entire index).
 | 51 | 
316.81201171875 | 481.6794738769531 | 555.954833984375 | 542.9498291015625 | Experimental setup. We evaluate the trade-oﬀbetween
speed, precision and memory on two datasets: 95 million
images from the Yfcc100M dataset [42] and Deep1B. For
Yfcc100M, we compute CNN descriptors as the one-before-
last layer of a ResNet [23], reduced to d = 128 with PCA.
The evaluation measures the trade-oﬀbetween:
 | 52 | 
330.13702392578125 | 550.6563110351562 | 557.7235107421875 | 601.5818481445312 | • Speed: How much time it takes to build the IVFADC
index from scratch and construct the whole k-NN graph
(k = 10) by searching nearest neighbors for all vectors
in the dataset. Thus, this is an end-to-end test that
includes indexing as well as search time;
 | 53 | 
330.1369934082031 | 609.7113037109375 | 555.9548950195312 | 650.1759033203125 | • Quality: We sample 10,000 images for which we com-
pute the exact nearest neighbors. Our accuracy mea-
sure is the fraction of 10 found nearest neighbors that
are within the ground-truth 10 nearest neighbors.
 | 54 | 
316.8119201660156 | 654.1895141601562 | 555.953369140625 | 719.2688598632812 | For Yfcc100M, we use a coarse quantizer (216 centroids),
and consider m = 16, 32 and 64 byte PQ encodings for each
vector. For Deep1B, we pre-process the vectors to d = 120
via OPQ, use |C1| = 218 and consider m = 20, 40. For a
given encoding, we vary τ from 1 to 256, to obtain trade-
oﬀs between eﬃciency and quality, as seen in Figure 5.
 | 55 | 
302.5529479980469 | 740.1904907226562 | 307.16168212890625 | 749.1568603515625 | 9
 | 56 | 
Figure 6:
Path in the k-NN graph of 95 million images from YFCC100M. The ﬁrst and the last image are
given; the algorithm computes the smoothest path between them.
Discussion. For Yfcc100M we used S = 1, R = 4. An
accuracy of more than 0.8 is obtained in 35 minutes. For
Deep1B, a lower-quality graph can be built in 6 hours,
with higher quality in about half a day.
We also experi-
mented with more GPUs by doubling the replica set, us-
ing 8 Maxwell M40s (the M40 is roughly equivalent in per-
formance to the Titan X). Performance is improved sub-
linearly (∼1.6× for m = 20, ∼1.7× for m = 40).
For comparison, the largest k-NN graph construction we
are aware of used a dataset comprising 36.5 million 384-
d vectors, which took a cluster of 128 CPU servers 108.7
hours of compute [45], using NN-Descent [15]. Note that
NN-Descent could also build or reﬁne the k-NN graph for
the datasets we consider, but it has a large memory over-
head over the graph storage, which is already 80 GB for
Deep1B. Moreover it requires random access across all vec-
tors (384 GB for Deep1B).
The largest GPU k-NN graph construction we found is a
brute-force construction using exact search with GEMM, of
a dataset of 20 million 15,000-d vectors, which took a cluster
of 32 Tesla C2050 GPUs 10 days [14]. Assuming computa-
tion scales with GEMM cost for the distance matrix, this
approach for Deep1B would take an impractical 200 days
of computation time on their cluster.
6.6
Using the k-NN graph
When a k-NN graph has been constructed for an image
dataset, we can ﬁnd paths in the graph between any two
images, provided there is a single connected component (this
is the case). For example, we can search the shortest path
between two images of ﬂowers, by propagating neighbors
from a starting image to a destination image. Denoting by
S and D the source and destination images, and dij the
distance between nodes, we search the path P = {p1, ..., pn}
with p1 = S and pn = D such that
min
P
max
i=1..n dpipi+1,
(12)
i.e., we want to favor smooth transitions. An example re-
sult is shown in Figure 6 from Yfcc100M4. It was ob-
tained after 20 seconds of propagation in a k-NN graph with
k = 15 neighbors. Since there are many ﬂower images in the
dataset, the transitions are smooth.
4The mapping from vectors to images is not available for
Deep1B
7.
CONCLUSION
The arithmetic throughput and memory bandwidth of
GPUs are well into the teraﬂops and hundreds of gigabytes
per second.
However, implementing algorithms that ap-
proach these performance levels is complex and counter-
intuitive. In this paper, we presented the algorithmic struc-
ture of similarity search methods that achieves near-optimal
performance on GPUs.
This work enables applications that needed complex ap-
proximate algorithms before. For example, the approaches
presented here make it possible to do exact k-means cluster-
ing or to compute the k-NN graph with simple brute-force
approaches in less time than a CPU (or a cluster of them)
would take to do this approximately.
GPU hardware is now very common on scientiﬁc work-
stations, due to their popularity for machine learning algo-
rithms. We believe that our work further demonstrates their
interest for database applications. Along with this work, we
are publishing a carefully engineered implementation of this
paper’s algorithms, so that these GPUs can now also be used
for eﬃcient similarity search.
8.
REFERENCES
[1] T. Alabi, J. D. Blanchard, B. Gordon, and R. Steinbach.
Fast k-selection algorithms for graphics processing units.
ACM Journal of Experimental Algorithmics,
17:4.2:4.1–4.2:4.29, October 2012.
[2] F. Andr´e, A.-M. Kermarrec, and N. L. Scouarnec. Cache
locality is not enough: High-performance nearest neighbor
search with product quantization fast scan. In Proc.
International Conference on Very Large DataBases, pages
288–299, 2015.
[3] Y. Avrithis, Y. Kalantidis, E. Anagnostopoulos, and I. Z.
Emiris. Web-scale image clustering revisited. In Proc.
International Conference on Computer Vision, pages
1502–1510, 2015.
[4] A. Babenko and V. Lempitsky. The inverted multi-index.
In Proc. IEEE Conference on Computer Vision and
Pattern Recognition, pages 3069–3076, June 2012.
[5] A. Babenko and V. Lempitsky. Improving bilayer product
quantization for billion-scale approximate nearest neighbors
in high dimensions. arXiv preprint arXiv:1404.1831, 2014.
[6] A. Babenko and V. Lempitsky. Eﬃcient indexing of
billion-scale datasets of deep descriptors. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition,
pages 2055–2063, June 2016.
[7] R. Barrientos, J. G´omez, C. Tenllado, M. Prieto, and
M. Marin. knn query processing in metric spaces using
GPUs. In International European Conference on Parallel
and Distributed Computing, volume 6852 of Lecture Notes
10

53.79800033569336 | 176.0284881591797 | 555.9295654296875 | 195.4558868408203 | Figure 6:
Path in the k-NN graph of 95 million images from YFCC100M. The ﬁrst and the last image are
given; the algorithm computes the smoothest path between them.
 |  | 
53.7979736328125 | 214.16958618164062 | 292.9587707519531 | 465.69281005859375 | Discussion. For Yfcc100M we used S = 1, R = 4. An
accuracy of more than 0.8 is obtained in 35 minutes. For
Deep1B, a lower-quality graph can be built in 6 hours,
with higher quality in about half a day.
We also experi-
mented with more GPUs by doubling the replica set, us-
ing 8 Maxwell M40s (the M40 is roughly equivalent in per-
formance to the Titan X). Performance is improved sub-
linearly (∼1.6× for m = 20, ∼1.7× for m = 40).
For comparison, the largest k-NN graph construction we
are aware of used a dataset comprising 36.5 million 384-
d vectors, which took a cluster of 128 CPU servers 108.7
hours of compute [45], using NN-Descent [15]. Note that
NN-Descent could also build or reﬁne the k-NN graph for
the datasets we consider, but it has a large memory over-
head over the graph storage, which is already 80 GB for
Deep1B. Moreover it requires random access across all vec-
tors (384 GB for Deep1B).
The largest GPU k-NN graph construction we found is a
brute-force construction using exact search with GEMM, of
a dataset of 20 million 15,000-d vectors, which took a cluster
of 32 Tesla C2050 GPUs 10 days [14]. Assuming computa-
tion scales with GEMM cost for the distance matrix, this
approach for Deep1B would take an impractical 200 days
of computation time on their cluster.
 | 1 | 
53.7979736328125 | 480.13909912109375 | 292.95880126953125 | 588.9077758789062 | 6.6
Using the k-NN graph
When a k-NN graph has been constructed for an image
dataset, we can ﬁnd paths in the graph between any two
images, provided there is a single connected component (this
is the case). For example, we can search the shortest path
between two images of ﬂowers, by propagating neighbors
from a starting image to a destination image. Denoting by
S and D the source and destination images, and dij the
distance between nodes, we search the path P = {p1, ..., pn}
with p1 = S and pn = D such that
 | 2 | 
139.21197509765625 | 602.534423828125 | 292.9095153808594 | 616.955810546875 | min
P
max
i=1..n dpipi+1,
(12)
 | 3 | 
53.79795837402344 | 629.4603881835938 | 292.92950439453125 | 680.270751953125 | i.e., we want to favor smooth transitions. An example re-
sult is shown in Figure 6 from Yfcc100M4. It was ob-
tained after 20 seconds of propagation in a k-NN graph with
k = 15 neighbors. Since there are many ﬂower images in the
dataset, the transitions are smooth.
 | 4 | 
53.798004150390625 | 699.8633422851562 | 292.9312744140625 | 717.3099975585938 | 4The mapping from vectors to images is not available for
Deep1B
 | 5 | 
316.81201171875 | 213.96014404296875 | 555.9727172851562 | 436.8008117675781 | 7.
CONCLUSION
The arithmetic throughput and memory bandwidth of
GPUs are well into the teraﬂops and hundreds of gigabytes
per second.
However, implementing algorithms that ap-
proach these performance levels is complex and counter-
intuitive. In this paper, we presented the algorithmic struc-
ture of similarity search methods that achieves near-optimal
performance on GPUs.
This work enables applications that needed complex ap-
proximate algorithms before. For example, the approaches
presented here make it possible to do exact k-means cluster-
ing or to compute the k-NN graph with simple brute-force
approaches in less time than a CPU (or a cluster of them)
would take to do this approximately.
GPU hardware is now very common on scientiﬁc work-
stations, due to their popularity for machine learning algo-
rithms. We believe that our work further demonstrates their
interest for database applications. Along with this work, we
are publishing a carefully engineered implementation of this
paper’s algorithms, so that these GPUs can now also be used
for eﬃcient similarity search.
 | 6 | 
316.81201171875 | 456.7080993652344 | 418.1084289550781 | 468.6632995605469 | 8.
REFERENCES
 | 7 | 
321.0460205078125 | 471.9780578613281 | 555.8294067382812 | 719.0512084960938 | [1] T. Alabi, J. D. Blanchard, B. Gordon, and R. Steinbach.
Fast k-selection algorithms for graphics processing units.
ACM Journal of Experimental Algorithmics,
17:4.2:4.1–4.2:4.29, October 2012.
[2] F. Andr´e, A.-M. Kermarrec, and N. L. Scouarnec. Cache
locality is not enough: High-performance nearest neighbor
search with product quantization fast scan. In Proc.
International Conference on Very Large DataBases, pages
288–299, 2015.
[3] Y. Avrithis, Y. Kalantidis, E. Anagnostopoulos, and I. Z.
Emiris. Web-scale image clustering revisited. In Proc.
International Conference on Computer Vision, pages
1502–1510, 2015.
[4] A. Babenko and V. Lempitsky. The inverted multi-index.
In Proc. IEEE Conference on Computer Vision and
Pattern Recognition, pages 3069–3076, June 2012.
[5] A. Babenko and V. Lempitsky. Improving bilayer product
quantization for billion-scale approximate nearest neighbors
in high dimensions. arXiv preprint arXiv:1404.1831, 2014.
[6] A. Babenko and V. Lempitsky. Eﬃcient indexing of
billion-scale datasets of deep descriptors. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition,
pages 2055–2063, June 2016.
[7] R. Barrientos, J. G´omez, C. Tenllado, M. Prieto, and
M. Marin. knn query processing in metric spaces using
GPUs. In International European Conference on Parallel
and Distributed Computing, volume 6852 of Lecture Notes
 | 8 | 
300.2490234375 | 740.1904907226562 | 309.46649169921875 | 749.1568603515625 | 10
 | 9 | 
in Computer Science, pages 380–392, Bordeaux, France,
September 2011. Springer.
[8] K. E. Batcher. Sorting networks and their applications. In
Proc. Spring Joint Computer Conference, AFIPS ’68
(Spring), pages 307–314, New York, NY, USA, 1968. ACM.
[9] P. Boncz, W. Lehner, and T. Neumann. Special issue:
Modern hardware. The VLDB Journal, 25(5):623–624,
2016.
[10] J. Canny, D. L. W. Hall, and D. Klein. A multi-teraﬂop
constituency parser using GPUs. In Proc. Empirical
Methods on Natural Language Processing, pages 1898–1907.
ACL, 2013.
[11] J. Canny and H. Zhao. Bidmach: Large-scale learning with
zero memory allocation. In BigLearn workshop, NIPS,
2013.
[12] B. Catanzaro, A. Keller, and M. Garland. A decomposition
for in-place matrix transposition. In Proc. ACM
Symposium on Principles and Practice of Parallel
Programming, PPoPP ’14, pages 193–206, 2014.
[13] J. Chhugani, A. D. Nguyen, V. W. Lee, W. Macy,
M. Hagog, Y.-K. Chen, A. Baransi, S. Kumar, and
P. Dubey. Eﬃcient implementation of sorting on multi-core
simd cpu architecture. Proc. VLDB Endow.,
1(2):1313–1324, August 2008.
[14] A. Dashti. Eﬃcient computation of k-nearest neighbor
graphs for large high-dimensional data sets on gpu clusters.
Master’s thesis, University of Wisconsin Milwaukee, August
2013.
[15] W. Dong, M. Charikar, and K. Li. Eﬃcient k-nearest
neighbor graph construction for generic similarity measures.
In WWW: Proceeding of the International Conference on
World Wide Web, pages 577–586, March 2011.
[16] M. Douze, H. J´egou, and F. Perronnin. Polysemous codes.
In Proc. European Conference on Computer Vision, pages
785–801. Springer, October 2016.
[17] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product
quantization. IEEE Trans. PAMI, 36(4):744–755, 2014.
[18] Y. Gong and S. Lazebnik. Iterative quantization: A
procrustean approach to learning binary codes. In Proc.
IEEE Conference on Computer Vision and Pattern
Recognition, pages 817–824, June 2011.
[19] Y. Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale
orderless pooling of deep convolutional activation features.
In Proc. European Conference on Computer Vision, pages
392–407, 2014.
[20] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep
image retrieval: Learning global representations for image
search. In Proc. European Conference on Computer Vision,
pages 241–257, 2016.
[21] S. Han, H. Mao, and W. J. Dally. Deep compression:
Compressing deep neural networks with pruning, trained
quantization and huﬀman coding. arXiv preprint
arXiv:1510.00149, 2015.
[22] K. He, F. Wen, and J. Sun. K-means hashing: An
aﬃnity-preserving quantization method for learning binary
compact codes. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pages 2938–2945, June
2013.
[23] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual
learning for image recognition. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition, pages
770–778, June 2016.
[24] X. He, D. Agarwal, and S. K. Prasad. Design and
implementation of a parallel priority queue on many-core
architectures. IEEE International Conference on High
Performance Computing, pages 1–10, 2012.
[25] H. J´egou, M. Douze, and C. Schmid. Product quantization
for nearest neighbor search. IEEE Trans. PAMI,
33(1):117–128, January 2011.
[26] H. J´egou, R. Tavenard, M. Douze, and L. Amsaleg.
Searching in one billion vectors: re-rank with source
coding. In International Conference on Acoustics, Speech,
and Signal Processing, pages 861–864, May 2011.
[27] Y. Kalantidis and Y. Avrithis. Locally optimized product
quantization for approximate nearest neighbor search. In
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pages 2329–2336, June 2014.
[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiﬁcation with deep convolutional neural networks. In
Advances in Neural Information Processing Systems, pages
1097–1105, 2012.
[29] F. T. Leighton. Introduction to Parallel Algorithms and
Architectures: Array, Trees, Hypercubes. Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 1992.
[30] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym.
NVIDIA Tesla: a uniﬁed graphics and computing
architecture. IEEE Micro, 28(2):39–55, March 2008.
[31] W. Liu and B. Vinter. Ad-heap: An eﬃcient heap data
structure for asymmetric multicore processors. In Proc. of
Workshop on General Purpose Processing Using GPUs,
pages 54:54–54:63. ACM, 2014.
[32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and phrases
and their compositionality. In Advances in Neural
Information Processing Systems, pages 3111–3119, 2013.
[33] L. Monroe, J. Wendelberger, and S. Michalak. Randomized
selection on the GPU. In Proc. ACM Symposium on High
Performance Graphics, pages 89–98, 2011.
[34] M. Norouzi and D. Fleet. Cartesian k-means. In Proc.
IEEE Conference on Computer Vision and Pattern
Recognition, pages 3017–3024, June 2013.
[35] M. Norouzi, A. Punjani, and D. J. Fleet. Fast search in
Hamming space with multi-index hashing. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition,
pages 3108–3115, 2012.
[36] J. Pan and D. Manocha. Fast GPU-based locality sensitive
hashing for k-nearest neighbor computation. In Proc. ACM
International Conference on Advances in Geographic
Information Systems, pages 211–220, 2011.
[37] L. Paulev´e, H. J´egou, and L. Amsaleg. Locality sensitive
hashing: a comparison of hash function types and querying
mechanisms. Pattern recognition letters, 31(11):1348–1358,
August 2010.
[38] O. Shamir. Fundamental limits of online and distributed
algorithms for statistical learning and estimation. In
Advances in Neural Information Processing Systems, pages
163–171, 2014.
[39] A. Sharif Razavian, H. Azizpour, J. Sullivan, and
S. Carlsson. CNN features oﬀ-the-shelf: an astounding
baseline for recognition. In CVPR workshops, pages
512–519, 2014.
[40] N. Sismanis, N. Pitsianis, and X. Sun. Parallel search of
k-nearest neighbors with synchronous operations. In IEEE
High Performance Extreme Computing Conference, pages
1–6, 2012.
[41] X. Tang, Z. Huang, D. M. Eyers, S. Mills, and M. Guo.
Eﬃcient selection algorithm for fast k-nn search on GPUs.
In IEEE International Parallel & Distributed Processing
Symposium, pages 397–406, 2015.
[42] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde,
K. Ni, D. Poland, D. Borth, and L.-J. Li. YFCC100M: The
new data in multimedia research. Communications of the
ACM, 59(2):64–73, January 2016.
[43] V. Volkov and J. W. Demmel. Benchmarking GPUs to tune
dense linear algebra. In Proc. ACM/IEEE Conference on
Supercomputing, pages 31:1–31:11, 2008.
[44] A. Wakatani and A. Murakami. GPGPU implementation of
nearest neighbor search with product quantization. In
IEEE International Symposium on Parallel and Distributed
Processing with Applications, pages 248–253, 2014.
[45] T. Warashina, K. Aoyama, H. Sawada, and T. Hattori.
Eﬃcient k-nearest neighbor graph construction using
mapreduce for large-scale data sets. IEICE Transactions,
11

53.79798889160156 | 57.532100677490234 | 292.9001770019531 | 719.4943237304688 | in Computer Science, pages 380–392, Bordeaux, France,
September 2011. Springer.
[8] K. E. Batcher. Sorting networks and their applications. In
Proc. Spring Joint Computer Conference, AFIPS ’68
(Spring), pages 307–314, New York, NY, USA, 1968. ACM.
[9] P. Boncz, W. Lehner, and T. Neumann. Special issue:
Modern hardware. The VLDB Journal, 25(5):623–624,
2016.
[10] J. Canny, D. L. W. Hall, and D. Klein. A multi-teraﬂop
constituency parser using GPUs. In Proc. Empirical
Methods on Natural Language Processing, pages 1898–1907.
ACL, 2013.
[11] J. Canny and H. Zhao. Bidmach: Large-scale learning with
zero memory allocation. In BigLearn workshop, NIPS,
2013.
[12] B. Catanzaro, A. Keller, and M. Garland. A decomposition
for in-place matrix transposition. In Proc. ACM
Symposium on Principles and Practice of Parallel
Programming, PPoPP ’14, pages 193–206, 2014.
[13] J. Chhugani, A. D. Nguyen, V. W. Lee, W. Macy,
M. Hagog, Y.-K. Chen, A. Baransi, S. Kumar, and
P. Dubey. Eﬃcient implementation of sorting on multi-core
simd cpu architecture. Proc. VLDB Endow.,
1(2):1313–1324, August 2008.
[14] A. Dashti. Eﬃcient computation of k-nearest neighbor
graphs for large high-dimensional data sets on gpu clusters.
Master’s thesis, University of Wisconsin Milwaukee, August
2013.
[15] W. Dong, M. Charikar, and K. Li. Eﬃcient k-nearest
neighbor graph construction for generic similarity measures.
In WWW: Proceeding of the International Conference on
World Wide Web, pages 577–586, March 2011.
[16] M. Douze, H. J´egou, and F. Perronnin. Polysemous codes.
In Proc. European Conference on Computer Vision, pages
785–801. Springer, October 2016.
[17] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product
quantization. IEEE Trans. PAMI, 36(4):744–755, 2014.
[18] Y. Gong and S. Lazebnik. Iterative quantization: A
procrustean approach to learning binary codes. In Proc.
IEEE Conference on Computer Vision and Pattern
Recognition, pages 817–824, June 2011.
[19] Y. Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale
orderless pooling of deep convolutional activation features.
In Proc. European Conference on Computer Vision, pages
392–407, 2014.
[20] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep
image retrieval: Learning global representations for image
search. In Proc. European Conference on Computer Vision,
pages 241–257, 2016.
[21] S. Han, H. Mao, and W. J. Dally. Deep compression:
Compressing deep neural networks with pruning, trained
quantization and huﬀman coding. arXiv preprint
arXiv:1510.00149, 2015.
[22] K. He, F. Wen, and J. Sun. K-means hashing: An
aﬃnity-preserving quantization method for learning binary
compact codes. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pages 2938–2945, June
2013.
[23] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual
learning for image recognition. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition, pages
770–778, June 2016.
[24] X. He, D. Agarwal, and S. K. Prasad. Design and
implementation of a parallel priority queue on many-core
architectures. IEEE International Conference on High
Performance Computing, pages 1–10, 2012.
[25] H. J´egou, M. Douze, and C. Schmid. Product quantization
for nearest neighbor search. IEEE Trans. PAMI,
33(1):117–128, January 2011.
[26] H. J´egou, R. Tavenard, M. Douze, and L. Amsaleg.
Searching in one billion vectors: re-rank with source
coding. In International Conference on Acoustics, Speech,
 |  | 
316.81201171875 | 57.532222747802734 | 555.9893188476562 | 712.0772705078125 | and Signal Processing, pages 861–864, May 2011.
[27] Y. Kalantidis and Y. Avrithis. Locally optimized product
quantization for approximate nearest neighbor search. In
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pages 2329–2336, June 2014.
[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiﬁcation with deep convolutional neural networks. In
Advances in Neural Information Processing Systems, pages
1097–1105, 2012.
[29] F. T. Leighton. Introduction to Parallel Algorithms and
Architectures: Array, Trees, Hypercubes. Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 1992.
[30] E. Lindholm, J. Nickolls, S. Oberman, and J. Montrym.
NVIDIA Tesla: a uniﬁed graphics and computing
architecture. IEEE Micro, 28(2):39–55, March 2008.
[31] W. Liu and B. Vinter. Ad-heap: An eﬃcient heap data
structure for asymmetric multicore processors. In Proc. of
Workshop on General Purpose Processing Using GPUs,
pages 54:54–54:63. ACM, 2014.
[32] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and phrases
and their compositionality. In Advances in Neural
Information Processing Systems, pages 3111–3119, 2013.
[33] L. Monroe, J. Wendelberger, and S. Michalak. Randomized
selection on the GPU. In Proc. ACM Symposium on High
Performance Graphics, pages 89–98, 2011.
[34] M. Norouzi and D. Fleet. Cartesian k-means. In Proc.
IEEE Conference on Computer Vision and Pattern
Recognition, pages 3017–3024, June 2013.
[35] M. Norouzi, A. Punjani, and D. J. Fleet. Fast search in
Hamming space with multi-index hashing. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition,
pages 3108–3115, 2012.
[36] J. Pan and D. Manocha. Fast GPU-based locality sensitive
hashing for k-nearest neighbor computation. In Proc. ACM
International Conference on Advances in Geographic
Information Systems, pages 211–220, 2011.
[37] L. Paulev´e, H. J´egou, and L. Amsaleg. Locality sensitive
hashing: a comparison of hash function types and querying
mechanisms. Pattern recognition letters, 31(11):1348–1358,
August 2010.
[38] O. Shamir. Fundamental limits of online and distributed
algorithms for statistical learning and estimation. In
Advances in Neural Information Processing Systems, pages
163–171, 2014.
[39] A. Sharif Razavian, H. Azizpour, J. Sullivan, and
S. Carlsson. CNN features oﬀ-the-shelf: an astounding
baseline for recognition. In CVPR workshops, pages
512–519, 2014.
[40] N. Sismanis, N. Pitsianis, and X. Sun. Parallel search of
k-nearest neighbors with synchronous operations. In IEEE
High Performance Extreme Computing Conference, pages
1–6, 2012.
[41] X. Tang, Z. Huang, D. M. Eyers, S. Mills, and M. Guo.
Eﬃcient selection algorithm for fast k-nn search on GPUs.
In IEEE International Parallel & Distributed Processing
Symposium, pages 397–406, 2015.
[42] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde,
K. Ni, D. Poland, D. Borth, and L.-J. Li. YFCC100M: The
new data in multimedia research. Communications of the
ACM, 59(2):64–73, January 2016.
[43] V. Volkov and J. W. Demmel. Benchmarking GPUs to tune
dense linear algebra. In Proc. ACM/IEEE Conference on
Supercomputing, pages 31:1–31:11, 2008.
[44] A. Wakatani and A. Murakami. GPGPU implementation of
nearest neighbor search with product quantization. In
IEEE International Symposium on Parallel and Distributed
Processing with Applications, pages 248–253, 2014.
[45] T. Warashina, K. Aoyama, H. Sawada, and T. Hattori.
Eﬃcient k-nearest neighbor graph construction using
mapreduce for large-scale data sets. IEICE Transactions,
 | 1 | 
300.24920654296875 | 740.1906127929688 | 309.4666748046875 | 749.156982421875 | 11
 | 2 | 
97-D(12):3142–3154, 2014.
[46] R. Weber, H.-J. Schek, and S. Blott. A quantitative
analysis and performance study for similarity-search
methods in high-dimensional spaces. In Proc. International
Conference on Very Large DataBases, pages 194–205, 1998.
[47] P. Wieschollek, O. Wang, A. Sorkine-Hornung, and
H. P. A. Lensch. Eﬃcient large-scale approximate nearest
neighbor search on the GPU. In Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pages
2027–2035, June 2016.
[48] S. Williams, A. Waterman, and D. Patterson. Rooﬂine: An
insightful visual performance model for multicore
architectures. Communications of the ACM, 52(4):65–76,
April 2009.
Appendix: Complexity analysis of WarpSelect
We derive the average number of times updates are triggered
in WarpSelect, for use in Section 4.3.
Let the input to k-selection be a sequence {a1, a2, ..., aℓ}
(1-based indexing), a randomly chosen permutation of a set
of distinct elements.
Elements are read sequentially in c
groups of size w (the warp; in our case, w = 32); assume ℓ
is a multiple of w, so c = ℓ/w. Recall that t is the thread
queue length.
We call elements prior to or at position n
in the min-k seen so far the successive min-k (at n). The
likelihood that an is in the successive min-k at n is:
α(n, k) :=
(
1
if n ≤k
k/n
if n > k
(13)
as each an, n > k has a k/n chance as all permutations are
equally likely, and all elements in the ﬁrst k qualify.
Counting the insertion sorts. In a given lane, an inser-
tion sort is triggered if the incoming value is in the successive
min-k + t values, but the lane has “seen” only wc0 + (c−c0)
values, where c0 is the previous won warp ballot. The prob-
ability of this happening is:
α(wc0 + (c −c0), k + t) ≈k + t
wc
for c > k.
(14)
The approximation considers that the thread queue has seen
all the wc values, not just those assigned to its lane. The
probability of any lane triggering an insertion sort is then:
1 −

1 −k + t
wc
w
≈k + t
c
.
(15)
Here the approximation is a ﬁrst-order Taylor expansion.
Summing up the probabilities over c gives an expected num-
ber of insertions of N2 ≈(k + t) log(c) = O(k log(ℓ/w)).
Counting full sorts. We seek N3 = π(ℓ, k, t, w), the ex-
pected number of full sorts required for WarpSelect.
Single lane. For now, we assume w = 1, so c = ℓ. Let
γ(ℓ, m, k) be the probability that in an sequence {a1, ..., aℓ},
exactly m of the elements as encountered by a sequential
scanner (w = 1) are in the successive min-k. Given m, there
are
  ℓ
m

places where these successive min-k elements can
occur. It is given by a recurrence relation:
γ(ℓ, m, k) :=













1
ℓ= 0 and m = 0
0
ℓ= 0 and m > 0
0
ℓ> 0 and m = 0
(γ(ℓ−1, m −1, k) · α(ℓ, k)+
γ(ℓ−1, m, k) · (1 −α(ℓ, k)))
otherwise.
(16)
The last case is the probability of: there is a ℓ−1 se-
quence with m −1 successive min-k elements preceding us,
and the current element is in the successive min-k, or the
current element is not in the successive min-k, m ones are
before us. We can then develop a recurrence relationship for
π(ℓ, k, t, 1). Note that
δ(ℓ, b, k, t) :=
min((bt+max(0,t−1)),ℓ)
X
m=bt
γ(ℓ, m, k)
(17)
for b where 0 ≤bt ≤ℓis the fraction of all sequences of
length ℓthat will force b sorts of data by winning the thread
queue ballot, as there have to be bt to (bt + max(0, t −1))
elements in the successive min-k for these sorts to happen (as
the min-k elements will overﬂow the thread queues). There
are at most ⌊ℓ/t⌋won ballots that can occur, as it takes t
separate sequential current min-k seen elements to win the
ballot.
π(ℓ, k, t, 1) is thus the expectation of this over all
possible b:
π(ℓ, k, t, 1) =
⌊ℓ/t⌋
X
b=1
b · δ(ℓ, b, k, t).
(18)
This can be computed by dynamic programming. Analyti-
cally, note that for t = 1, k = 1, π(ℓ, 1, 1, 1) is the harmonic
number Hℓ= 1+ 1
2 + 1
3 +...+ 1
ℓ, which converges to ln(ℓ)+γ
(the Euler-Mascheroni constant γ) as ℓ→∞.
For t = 1, k > 1, ℓ> k, π(ℓ, k, 1, 1) = k + k(Hℓ−Hk)
or O(k log(ℓ)), as the ﬁrst k elements are in the successive
min-k, and the expectation for the rest is
k
k+1 +
k
k+2 +...+ k
ℓ.
For t > 1, k > 1, ℓ> k, note that there are some number
D, k ≤D ≤ℓof successive min-k determinations D made
for each possible {a1, ..., aℓ}. The number of won ballots for
each case is by deﬁnition ⌊D/t⌋, as the thread queue must
ﬁll up t times. Thus, π(ℓ, k, t, 1) = O(k log(ℓ)/t).
Multiple lanes.
The w > 1 case is complicated by the
fact that there are joint probabilities to consider (if more
than one of the w workers triggers a sort for a given group,
only one sort takes place). However, the likelihood can be
bounded. Let π′(ℓ, k, t, w) be the expected won ballots as-
suming no mutual interference between the w workers for
winning ballots (i.e., we win b ballots if there are b ≤w
workers that independently win a ballot at a single step),
but with the shared min-k set after each sort from the joint
sequence. Assume that k ≥w. Then:
π′(ℓ, k, 1, w) ≤w
  k
w

+
⌈ℓ/w⌉−⌈k/w⌉
X
i=1
k
w(⌈k/w⌉+ i)
!
≤wπ(⌈ℓ/w⌉, k, 1, 1) = O(wk log(ℓ/w))
(19)
where the likelihood of the w workers seeing a successive
min-k element has an upper bound of that of the ﬁrst worker
at each step. As before, the number of won ballots is scaled
by t, so π′(ℓ, k, t, w) = O(wk log(ℓ/w)/t). Mutual interfer-
ence can only reduce the number of ballots, so we obtain the
same upper bound for π(ℓ, k, t, w).
12

53.79798889160156 | 57.532100677490234 | 292.86004638671875 | 185.0540771484375 | 97-D(12):3142–3154, 2014.
[46] R. Weber, H.-J. Schek, and S. Blott. A quantitative
analysis and performance study for similarity-search
methods in high-dimensional spaces. In Proc. International
Conference on Very Large DataBases, pages 194–205, 1998.
[47] P. Wieschollek, O. Wang, A. Sorkine-Hornung, and
H. P. A. Lensch. Eﬃcient large-scale approximate nearest
neighbor search on the GPU. In Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pages
2027–2035, June 2016.
[48] S. Williams, A. Waterman, and D. Patterson. Rooﬂine: An
insightful visual performance model for multicore
architectures. Communications of the ACM, 52(4):65–76,
April 2009.
 |  | 
53.797996520996094 | 196.05401611328125 | 286.3022155761719 | 208.00921630859375 | Appendix: Complexity analysis of WarpSelect
 | 1 | 
53.7979736328125 | 212.66831970214844 | 292.9319152832031 | 316.7777099609375 | We derive the average number of times updates are triggered
in WarpSelect, for use in Section 4.3.
Let the input to k-selection be a sequence {a1, a2, ..., aℓ}
(1-based indexing), a randomly chosen permutation of a set
of distinct elements.
Elements are read sequentially in c
groups of size w (the warp; in our case, w = 32); assume ℓ
is a multiple of w, so c = ℓ/w. Recall that t is the thread
queue length.
We call elements prior to or at position n
in the min-k seen so far the successive min-k (at n). The
likelihood that an is in the successive min-k at n is:
 | 2 | 
115.21199035644531 | 336.9523010253906 | 155.29551696777344 | 345.918701171875 | α(n, k) :=
 | 3 | 
157.85498046875 | 328.0612487792969 | 292.9095458984375 | 352.4646911621094 | (
1
if n ≤k
k/n
if n > k
(13)
 | 4 | 
53.79798126220703 | 358.87030029296875 | 292.9225769042969 | 378.2976989746094 | as each an, n > k has a k/n chance as all permutations are
equally likely, and all elements in the ﬁrst k qualify.
 | 5 | 
53.797950744628906 | 383.7773132324219 | 292.9318542480469 | 434.5867004394531 | Counting the insertion sorts. In a given lane, an inser-
tion sort is triggered if the incoming value is in the successive
min-k + t values, but the lane has “seen” only wc0 + (c−c0)
values, where c0 is the previous won warp ballot. The prob-
ability of this happening is:
 | 6 | 
88.83094787597656 | 439.56829833984375 | 215.6024932861328 | 455.3297119140625 | α(wc0 + (c −c0), k + t) ≈k + t
 | 7 | 
200.36099243164062 | 445.3674621582031 | 292.9095458984375 | 460.41485595703125 | wc
for c > k.
(14)
 | 8 | 
53.79798889160156 | 463.6524658203125 | 292.91571044921875 | 493.5408630371094 | The approximation considers that the thread queue has seen
all the wc values, not just those assigned to its lane. The
probability of any lane triggering an insertion sort is then:
 | 9 | 
118.22999572753906 | 499.2054443359375 | 177.60153198242188 | 514.3739013671875 | 1 −

1 −k + t
 | 10 | 
162.35899353027344 | 511.4884338378906 | 173.19937133789062 | 520.454833984375 | wc
 | 11 | 
178.79798889160156 | 497.2043151855469 | 224.71353149414062 | 514.2576904296875 | w
≈k + t
 | 12 | 
212.8939971923828 | 505.4075012207031 | 292.9095764160156 | 520.454833984375 | c
.
(15)
 | 13 | 
53.79801940917969 | 526.1295166015625 | 292.9229431152344 | 556.897705078125 | Here the approximation is a ﬁrst-order Taylor expansion.
Summing up the probabilities over c gives an expected num-
ber of insertions of N2 ≈(k + t) log(c) = O(k log(ℓ/w)).
 | 14 | 
53.798004150390625 | 561.4965209960938 | 292.9056701660156 | 580.9238891601562 | Counting full sorts. We seek N3 = π(ℓ, k, t, w), the ex-
pected number of full sorts required for WarpSelect.
 | 15 | 
53.79795837402344 | 586.4035034179688 | 292.907470703125 | 647.6738891601562 | Single lane. For now, we assume w = 1, so c = ℓ. Let
γ(ℓ, m, k) be the probability that in an sequence {a1, ..., aℓ},
exactly m of the elements as encountered by a sequential
scanner (w = 1) are in the successive min-k. Given m, there
are
  ℓ
m

places where these successive min-k elements can
occur. It is given by a recurrence relation:
 | 16 | 
53.79795837402344 | 687.0265502929688 | 103.70049285888672 | 695.992919921875 | γ(ℓ, m, k) :=
 | 17 | 
106.26095581054688 | 661.616455078125 | 114.45624542236328 | 692.1018676757812 | 






 | 18 | 
106.26095581054688 | 699.2754516601562 | 114.45624542236328 | 721.69189453125 | 





 | 19 | 
114.45195770263672 | 662.1895751953125 | 292.9095153808594 | 732.8539428710938 | 1
ℓ= 0 and m = 0
0
ℓ= 0 and m > 0
0
ℓ> 0 and m = 0
(γ(ℓ−1, m −1, k) · α(ℓ, k)+
γ(ℓ−1, m, k) · (1 −α(ℓ, k)))
otherwise.
(16)
 | 20 | 
316.8118896484375 | 56.63737869262695 | 555.9279174804688 | 118.02400207519531 | The last case is the probability of: there is a ℓ−1 se-
quence with m −1 successive min-k elements preceding us,
and the current element is in the successive min-k, or the
current element is not in the successive min-k, m ones are
before us. We can then develop a recurrence relationship for
π(ℓ, k, t, 1). Note that
 | 21 | 
350.2868957519531 | 143.8766326904297 | 402.597412109375 | 152.84303283691406 | δ(ℓ, b, k, t) :=
 | 22 | 
405.1568908691406 | 134.22998046875 | 483.29205322265625 | 150.74595642089844 | min((bt+max(0,t−1)),ℓ)
X
 | 23 | 
435.09686279296875 | 143.87657165527344 | 555.9229125976562 | 162.7670440673828 | m=bt
γ(ℓ, m, k)
(17)
 | 24 | 
316.8118896484375 | 172.7683563232422 | 555.9349975585938 | 265.5379638671875 | for b where 0 ≤bt ≤ℓis the fraction of all sequences of
length ℓthat will force b sorts of data by winning the thread
queue ballot, as there have to be bt to (bt + max(0, t −1))
elements in the successive min-k for these sorts to happen (as
the min-k elements will overﬂow the thread queues). There
are at most ⌊ℓ/t⌋won ballots that can occur, as it takes t
separate sequential current min-k seen elements to win the
ballot.
π(ℓ, k, t, 1) is thus the expectation of this over all
possible b:
 | 25 | 
373.6169128417969 | 291.39056396484375 | 425.1917724609375 | 300.3569641113281 | π(ℓ, k, t, 1) =
 | 26 | 
427.75592041015625 | 281.74395751953125 | 444.3870849609375 | 298.2599182128906 | ⌊ℓ/t⌋
X
 | 27 | 
429.81292724609375 | 291.2743835449219 | 555.9224243164062 | 310.281005859375 | b=1
b · δ(ℓ, b, k, t).
(18)
 | 28 | 
316.8119201660156 | 316.16455078125 | 555.9638671875 | 346.46197509765625 | This can be computed by dynamic programming. Analyti-
cally, note that for t = 1, k = 1, π(ℓ, 1, 1, 1) is the harmonic
number Hℓ= 1+ 1
 | 29 | 
388.2879943847656 | 335.748291015625 | 407.122314453125 | 349.0948486328125 | 2 + 1
 | 30 | 
403.4700012207031 | 335.748291015625 | 439.1223449707031 | 349.0948486328125 | 3 +...+ 1
 | 31 | 
316.8119812011719 | 337.08544921875 | 555.4051513671875 | 356.5128479003906 | ℓ, which converges to ln(ℓ)+γ
(the Euler-Mascheroni constant γ) as ℓ→∞.
 | 32 | 
316.81201171875 | 368.35125732421875 | 555.9185791015625 | 401.39886474609375 | For t = 1, k > 1, ℓ> k, π(ℓ, k, 1, 1) = k + k(Hℓ−Hk)
or O(k log(ℓ)), as the ﬁrst k elements are in the successive
min-k, and the expectation for the rest is
k
k+1 +
k
k+2 +...+ k
 | 33 | 
548.572998046875 | 389.38946533203125 | 555.9193725585938 | 400.74591064453125 | ℓ.
 | 34 | 
316.81195068359375 | 410.31146240234375 | 555.9274291992188 | 461.120849609375 | For t > 1, k > 1, ℓ> k, note that there are some number
D, k ≤D ≤ℓof successive min-k determinations D made
for each possible {a1, ..., aℓ}. The number of won ballots for
each case is by deﬁnition ⌊D/t⌋, as the thread queue must
ﬁll up t times. Thus, π(ℓ, k, t, 1) = O(k log(ℓ)/t).
 | 35 | 
316.81201171875 | 468.5924377441406 | 555.9638061523438 | 571.705810546875 | Multiple lanes.
The w > 1 case is complicated by the
fact that there are joint probabilities to consider (if more
than one of the w workers triggers a sort for a given group,
only one sort takes place). However, the likelihood can be
bounded. Let π′(ℓ, k, t, w) be the expected won ballots as-
suming no mutual interference between the w workers for
winning ballots (i.e., we win b ballots if there are b ≤w
workers that independently win a ballot at a single step),
but with the shared min-k set after each sort from the joint
sequence. Assume that k ≥w. Then:
 | 36 | 
328.6700439453125 | 595.5098266601562 | 395.62030029296875 | 606.5248413085938 | π′(ℓ, k, 1, w) ≤w
 | 37 | 
395.8570556640625 | 584.9364624023438 | 416.9490966796875 | 600.726806640625 |   k
 | 38 | 
411.260009765625 | 603.6405029296875 | 417.8682556152344 | 612.6068725585938 | w
 | 39 | 
419.301025390625 | 591.3573608398438 | 433.8891906738281 | 606.5248413085938 | 
+
 | 40 | 
435.9400329589844 | 587.911865234375 | 481.2351989746094 | 604.4277954101562 | ⌈ℓ/w⌉−⌈k/w⌉
X
 | 41 | 
452.634033203125 | 610.2813110351562 | 464.5393981933594 | 616.2589111328125 | i=1
 | 42 | 
483.9670104980469 | 591.7594604492188 | 537.1055908203125 | 612.6068725585938 | k
w(⌈k/w⌉+ i)
 | 43 | 
538.2980346679688 | 588.6674194335938 | 545.5966796875 | 597.6338500976562 | !
 | 44 | 
316.8121032714844 | 620.1322631835938 | 555.941650390625 | 702.8388671875 | ≤wπ(⌈ℓ/w⌉, k, 1, 1) = O(wk log(ℓ/w))
(19)
where the likelihood of the w workers seeing a successive
min-k element has an upper bound of that of the ﬁrst worker
at each step. As before, the number of won ballots is scaled
by t, so π′(ℓ, k, t, w) = O(wk log(ℓ/w)/t). Mutual interfer-
ence can only reduce the number of ballots, so we obtain the
same upper bound for π(ℓ, k, t, w).
 | 45 | 
300.2491455078125 | 740.1904907226562 | 309.46661376953125 | 749.1568603515625 | 12
 | 46 | 



### Extracted from Dense Passage Retrieval for Open-Domain Question Answering.pdf ###

Dense Passage Retrieval for Open-Domain Question Answering
Vladimir Karpukhin∗, Barlas O˘guz∗, Sewon Min†, Patrick Lewis,
Ledell Wu, Sergey Edunov, Danqi Chen‡, Wen-tau Yih
Facebook AI
†University of Washington
‡Princeton University
{vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com
sewon@cs.washington.edu
danqic@cs.princeton.edu
Abstract
Open-domain question answering relies on ef-
ﬁcient passage retrieval to select candidate
contexts, where traditional sparse vector space
models, such as TF-IDF or BM25, are the de
facto method.
In this work, we show that
retrieval can be practically implemented us-
ing dense representations alone, where em-
beddings are learned from a small number
of questions and passages by a simple dual-
encoder framework.
When evaluated on a
wide range of open-domain QA datasets, our
dense retriever outperforms a strong Lucene-
BM25 system greatly by 9%-19% absolute in
terms of top-20 passage retrieval accuracy, and
helps our end-to-end QA system establish new
state-of-the-art on multiple open-domain QA
benchmarks.1
1
Introduction
Open-domain question answering (QA) (Voorhees,
1999) is a task that answers factoid questions us-
ing a large collection of documents. While early
QA systems are often complicated and consist of
multiple components (Ferrucci (2012); Moldovan
et al. (2003), inter alia), the advances of reading
comprehension models suggest a much simpliﬁed
two-stage framework: (1) a context retriever ﬁrst
selects a small subset of passages where some
of them contain the answer to the question, and
then (2) a machine reader can thoroughly exam-
ine the retrieved contexts and identify the correct
answer (Chen et al., 2017). Although reducing
open-domain QA to machine reading is a very rea-
sonable strategy, a huge performance degradation
is often observed in practice2, indicating the needs
of improving retrieval.
∗Equal contribution
1The code and trained models have been released at
https://github.com/facebookresearch/DPR.
2For instance, the exact match score on SQuAD v1.1 drops
from above 80% to less than 40% (Yang et al., 2019a).
Retrieval in open-domain QA is usually imple-
mented using TF-IDF or BM25 (Robertson and
Zaragoza, 2009), which matches keywords efﬁ-
ciently with an inverted index and can be seen
as representing the question and context in high-
dimensional, sparse vectors (with weighting). Con-
versely, the dense, latent semantic encoding is com-
plementary to sparse representations by design. For
example, synonyms or paraphrases that consist of
completely different tokens may still be mapped to
vectors close to each other. Consider the question
“Who is the bad guy in lord of the rings?”, which can
be answered from the context “Sala Baker is best
known for portraying the villain Sauron in the Lord
of the Rings trilogy.” A term-based system would
have difﬁculty retrieving such a context, while
a dense retrieval system would be able to better
match “bad guy” with “villain” and fetch the cor-
rect context. Dense encodings are also learnable
by adjusting the embedding functions, which pro-
vides additional ﬂexibility to have a task-speciﬁc
representation. With special in-memory data struc-
tures and indexing schemes, retrieval can be done
efﬁciently using maximum inner product search
(MIPS) algorithms (e.g., Shrivastava and Li (2014);
Guo et al. (2016)).
However, it is generally believed that learn-
ing a good dense vector representation needs a
large number of labeled pairs of question and con-
texts. Dense retrieval methods have thus never
be shown to outperform TF-IDF/BM25 for open-
domain QA before ORQA (Lee et al., 2019), which
proposes a sophisticated inverse cloze task (ICT)
objective, predicting the blocks that contain the
masked sentence, for additional pretraining. The
question encoder and the reader model are then ﬁne-
tuned using pairs of questions and answers jointly.
Although ORQA successfully demonstrates that
dense retrieval can outperform BM25, setting new
state-of-the-art results on multiple open-domain
arXiv:2004.04906v3  [cs.CL]  30 Sep 2020

104.74800109863281 | 70.6820068359375 | 492.7985534667969 | 85.02820587158203 | Dense Passage Retrieval for Open-Domain Question Answering
 |  | 
97.02801513671875 | 104.8498306274414 | 503.5038757324219 | 187.7554473876953 | Vladimir Karpukhin∗, Barlas O˘guz∗, Sewon Min†, Patrick Lewis,
Ledell Wu, Sergey Edunov, Danqi Chen‡, Wen-tau Yih
Facebook AI
†University of Washington
‡Princeton University
{vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com
sewon@cs.washington.edu
danqic@cs.princeton.edu
 | 1 | 
158.8909912109375 | 224.32415771484375 | 203.37628173828125 | 236.27935791015625 | Abstract
 | 2 | 
88.64899444580078 | 245.9064483642578 | 274.9155578613281 | 447.1520080566406 | Open-domain question answering relies on ef-
ﬁcient passage retrieval to select candidate
contexts, where traditional sparse vector space
models, such as TF-IDF or BM25, are the de
facto method.
In this work, we show that
retrieval can be practically implemented us-
ing dense representations alone, where em-
beddings are learned from a small number
of questions and passages by a simple dual-
encoder framework.
When evaluated on a
wide range of open-domain QA datasets, our
dense retriever outperforms a strong Lucene-
BM25 system greatly by 9%-19% absolute in
terms of top-20 passage retrieval accuracy, and
helps our end-to-end QA system establish new
state-of-the-art on multiple open-domain QA
benchmarks.1
 | 3 | 
72.0 | 457.52410888671875 | 154.8136749267578 | 469.47930908203125 | 1
Introduction
 | 4 | 
71.18199920654297 | 479.0426330566406 | 292.0834655761719 | 706.6847534179688 | Open-domain question answering (QA) (Voorhees,
1999) is a task that answers factoid questions us-
ing a large collection of documents. While early
QA systems are often complicated and consist of
multiple components (Ferrucci (2012); Moldovan
et al. (2003), inter alia), the advances of reading
comprehension models suggest a much simpliﬁed
two-stage framework: (1) a context retriever ﬁrst
selects a small subset of passages where some
of them contain the answer to the question, and
then (2) a machine reader can thoroughly exam-
ine the retrieved contexts and identify the correct
answer (Chen et al., 2017). Although reducing
open-domain QA to machine reading is a very rea-
sonable strategy, a huge performance degradation
is often observed in practice2, indicating the needs
of improving retrieval.
 | 5 | 
72.0 | 712.73388671875 | 290.2686462402344 | 765.1323852539062 | ∗Equal contribution
1The code and trained models have been released at
https://github.com/facebookresearch/DPR.
2For instance, the exact match score on SQuAD v1.1 drops
from above 80% to less than 40% (Yang et al., 2019a).
 | 6 | 
303.0320129394531 | 225.1443328857422 | 527.4561157226562 | 765.6279907226562 | Retrieval in open-domain QA is usually imple-
mented using TF-IDF or BM25 (Robertson and
Zaragoza, 2009), which matches keywords efﬁ-
ciently with an inverted index and can be seen
as representing the question and context in high-
dimensional, sparse vectors (with weighting). Con-
versely, the dense, latent semantic encoding is com-
plementary to sparse representations by design. For
example, synonyms or paraphrases that consist of
completely different tokens may still be mapped to
vectors close to each other. Consider the question
“Who is the bad guy in lord of the rings?”, which can
be answered from the context “Sala Baker is best
known for portraying the villain Sauron in the Lord
of the Rings trilogy.” A term-based system would
have difﬁculty retrieving such a context, while
a dense retrieval system would be able to better
match “bad guy” with “villain” and fetch the cor-
rect context. Dense encodings are also learnable
by adjusting the embedding functions, which pro-
vides additional ﬂexibility to have a task-speciﬁc
representation. With special in-memory data struc-
tures and indexing schemes, retrieval can be done
efﬁciently using maximum inner product search
(MIPS) algorithms (e.g., Shrivastava and Li (2014);
Guo et al. (2016)).
However, it is generally believed that learn-
ing a good dense vector representation needs a
large number of labeled pairs of question and con-
texts. Dense retrieval methods have thus never
be shown to outperform TF-IDF/BM25 for open-
domain QA before ORQA (Lee et al., 2019), which
proposes a sophisticated inverse cloze task (ICT)
objective, predicting the blocks that contain the
masked sentence, for additional pretraining. The
question encoder and the reader model are then ﬁne-
tuned using pairs of questions and answers jointly.
Although ORQA successfully demonstrates that
dense retrieval can outperform BM25, setting new
state-of-the-art results on multiple open-domain
 | 7 | 
10.940000534057617 | 263.25 | 37.619998931884766 | 609.8900146484375 | arXiv:2004.04906v3  [cs.CL]  30 Sep 2020
 | 8 | 
QA datasets, it also suffers from two weaknesses.
First, ICT pretraining is computationally intensive
and it is not completely clear that regular sentences
are good surrogates of questions in the objective
function. Second, because the context encoder is
not ﬁne-tuned using pairs of questions and answers,
the corresponding representations could be subop-
timal.
In this paper, we address the question: can we
train a better dense embedding model using only
pairs of questions and passages (or answers), with-
out additional pretraining? By leveraging the now
standard BERT pretrained model (Devlin et al.,
2019) and a dual-encoder architecture (Bromley
et al., 1994), we focus on developing the right
training scheme using a relatively small number
of question and passage pairs. Through a series
of careful ablation studies, our ﬁnal solution is
surprisingly simple: the embedding is optimized
for maximizing inner products of the question and
relevant passage vectors, with an objective compar-
ing all pairs of questions and passages in a batch.
Our Dense Passage Retriever (DPR) is exception-
ally strong. It not only outperforms BM25 by a
large margin (65.2% vs. 42.9% in Top-5 accuracy),
but also results in a substantial improvement on
the end-to-end QA accuracy compared to ORQA
(41.5% vs. 33.3%) in the open Natural Questions
setting (Lee et al., 2019; Kwiatkowski et al., 2019).
Our contributions are twofold. First, we demon-
strate that with the proper training setup, sim-
ply ﬁne-tuning the question and passage encoders
on existing question-passage pairs is sufﬁcient to
greatly outperform BM25. Our empirical results
also suggest that additional pretraining may not be
needed. Second, we verify that, in the context of
open-domain question answering, a higher retrieval
precision indeed translates to a higher end-to-end
QA accuracy. By applying a modern reader model
to the top retrieved passages, we achieve compara-
ble or better results on multiple QA datasets in the
open-retrieval setting, compared to several, much
complicated systems.
2
Background
The problem of open-domain QA studied in this
paper can be described as follows. Given a factoid
question, such as “Who ﬁrst voiced Meg on Family
Guy?” or “Where was the 8th Dalai Lama born?”, a
system is required to answer it using a large corpus
of diversiﬁed topics. More speciﬁcally, we assume
the extractive QA setting, in which the answer is
restricted to a span appearing in one or more pas-
sages in the corpus. Assume that our collection
contains D documents, d1, d2, · · · , dD. We ﬁrst
split each of the documents into text passages of
equal lengths as the basic retrieval units3 and get M
total passages in our corpus C = {p1, p2, . . . , pM},
where each passage pi can be viewed as a sequence
of tokens w(i)
1 , w(i)
2 , · · · , w(i)
|pi|. Given a question q,
the task is to ﬁnd a span w(i)
s , w(i)
s+1, · · · , w(i)
e
from
one of the passages pi that can answer the question.
Notice that to cover a wide variety of domains, the
corpus size can easily range from millions of docu-
ments (e.g., Wikipedia) to billions (e.g., the Web).
As a result, any open-domain QA system needs to
include an efﬁcient retriever component that can se-
lect a small set of relevant texts, before applying the
reader to extract the answer (Chen et al., 2017).4
Formally speaking, a retriever R : (q, C) →CF
is a function that takes as input a question q and a
corpus C and returns a much smaller ﬁlter set of
texts CF ⊂C, where |CF| = k ≪|C|. For a ﬁxed
k, a retriever can be evaluated in isolation on top-k
retrieval accuracy, which is the fraction of ques-
tions for which CF contains a span that answers the
question.
3
Dense Passage Retriever (DPR)
We focus our research in this work on improv-
ing the retrieval component in open-domain QA.
Given a collection of M text passages, the goal of
our dense passage retriever (DPR) is to index all
the passages in a low-dimensional and continuous
space, such that it can retrieve efﬁciently the top
k passages relevant to the input question for the
reader at run-time. Note that M can be very large
(e.g., 21 million passages in our experiments, de-
scribed in Section 4.1) and k is usually small, such
as 20–100.
3.1
Overview
Our dense passage retriever (DPR) uses a dense
encoder EP (·) which maps any text passage to a d-
dimensional real-valued vectors and builds an index
for all the M passages that we will use for retrieval.
3The ideal size and boundary of a text passage are func-
tions of both the retriever and reader. We also experimented
with natural paragraphs in our preliminary trials and found that
using ﬁxed-length passages performs better in both retrieval
and ﬁnal QA accuracy, as observed by Wang et al. (2019).
4Exceptions include (Seo et al., 2019) and (Roberts et al.,
2020), which retrieves and generates the answers, respectively.

71.63999938964844 | 65.41136169433594 | 292.1755676269531 | 647.4107666015625 | QA datasets, it also suffers from two weaknesses.
First, ICT pretraining is computationally intensive
and it is not completely clear that regular sentences
are good surrogates of questions in the objective
function. Second, because the context encoder is
not ﬁne-tuned using pairs of questions and answers,
the corresponding representations could be subop-
timal.
In this paper, we address the question: can we
train a better dense embedding model using only
pairs of questions and passages (or answers), with-
out additional pretraining? By leveraging the now
standard BERT pretrained model (Devlin et al.,
2019) and a dual-encoder architecture (Bromley
et al., 1994), we focus on developing the right
training scheme using a relatively small number
of question and passage pairs. Through a series
of careful ablation studies, our ﬁnal solution is
surprisingly simple: the embedding is optimized
for maximizing inner products of the question and
relevant passage vectors, with an objective compar-
ing all pairs of questions and passages in a batch.
Our Dense Passage Retriever (DPR) is exception-
ally strong. It not only outperforms BM25 by a
large margin (65.2% vs. 42.9% in Top-5 accuracy),
but also results in a substantial improvement on
the end-to-end QA accuracy compared to ORQA
(41.5% vs. 33.3%) in the open Natural Questions
setting (Lee et al., 2019; Kwiatkowski et al., 2019).
Our contributions are twofold. First, we demon-
strate that with the proper training setup, sim-
ply ﬁne-tuning the question and passage encoders
on existing question-passage pairs is sufﬁcient to
greatly outperform BM25. Our empirical results
also suggest that additional pretraining may not be
needed. Second, we verify that, in the context of
open-domain question answering, a higher retrieval
precision indeed translates to a higher end-to-end
QA accuracy. By applying a modern reader model
to the top retrieved passages, we achieve compara-
ble or better results on multiple QA datasets in the
open-retrieval setting, compared to several, much
complicated systems.
 |  | 
72.0 | 662.7261352539062 | 152.82911682128906 | 674.6813354492188 | 2
Background
 | 1 | 
71.60700225830078 | 686.8643798828125 | 290.27313232421875 | 765.5858764648438 | The problem of open-domain QA studied in this
paper can be described as follows. Given a factoid
question, such as “Who ﬁrst voiced Meg on Family
Guy?” or “Where was the 8th Dalai Lama born?”, a
system is required to answer it using a large corpus
of diversiﬁed topics. More speciﬁcally, we assume
 | 2 | 
306.88299560546875 | 65.41136169433594 | 527.3512573242188 | 190.18397521972656 | the extractive QA setting, in which the answer is
restricted to a span appearing in one or more pas-
sages in the corpus. Assume that our collection
contains D documents, d1, d2, · · · , dD. We ﬁrst
split each of the documents into text passages of
equal lengths as the basic retrieval units3 and get M
total passages in our corpus C = {p1, p2, . . . , pM},
where each passage pi can be viewed as a sequence
of tokens w(i)
1 , w(i)
2 , · · · , w(i)
|pi|. Given a question q,
 | 3 | 
306.88299560546875 | 190.400146484375 | 527.4571533203125 | 313.43499755859375 | the task is to ﬁnd a span w(i)
s , w(i)
s+1, · · · , w(i)
e
from
one of the passages pi that can answer the question.
Notice that to cover a wide variety of domains, the
corpus size can easily range from millions of docu-
ments (e.g., Wikipedia) to billions (e.g., the Web).
As a result, any open-domain QA system needs to
include an efﬁcient retriever component that can se-
lect a small set of relevant texts, before applying the
reader to extract the answer (Chen et al., 2017).4
 | 4 | 
307.2760009765625 | 315.65484619140625 | 527.3558959960938 | 421.8017578125 | Formally speaking, a retriever R : (q, C) →CF
is a function that takes as input a question q and a
corpus C and returns a much smaller ﬁlter set of
texts CF ⊂C, where |CF| = k ≪|C|. For a ﬁxed
k, a retriever can be evaluated in isolation on top-k
retrieval accuracy, which is the fraction of ques-
tions for which CF contains a span that answers the
question.
 | 5 | 
307.2760009765625 | 434.48016357421875 | 485.1335144042969 | 446.43536376953125 | 3
Dense Passage Retriever (DPR)
 | 6 | 
306.76300048828125 | 456.7313232421875 | 527.4519653320312 | 603.2147216796875 | We focus our research in this work on improv-
ing the retrieval component in open-domain QA.
Given a collection of M text passages, the goal of
our dense passage retriever (DPR) is to index all
the passages in a low-dimensional and continuous
space, such that it can retrieve efﬁciently the top
k passages relevant to the input question for the
reader at run-time. Note that M can be very large
(e.g., 21 million passages in our experiments, de-
scribed in Section 4.1) and k is usually small, such
as 20–100.
 | 7 | 
307.2760009765625 | 615.3161010742188 | 376.43963623046875 | 626.2251586914062 | 3.1
Overview
 | 8 | 
307.2760009765625 | 633.225341796875 | 527.4560546875 | 684.8372802734375 | Our dense passage retriever (DPR) uses a dense
encoder EP (·) which maps any text passage to a d-
dimensional real-valued vectors and builds an index
for all the M passages that we will use for retrieval.
 | 9 | 
306.9530029296875 | 693.9356689453125 | 527.0357055664062 | 765.110595703125 | 3The ideal size and boundary of a text passage are func-
tions of both the retriever and reader. We also experimented
with natural paragraphs in our preliminary trials and found that
using ﬁxed-length passages performs better in both retrieval
and ﬁnal QA accuracy, as observed by Wang et al. (2019).
4Exceptions include (Seo et al., 2019) and (Roberts et al.,
2020), which retrieves and generates the answers, respectively.
 | 10 | 
At run-time, DPR applies a different encoder EQ(·)
that maps the input question to a d-dimensional
vector, and retrieves k passages of which vectors
are the closest to the question vector. We deﬁne
the similarity between the question and the passage
using the dot product of their vectors:
sim(q, p) = EQ(q)⊺EP (p).
(1)
Although more expressive model forms for measur-
ing the similarity between a question and a passage
do exist, such as networks consisting of multiple
layers of cross attentions, the similarity function
needs to be decomposable so that the represen-
tations of the collection of passages can be pre-
computed. Most decomposable similarity functions
are some transformations of Euclidean distance
(L2). For instance, cosine is equivalent to inner
product for unit vectors and the Mahalanobis dis-
tance is equivalent to L2 distance in a transformed
space. Inner product search has been widely used
and studied, as well as its connection to cosine
similarity and L2 distance (Mussmann and Ermon,
2016; Ram and Gray, 2012). As our ablation study
ﬁnds other similarity functions perform compara-
bly (Section 5.2; Appendix B), we thus choose
the simpler inner product function and improve the
dense passage retriever by learning better encoders.
Encoders
Although in principle the question and
passage encoders can be implemented by any neu-
ral networks, in this work we use two independent
BERT (Devlin et al., 2019) networks (base, un-
cased) and take the representation at the [CLS]
token as the output, so d = 768.
Inference
During inference time, we apply the
passage encoder EP to all the passages and index
them using FAISS (Johnson et al., 2017) ofﬂine.
FAISS is an extremely efﬁcient, open-source li-
brary for similarity search and clustering of dense
vectors, which can easily be applied to billions of
vectors. Given a question q at run-time, we derive
its embedding vq = EQ(q) and retrieve the top k
passages with embeddings closest to vq.
3.2
Training
Training the encoders so that the dot-product sim-
ilarity (Eq. (1)) becomes a good ranking function
for retrieval is essentially a metric learning prob-
lem (Kulis, 2013). The goal is to create a vector
space such that relevant pairs of questions and pas-
sages will have smaller distance (i.e., higher simi-
larity) than the irrelevant ones, by learning a better
embedding function.
Let D = {⟨qi, p+
i , p−
i,1, · · · , p−
i,n⟩}m
i=1 be the
training data that consists of m instances. Each
instance contains one question qi and one relevant
(positive) passage p+
i , along with n irrelevant (neg-
ative) passages p−
i,j. We optimize the loss function
as the negative log likelihood of the positive pas-
sage:
L(qi, p+
i , p−
i,1, · · · , p−
i,n)
(2)
=
−log
esim(qi,p+
i )
esim(qi,p+
i ) + Pn
j=1 esim(qi,p−
i,j) .
Positive and negative passages
For retrieval
problems, it is often the case that positive examples
are available explicitly, while negative examples
need to be selected from an extremely large pool.
For instance, passages relevant to a question may
be given in a QA dataset, or can be found using the
answer. All other passages in the collection, while
not speciﬁed explicitly, can be viewed as irrelevant
by default. In practice, how to select negative ex-
amples is often overlooked but could be decisive
for learning a high-quality encoder. We consider
three different types of negatives: (1) Random: any
random passage from the corpus; (2) BM25: top
passages returned by BM25 which don’t contain
the answer but match most question tokens; (3)
Gold: positive passages paired with other questions
which appear in the training set. We will discuss the
impact of different types of negative passages and
training schemes in Section 5.2. Our best model
uses gold passages from the same mini-batch and
one BM25 negative passage. In particular, re-using
gold passages from the same batch as negatives
can make the computation efﬁcient while achiev-
ing great performance. We discuss this approach
below.
In-batch negatives
Assume that we have B
questions in a mini-batch and each one is asso-
ciated with a relevant passage. Let Q and P be the
(B×d) matrix of question and passage embeddings
in a batch of size B. S = QPT is a (B × B) ma-
trix of similarity scores, where each row of which
corresponds to a question, paired with B passages.
In this way, we reuse computation and effectively
train on B2 (qi, pj) question/passage pairs in each
batch. Any (qi, pj) pair is a positive example when
i = j, and negative otherwise. This creates B train-
ing instances in each batch, where there are B −1

71.60700225830078 | 65.09983825683594 | 291.545654296875 | 144.1487579345703 | At run-time, DPR applies a different encoder EQ(·)
that maps the input question to a d-dimensional
vector, and retrieves k passages of which vectors
are the closest to the question vector. We deﬁne
the similarity between the question and the passage
using the dot product of their vectors:
 |  | 
120.61500549316406 | 154.03289794921875 | 290.2679748535156 | 170.579345703125 | sim(q, p) = EQ(q)⊺EP (p).
(1)
 | 1 | 
71.60700225830078 | 183.31182861328125 | 292.179443359375 | 437.99627685546875 | Although more expressive model forms for measur-
ing the similarity between a question and a passage
do exist, such as networks consisting of multiple
layers of cross attentions, the similarity function
needs to be decomposable so that the represen-
tations of the collection of passages can be pre-
computed. Most decomposable similarity functions
are some transformations of Euclidean distance
(L2). For instance, cosine is equivalent to inner
product for unit vectors and the Mahalanobis dis-
tance is equivalent to L2 distance in a transformed
space. Inner product search has been widely used
and studied, as well as its connection to cosine
similarity and L2 distance (Mussmann and Ermon,
2016; Ram and Gray, 2012). As our ablation study
ﬁnds other similarity functions perform compara-
bly (Section 5.2; Appendix B), we thus choose
the simpler inner product function and improve the
dense passage retriever by learning better encoders.
 | 2 | 
72.0 | 448.4880676269531 | 292.0801696777344 | 527.2427978515625 | Encoders
Although in principle the question and
passage encoders can be implemented by any neu-
ral networks, in this work we use two independent
BERT (Devlin et al., 2019) networks (base, un-
cased) and take the representation at the [CLS]
token as the output, so d = 768.
 | 3 | 
71.72699737548828 | 537.7081298828125 | 292.175537109375 | 657.8522338867188 | Inference
During inference time, we apply the
passage encoder EP to all the passages and index
them using FAISS (Johnson et al., 2017) ofﬂine.
FAISS is an extremely efﬁcient, open-source li-
brary for similarity search and clustering of dense
vectors, which can easily be applied to billions of
vectors. Given a question q at run-time, we derive
its embedding vq = EQ(q) and retrieve the top k
passages with embeddings closest to vq.
 | 4 | 
71.99998474121094 | 669.049072265625 | 136.9636688232422 | 679.9581298828125 | 3.2
Training
 | 5 | 
71.66200256347656 | 686.897216796875 | 292.0826110839844 | 765.6175537109375 | Training the encoders so that the dot-product sim-
ilarity (Eq. (1)) becomes a good ranking function
for retrieval is essentially a metric learning prob-
lem (Kulis, 2013). The goal is to create a vector
space such that relevant pairs of questions and pas-
sages will have smaller distance (i.e., higher simi-
 | 6 | 
306.9159851074219 | 65.54347229003906 | 527.3511962890625 | 184.79676818847656 | larity) than the irrelevant ones, by learning a better
embedding function.
Let D = {⟨qi, p+
i , p−
i,1, · · · , p−
i,n⟩}m
i=1 be the
training data that consists of m instances. Each
instance contains one question qi and one relevant
(positive) passage p+
i , along with n irrelevant (neg-
ative) passages p−
i,j. We optimize the loss function
as the negative log likelihood of the positive pas-
sage:
 | 7 | 
347.92999267578125 | 194.92172241210938 | 525.5430297851562 | 211.4822998046875 | L(qi, p+
i , p−
i,1, · · · , p−
i,n)
(2)
 | 8 | 
329.48199462890625 | 213.7812042236328 | 463.98565673828125 | 235.75018310546875 | =
−log
esim(qi,p+
i )
 | 9 | 
375.3370056152344 | 224.84107971191406 | 513.3016967773438 | 254.7742156982422 | esim(qi,p+
i ) + Pn
j=1 esim(qi,p−
i,j) .
 | 10 | 
306.88299560546875 | 259.5939025878906 | 527.451416015625 | 595.7837524414062 | Positive and negative passages
For retrieval
problems, it is often the case that positive examples
are available explicitly, while negative examples
need to be selected from an extremely large pool.
For instance, passages relevant to a question may
be given in a QA dataset, or can be found using the
answer. All other passages in the collection, while
not speciﬁed explicitly, can be viewed as irrelevant
by default. In practice, how to select negative ex-
amples is often overlooked but could be decisive
for learning a high-quality encoder. We consider
three different types of negatives: (1) Random: any
random passage from the corpus; (2) BM25: top
passages returned by BM25 which don’t contain
the answer but match most question tokens; (3)
Gold: positive passages paired with other questions
which appear in the training set. We will discuss the
impact of different types of negative passages and
training schemes in Section 5.2. Our best model
uses gold passages from the same mini-batch and
one BM25 negative passage. In particular, re-using
gold passages from the same batch as negatives
can make the computation efﬁcient while achiev-
ing great performance. We discuss this approach
below.
 | 11 | 
305.9989929199219 | 605.3992309570312 | 527.4526977539062 | 765.5964965820312 | In-batch negatives
Assume that we have B
questions in a mini-batch and each one is asso-
ciated with a relevant passage. Let Q and P be the
(B×d) matrix of question and passage embeddings
in a batch of size B. S = QPT is a (B × B) ma-
trix of similarity scores, where each row of which
corresponds to a question, paired with B passages.
In this way, we reuse computation and effectively
train on B2 (qi, pj) question/passage pairs in each
batch. Any (qi, pj) pair is a positive example when
i = j, and negative otherwise. This creates B train-
ing instances in each batch, where there are B −1
 | 12 | 
negative passages for each question.
The trick of in-batch negatives has been used in
the full batch setting (Yih et al., 2011) and more
recently for mini-batch (Henderson et al., 2017;
Gillick et al., 2019). It has been shown to be an
effective strategy for learning a dual-encoder model
that boosts the number of training examples.
4
Experimental Setup
In this section, we describe the data we used for
experiments and the basic setup.
4.1
Wikipedia Data Pre-processing
Following (Lee et al., 2019), we use the English
Wikipedia dump from Dec. 20, 2018 as the source
documents for answering questions. We ﬁrst apply
the pre-processing code released in DrQA (Chen
et al., 2017) to extract the clean, text-portion of
articles from the Wikipedia dump. This step re-
moves semi-structured data, such as tables, info-
boxes, lists, as well as the disambiguation pages.
We then split each article into multiple, disjoint text
blocks of 100 words as passages, serving as our
basic retrieval units, following (Wang et al., 2019),
which results in 21,015,324 passages in the end.5
Each passage is also prepended with the title of the
Wikipedia article where the passage is from, along
with an [SEP] token.
4.2
Question Answering Datasets
We use the same ﬁve QA datasets and train-
ing/dev/testing splitting method as in previous
work (Lee et al., 2019). Below we brieﬂy describe
each dataset and refer readers to their paper for the
details of data preparation.
Natural Questions (NQ) (Kwiatkowski et al.,
2019) was designed for end-to-end question an-
swering.
The questions were mined from real
Google search queries and the answers were spans
in Wikipedia articles identiﬁed by annotators.
TriviaQA (Joshi et al., 2017) contains a set of trivia
questions with answers that were originally scraped
from the Web.
WebQuestions (WQ) (Berant et al., 2013) consists
of questions selected using Google Suggest API,
where the answers are entities in Freebase.
CuratedTREC (TREC) (Baudiˇs and ˇSediv`y,
2015) sources questions from TREC QA tracks
5However, Wang et al. (2019) also propose splitting docu-
ments into overlapping passages, which we do not ﬁnd advan-
tageous compared to the non-overlapping version.
Dataset
Train
Dev
Test
Natural Questions
79,168
58,880
8,757
3,610
TriviaQA
78,785
60,413
8,837
11,313
WebQuestions
3,417
2,474
361
2,032
CuratedTREC
1,353
1,125
133
694
SQuAD
78,713
70,096
8,886
10,570
Table 1: Number of questions in each QA dataset. The
two columns of Train denote the original training ex-
amples in the dataset and the actual questions used for
training DPR after ﬁltering. See text for more details.
as well as various Web sources and is intended for
open-domain QA from unstructured corpora.
SQuAD v1.1 (Rajpurkar et al., 2016) is a popu-
lar benchmark dataset for reading comprehension.
Annotators were presented with a Wikipedia para-
graph, and asked to write questions that could be
answered from the given text. Although SQuAD
has been used previously for open-domain QA re-
search, it is not ideal because many questions lack
context in absence of the provided paragraph. We
still include it in our experiments for providing
a fair comparison to previous work and we will
discuss more in Section 5.1.
Selection of positive passages
Because only
pairs of questions and answers are provided in
TREC, WebQuestions and TriviaQA6, we use the
highest-ranked passage from BM25 that contains
the answer as the positive passage. If none of the
top 100 retrieved passages has the answer, the ques-
tion will be discarded. For SQuAD and Natural
Questions, since the original passages have been
split and processed differently than our pool of
candidate passages, we match and replace each
gold passage with the corresponding passage in the
candidate pool.7 We discard the questions when
the matching is failed due to different Wikipedia
versions or pre-processing. Table 1 shows the num-
ber of questions in training/dev/test sets for all the
datasets and the actual questions used for training
the retriever.
5
Experiments: Passage Retrieval
In this section, we evaluate the retrieval perfor-
mance of our Dense Passage Retriever (DPR),
along with analysis on how its output differs from
6We use the unﬁltered TriviaQA version and discard the
noisy evidence documents mined from Bing.
7The improvement of using gold contexts over passages
that contain answers is small.
See Section 5.2 and Ap-
pendix A.

72.0 | 65.49368286132812 | 291.1741027832031 | 158.1157989501953 | negative passages for each question.
The trick of in-batch negatives has been used in
the full batch setting (Yih et al., 2011) and more
recently for mini-batch (Henderson et al., 2017;
Gillick et al., 2019). It has been shown to be an
effective strategy for learning a dual-encoder model
that boosts the number of training examples.
 |  | 
72.0 | 171.51617431640625 | 191.88674926757812 | 183.47137451171875 | 4
Experimental Setup
 | 1 | 
72.0 | 194.2833709716797 | 290.4507751464844 | 218.82374572753906 | In this section, we describe the data we used for
experiments and the basic setup.
 | 2 | 
72.0 | 231.64707946777344 | 241.4074249267578 | 242.55616760253906 | 4.1
Wikipedia Data Pre-processing
 | 3 | 
71.48699951171875 | 249.96934509277344 | 292.175537109375 | 410.02801513671875 | Following (Lee et al., 2019), we use the English
Wikipedia dump from Dec. 20, 2018 as the source
documents for answering questions. We ﬁrst apply
the pre-processing code released in DrQA (Chen
et al., 2017) to extract the clean, text-portion of
articles from the Wikipedia dump. This step re-
moves semi-structured data, such as tables, info-
boxes, lists, as well as the disambiguation pages.
We then split each article into multiple, disjoint text
blocks of 100 words as passages, serving as our
basic retrieval units, following (Wang et al., 2019),
which results in 21,015,324 passages in the end.5
 | 4 | 
71.48699951171875 | 412.725830078125 | 290.2657775878906 | 450.6497497558594 | Each passage is also prepended with the title of the
Wikipedia article where the passage is from, along
with an [SEP] token.
 | 5 | 
71.99999237060547 | 463.4720458984375 | 233.5092315673828 | 474.38116455078125 | 4.2
Question Answering Datasets
 | 6 | 
71.45500183105469 | 481.7953186035156 | 292.07537841796875 | 724.8189697265625 | We use the same ﬁve QA datasets and train-
ing/dev/testing splitting method as in previous
work (Lee et al., 2019). Below we brieﬂy describe
each dataset and refer readers to their paper for the
details of data preparation.
Natural Questions (NQ) (Kwiatkowski et al.,
2019) was designed for end-to-end question an-
swering.
The questions were mined from real
Google search queries and the answers were spans
in Wikipedia articles identiﬁed by annotators.
TriviaQA (Joshi et al., 2017) contains a set of trivia
questions with answers that were originally scraped
from the Web.
WebQuestions (WQ) (Berant et al., 2013) consists
of questions selected using Google Suggest API,
where the answers are entities in Freebase.
CuratedTREC (TREC) (Baudiˇs and ˇSediv`y,
2015) sources questions from TREC QA tracks
 | 7 | 
72.0 | 734.6986694335938 | 291.7576904296875 | 765.1323852539062 | 5However, Wang et al. (2019) also propose splitting docu-
ments into overlapping passages, which we do not ﬁnd advan-
tageous compared to the non-overlapping version.
 | 8 | 
313.25299072265625 | 66.82611846923828 | 520.2154541015625 | 75.79251861572266 | Dataset
Train
Dev
Test
 | 9 | 
313.25299072265625 | 82.34799194335938 | 520.2156372070312 | 131.16436767578125 | Natural Questions
79,168
58,880
8,757
3,610
TriviaQA
78,785
60,413
8,837
11,313
WebQuestions
3,417
2,474
361
2,032
CuratedTREC
1,353
1,125
133
694
SQuAD
78,713
70,096
8,886
10,570
 | 10 | 
306.9670104980469 | 145.5144805908203 | 527.1986694335938 | 191.34213256835938 | Table 1: Number of questions in each QA dataset. The
two columns of Train denote the original training ex-
amples in the dataset and the actual questions used for
training DPR after ﬁltering. See text for more details.
 | 11 | 
306.88299560546875 | 216.07208251953125 | 527.4576416015625 | 390.0217590332031 | as well as various Web sources and is intended for
open-domain QA from unstructured corpora.
SQuAD v1.1 (Rajpurkar et al., 2016) is a popu-
lar benchmark dataset for reading comprehension.
Annotators were presented with a Wikipedia para-
graph, and asked to write questions that could be
answered from the given text. Although SQuAD
has been used previously for open-domain QA re-
search, it is not ideal because many questions lack
context in absence of the provided paragraph. We
still include it in our experiments for providing
a fair comparison to previous work and we will
discuss more in Section 5.1.
 | 12 | 
306.93701171875 | 401.53106689453125 | 527.359130859375 | 629.3267822265625 | Selection of positive passages
Because only
pairs of questions and answers are provided in
TREC, WebQuestions and TriviaQA6, we use the
highest-ranked passage from BM25 that contains
the answer as the positive passage. If none of the
top 100 retrieved passages has the answer, the ques-
tion will be discarded. For SQuAD and Natural
Questions, since the original passages have been
split and processed differently than our pool of
candidate passages, we match and replace each
gold passage with the corresponding passage in the
candidate pool.7 We discard the questions when
the matching is failed due to different Wikipedia
versions or pre-processing. Table 1 shows the num-
ber of questions in training/dev/test sets for all the
datasets and the actual questions used for training
the retriever.
 | 13 | 
307.2760009765625 | 642.8851928710938 | 487.5365295410156 | 654.8403930664062 | 5
Experiments: Passage Retrieval
 | 14 | 
307.2760009765625 | 665.766357421875 | 527.3513793945312 | 703.8597412109375 | In this section, we evaluate the retrieval perfor-
mance of our Dense Passage Retriever (DPR),
along with analysis on how its output differs from
 | 15 | 
307.2760009765625 | 713.9447021484375 | 527.029541015625 | 765.1323852539062 | 6We use the unﬁltered TriviaQA version and discard the
noisy evidence documents mined from Bing.
7The improvement of using gold contexts over passages
that contain answers is small.
See Section 5.2 and Ap-
pendix A.
 | 16 | 
Training
Retriever
Top-20
Top-100
NQ
TriviaQA
WQ
TREC
SQuAD
NQ
TriviaQA
WQ
TREC
SQuAD
None
BM25
59.1
66.9
55.0
70.9
68.8
73.7
76.7
71.1
84.1
80.0
Single
DPR
78.4
79.4
73.2
79.8
63.2
85.4
85.0
81.4
89.1
77.2
BM25 + DPR
76.6
79.8
71.0
85.2
71.5
83.8
84.5
80.5
92.7
81.3
Multi
DPR
79.4
78.8
75.0
89.1
51.6
86.0
84.7
82.9
93.9
67.6
BM25 + DPR
78.0
79.9
74.7
88.5
66.2
83.9
84.4
82.3
94.1
78.6
Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved
passages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained
using individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.
traditional retrieval methods, the effects of different
training schemes and the run-time efﬁciency.
The DPR model used in our main experiments
is trained using the in-batch negative setting (Sec-
tion 3.2) with a batch size of 128 and one additional
BM25 negative passage per question. We trained
the question and passage encoders for up to 40
epochs for large datasets (NQ, TriviaQA, SQuAD)
and 100 epochs for small datasets (TREC, WQ),
with a learning rate of 10−5 using Adam, linear
scheduling with warm-up and dropout rate 0.1.
While it is good to have the ﬂexibility to adapt
the retriever to each dataset, it would also be de-
sirable to obtain a single retriever that works well
across the board. To this end, we train a multi-
dataset encoder by combining training data from
all datasets excluding SQuAD.8 In addition to DPR,
we also present the results of BM25, the traditional
retrieval method9 and BM25+DPR, using a linear
combination of their scores as the new ranking
function. Speciﬁcally, we obtain two initial sets
of top-2000 passages based on BM25 and DPR,
respectively, and rerank the union of them using
BM25(q,p) + λ · sim(q, p) as the ranking function.
We used λ = 1.1 based on the retrieval accuracy in
the development set.
5.1
Main Results
Table 2 compares different passage retrieval sys-
tems on ﬁve QA datasets, using the top-k accuracy
(k ∈{20, 100}). With the exception of SQuAD,
DPR performs consistently better than BM25 on
all datasets. The gap is especially large when k is
small (e.g., 78.4% vs. 59.1% for top-20 accuracy
on Natural Questions). When training with mul-
8SQuAD is limited to a small set of Wikipedia documents
and thus introduces unwanted bias. We will discuss this issue
more in Section 5.1.
9Lucene implementation. BM25 parameters b = 0.4 (doc-
ument length normalization) and k1 = 0.9 (term frequency
scaling) are tuned using development sets.
20
40
60
80
100
k: # of retrieved passages
40
50
60
70
80
90
Top-k accuracy (%)
BM25
# Train: 1k
# Train: 10k
# Train: 20k
# Train: 40k
# Train: all (59k)
Figure 1: Retriever top-k accuracy with different num-
bers of training examples used in our dense passage re-
triever vs BM25. The results are measured on the de-
velopment set of Natural Questions. Our DPR trained
using 1,000 examples already outperforms BM25.
tiple datasets, TREC, the smallest dataset of the
ﬁve, beneﬁts greatly from more training examples.
In contrast, Natural Questions and WebQuestions
improve modestly and TriviaQA degrades slightly.
Results can be improved further in some cases by
combining DPR with BM25 in both single- and
multi-dataset settings.
We conjecture that the lower performance on
SQuAD is due to two reasons. First, the annota-
tors wrote questions after seeing the passage. As
a result, there is a high lexical overlap between
passages and questions, which gives BM25 a clear
advantage. Second, the data was collected from
only 500+ Wikipedia articles and thus the distribu-
tion of training examples is extremely biased, as
argued previously by Lee et al. (2019).
5.2
Ablation Study on Model Training
To understand further how different model training
options affect the results, we conduct several addi-
tional experiments and discuss our ﬁndings below.

84.50700378417969 | 66.82611846923828 | 513.0336303710938 | 85.83740234375 | Training
Retriever
Top-20
Top-100
NQ
TriviaQA
WQ
TREC
SQuAD
NQ
TriviaQA
WQ
TREC
SQuAD
 |  | 
84.50700378417969 | 92.30996704101562 | 506.43414306640625 | 101.2763671875 | None
BM25
59.1
66.9
55.0
70.9
68.8
73.7
76.7
71.1
84.1
80.0
 | 1 | 
84.50700378417969 | 107.66809844970703 | 506.43719482421875 | 126.67938232421875 | Single
DPR
78.4
79.4
73.2
79.8
63.2
85.4
85.0
81.4
89.1
77.2
BM25 + DPR
76.6
79.8
71.0
85.2
71.5
83.8
84.5
80.5
92.7
81.3
 | 2 | 
84.50700378417969 | 133.0711212158203 | 506.43719482421875 | 152.0814208984375 | Multi
DPR
79.4
78.8
75.0
89.1
51.6
86.0
84.7
82.9
93.9
67.6
BM25 + DPR
78.0
79.9
74.7
88.5
66.2
83.9
84.4
82.3
94.1
78.6
 | 3 | 
71.69100189208984 | 166.4314727783203 | 525.5479736328125 | 200.30410766601562 | Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved
passages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained
using individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.
 | 4 | 
71.48699951171875 | 224.1778564453125 | 292.17425537109375 | 573.8687744140625 | traditional retrieval methods, the effects of different
training schemes and the run-time efﬁciency.
The DPR model used in our main experiments
is trained using the in-batch negative setting (Sec-
tion 3.2) with a batch size of 128 and one additional
BM25 negative passage per question. We trained
the question and passage encoders for up to 40
epochs for large datasets (NQ, TriviaQA, SQuAD)
and 100 epochs for small datasets (TREC, WQ),
with a learning rate of 10−5 using Adam, linear
scheduling with warm-up and dropout rate 0.1.
While it is good to have the ﬂexibility to adapt
the retriever to each dataset, it would also be de-
sirable to obtain a single retriever that works well
across the board. To this end, we train a multi-
dataset encoder by combining training data from
all datasets excluding SQuAD.8 In addition to DPR,
we also present the results of BM25, the traditional
retrieval method9 and BM25+DPR, using a linear
combination of their scores as the new ranking
function. Speciﬁcally, we obtain two initial sets
of top-2000 passages based on BM25 and DPR,
respectively, and rerank the union of them using
BM25(q,p) + λ · sim(q, p) as the ranking function.
We used λ = 1.1 based on the retrieval accuracy in
the development set.
 | 5 | 
72.0 | 585.4790649414062 | 158.06190490722656 | 596.3881225585938 | 5.1
Main Results
 | 6 | 
71.63999938964844 | 603.1063842773438 | 292.0823669433594 | 695.4190063476562 | Table 2 compares different passage retrieval sys-
tems on ﬁve QA datasets, using the top-k accuracy
(k ∈{20, 100}). With the exception of SQuAD,
DPR performs consistently better than BM25 on
all datasets. The gap is especially large when k is
small (e.g., 78.4% vs. 59.1% for top-20 accuracy
on Natural Questions). When training with mul-
 | 7 | 
72.0 | 703.898681640625 | 291.75616455078125 | 765.1323852539062 | 8SQuAD is limited to a small set of Wikipedia documents
and thus introduces unwanted bias. We will discuss this issue
more in Section 5.1.
9Lucene implementation. BM25 parameters b = 0.4 (doc-
ument length normalization) and k1 = 0.9 (term frequency
scaling) are tuned using development sets.
 | 8 | 
366.0042419433594 | 360.1532287597656 | 512.239501953125 | 380.1865234375 | 20
40
60
80
100
k: # of retrieved passages
 | 9 | 
325.63848876953125 | 355.0751037597656 | 333.2705078125 | 365.2511291503906 | 40
 | 10 | 
325.63848876953125 | 328.46392822265625 | 333.2705078125 | 338.63995361328125 | 50
 | 11 | 
325.63848876953125 | 301.85272216796875 | 333.2705078125 | 312.02874755859375 | 60
 | 12 | 
325.63848876953125 | 275.24151611328125 | 333.2705078125 | 285.41754150390625 | 70
 | 13 | 
325.63848876953125 | 248.6303253173828 | 333.2705078125 | 258.80633544921875 | 80
 | 14 | 
325.63848876953125 | 222.01913452148438 | 333.2705078125 | 232.19512939453125 | 90
 | 15 | 
312.5119934082031 | 254.94161987304688 | 326.08001708984375 | 332.3576354980469 | Top-k accuracy (%)
 | 16 | 
454.60723876953125 | 298.2438049316406 | 508.8536682128906 | 356.06695556640625 | BM25
# Train: 1k
# Train: 10k
# Train: 20k
# Train: 40k
# Train: all (59k)
 | 17 | 
307.0270080566406 | 389.7479553222656 | 527.2003784179688 | 447.7620544433594 | Figure 1: Retriever top-k accuracy with different num-
bers of training examples used in our dense passage re-
triever vs BM25. The results are measured on the de-
velopment set of Natural Questions. Our DPR trained
using 1,000 examples already outperforms BM25.
 | 18 | 
307.2760009765625 | 474.5333251953125 | 527.4494018554688 | 690.2947387695312 | tiple datasets, TREC, the smallest dataset of the
ﬁve, beneﬁts greatly from more training examples.
In contrast, Natural Questions and WebQuestions
improve modestly and TriviaQA degrades slightly.
Results can be improved further in some cases by
combining DPR with BM25 in both single- and
multi-dataset settings.
We conjecture that the lower performance on
SQuAD is due to two reasons. First, the annota-
tors wrote questions after seeing the passage. As
a result, there is a high lexical overlap between
passages and questions, which gives BM25 a clear
advantage. Second, the data was collected from
only 500+ Wikipedia articles and thus the distribu-
tion of training examples is extremely biased, as
argued previously by Lee et al. (2019).
 | 19 | 
307.2760009765625 | 706.9771118164062 | 492.2615051269531 | 717.8861694335938 | 5.2
Ablation Study on Model Training
 | 20 | 
306.93701171875 | 727.664306640625 | 527.3514404296875 | 765.6017456054688 | To understand further how different model training
options affect the results, we conduct several addi-
tional experiments and discuss our ﬁndings below.
 | 21 | 
Sample efﬁciency
We explore how many train-
ing examples are needed to achieve good passage
retrieval performance. Figure 1 illustrates the top-k
retrieval accuracy with respect to different num-
bers of training examples, measured on the devel-
opment set of Natural Questions. As is shown, a
dense passage retriever trained using only 1,000 ex-
amples already outperforms BM25. This suggests
that with a general pretrained language model, it is
possible to train a high-quality dense retriever with
a small number of question–passage pairs. Adding
more training examples (from 1k to 59k) further
improves the retrieval accuracy consistently.
In-batch negative training
We test different
training schemes on the development set of Natural
Questions and summarize the results in Table 3.
The top block is the standard 1-of-N training set-
ting, where each question in the batch is paired
with a positive passage and its own set of n neg-
ative passages (Eq. (2)). We ﬁnd that the choice
of negatives — random, BM25 or gold passages
(positive passages from other questions) — does
not impact the top-k accuracy much in this setting
when k ≥20.
The middle bock is the in-batch negative training
(Section 3.2) setting. We ﬁnd that using a similar
conﬁguration (7 gold negative passages), in-batch
negative training improves the results substantially.
The key difference between the two is whether the
gold negative passages come from the same batch
or from the whole training set. Effectively, in-batch
negative training is an easy and memory-efﬁcient
way to reuse the negative examples already in the
batch rather than creating new ones. It produces
more pairs and thus increases the number of train-
ing examples, which might contribute to the good
model performance. As a result, accuracy consis-
tently improves as the batch size grows.
Finally, we explore in-batch negative training
with additional “hard” negative passages that have
high BM25 scores given the question, but do not
contain the answer string (the bottom block). These
additional passages are used as negative passages
for all questions in the same batch. We ﬁnd that
adding a single BM25 negative passage improves
the result substantially while adding two does not
help further.
Impact of gold passages
We use passages that
match the gold contexts in the original datasets
(when available) as positive examples (Section 4.2).
Type
#N
IB
Top-5
Top-20
Top-100
Random
7

47.0
64.3
77.8
BM25
7

50.0
63.3
74.8
Gold
7

42.6
63.1
78.3
Gold
7

51.1
69.1
80.8
Gold
31

52.1
70.8
82.1
Gold
127

55.8
73.0
83.1
G.+BM25(1)
31+32

65.0
77.3
84.4
G.+BM25(2)
31+64

64.5
76.4
84.0
G.+BM25(1)
127+128

65.8
78.0
84.9
Table 3: Comparison of different training schemes,
measured as top-k retrieval accuracy on Natural Ques-
tions (development set).
#N: number of negative
examples, IB: in-batch training.
G.+BM25(1) and
G.+BM25(2) denote in-batch training with 1 or 2 ad-
ditional BM25 negatives, which serve as negative pas-
sages for all questions in the batch.
Our experiments on Natural Questions show that
switching to distantly-supervised passages (using
the highest-ranked BM25 passage that contains the
answer), has only a small impact: 1 point lower
top-k accuracy for retrieval. Appendix A contains
more details.
Similarity and loss
Besides dot product, cosine
and Euclidean L2 distance are also commonly used
as decomposable similarity functions. We test these
alternatives and ﬁnd that L2 performs compara-
ble to dot product, and both of them are superior
to cosine. Similarly, in addition to negative log-
likelihood, a popular option for ranking is triplet
loss, which compares a positive passage and a nega-
tive one directly with respect to a question (Burges
et al., 2005). Our experiments show that using
triplet loss does not affect the results much. More
details can be found in Appendix B.
Cross-dataset generalization
One interesting
question regarding DPR’s discriminative training
is how much performance degradation it may suf-
fer from a non-iid setting. In other words, can
it still generalize well when directly applied to
a different dataset without additional ﬁne-tuning?
To test the cross-dataset generalization, we train
DPR on Natural Questions only and test it directly
on the smaller WebQuestions and CuratedTREC
datasets. We ﬁnd that DPR generalizes well, with
3-5 points loss from the best performing ﬁne-tuned
model in top-20 retrieval accuracy (69.9/86.3 vs.
75.0/89.1 for WebQuestions and TREC, respec-
tively), while still greatly outperforming the BM25
baseline (55.0/70.9).

72.0 | 65.39408111572266 | 292.0832214355469 | 238.99378967285156 | Sample efﬁciency
We explore how many train-
ing examples are needed to achieve good passage
retrieval performance. Figure 1 illustrates the top-k
retrieval accuracy with respect to different num-
bers of training examples, measured on the devel-
opment set of Natural Questions. As is shown, a
dense passage retriever trained using only 1,000 ex-
amples already outperforms BM25. This suggests
that with a general pretrained language model, it is
possible to train a high-quality dense retriever with
a small number of question–passage pairs. Adding
more training examples (from 1k to 59k) further
improves the retrieval accuracy consistently.
 |  | 
71.60700225830078 | 252.99510192871094 | 292.175537109375 | 713.4927368164062 | In-batch negative training
We test different
training schemes on the development set of Natural
Questions and summarize the results in Table 3.
The top block is the standard 1-of-N training set-
ting, where each question in the batch is paired
with a positive passage and its own set of n neg-
ative passages (Eq. (2)). We ﬁnd that the choice
of negatives — random, BM25 or gold passages
(positive passages from other questions) — does
not impact the top-k accuracy much in this setting
when k ≥20.
The middle bock is the in-batch negative training
(Section 3.2) setting. We ﬁnd that using a similar
conﬁguration (7 gold negative passages), in-batch
negative training improves the results substantially.
The key difference between the two is whether the
gold negative passages come from the same batch
or from the whole training set. Effectively, in-batch
negative training is an easy and memory-efﬁcient
way to reuse the negative examples already in the
batch rather than creating new ones. It produces
more pairs and thus increases the number of train-
ing examples, which might contribute to the good
model performance. As a result, accuracy consis-
tently improves as the batch size grows.
Finally, we explore in-batch negative training
with additional “hard” negative passages that have
high BM25 scores given the question, but do not
contain the answer string (the bottom block). These
additional passages are used as negative passages
for all questions in the same batch. We ﬁnd that
adding a single BM25 negative passage improves
the result substantially while adding two does not
help further.
 | 1 | 
71.63999938964844 | 727.4940795898438 | 292.1722106933594 | 765.5752563476562 | Impact of gold passages
We use passages that
match the gold contexts in the original datasets
(when available) as positive examples (Section 4.2).
 | 2 | 
309.52099609375 | 66.82611846923828 | 519.1106567382812 | 75.79251861572266 | Type
#N
IB
Top-5
Top-20
Top-100
 | 3 | 
309.52099609375 | 80.18161010742188 | 511.433349609375 | 111.2393798828125 | Random
7

47.0
64.3
77.8
BM25
7

50.0
63.3
74.8
Gold
7

42.6
63.1
78.3
 | 4 | 
309.52099609375 | 115.54660034179688 | 511.433349609375 | 146.6043701171875 | Gold
7

51.1
69.1
80.8
Gold
31

52.1
70.8
82.1
Gold
127

55.8
73.0
83.1
 | 5 | 
309.52099609375 | 152.22964477539062 | 511.433349609375 | 185.9244384765625 | G.+BM25(1)
31+32

65.0
77.3
84.4
G.+BM25(2)
31+64

64.5
76.4
84.0
G.+BM25(1)
127+128

65.8
78.0
84.9
 | 6 | 
306.9670104980469 | 200.2744903564453 | 527.2003173828125 | 281.9681701660156 | Table 3: Comparison of different training schemes,
measured as top-k retrieval accuracy on Natural Ques-
tions (development set).
#N: number of negative
examples, IB: in-batch training.
G.+BM25(1) and
G.+BM25(2) denote in-batch training with 1 or 2 ad-
ditional BM25 negatives, which serve as negative pas-
sages for all questions in the batch.
 | 7 | 
307.2760009765625 | 305.8183288574219 | 525.7266845703125 | 384.55474853515625 | Our experiments on Natural Questions show that
switching to distantly-supervised passages (using
the highest-ranked BM25 passage that contains the
answer), has only a small impact: 1 point lower
top-k accuracy for retrieval. Appendix A contains
more details.
 | 8 | 
307.2760009765625 | 394.7050476074219 | 527.359130859375 | 554.7547607421875 | Similarity and loss
Besides dot product, cosine
and Euclidean L2 distance are also commonly used
as decomposable similarity functions. We test these
alternatives and ﬁnd that L2 performs compara-
ble to dot product, and both of them are superior
to cosine. Similarly, in addition to negative log-
likelihood, a popular option for ranking is triplet
loss, which compares a positive passage and a nega-
tive one directly with respect to a question (Burges
et al., 2005). Our experiments show that using
triplet loss does not affect the results much. More
details can be found in Appendix B.
 | 9 | 
306.93701171875 | 564.904052734375 | 527.451416015625 | 765.6017456054688 | Cross-dataset generalization
One interesting
question regarding DPR’s discriminative training
is how much performance degradation it may suf-
fer from a non-iid setting. In other words, can
it still generalize well when directly applied to
a different dataset without additional ﬁne-tuning?
To test the cross-dataset generalization, we train
DPR on Natural Questions only and test it directly
on the smaller WebQuestions and CuratedTREC
datasets. We ﬁnd that DPR generalizes well, with
3-5 points loss from the best performing ﬁne-tuned
model in top-20 retrieval accuracy (69.9/86.3 vs.
75.0/89.1 for WebQuestions and TREC, respec-
tively), while still greatly outperforming the BM25
baseline (55.0/70.9).
 | 10 | 
5.3
Qualitative Analysis
Although DPR performs better than BM25 in gen-
eral, passages retrieved by these two methods dif-
fer qualitatively.
Term-matching methods like
BM25 are sensitive to highly selective keywords
and phrases, while DPR captures lexical variations
or semantic relationships better. See Appendix C
for examples and more discussion.
5.4
Run-time Efﬁciency
The main reason that we require a retrieval compo-
nent for open-domain QA is to reduce the number
of candidate passages that the reader needs to con-
sider, which is crucial for answering user’s ques-
tions in real-time. We proﬁled the passage retrieval
speed on a server with Intel Xeon CPU E5-2698 v4
@ 2.20GHz and 512GB memory. With the help of
FAISS in-memory index for real-valued vectors10,
DPR can be made incredibly efﬁcient, processing
995.0 questions per second, returning top 100 pas-
sages per question. In contrast, BM25/Lucene (im-
plemented in Java, using ﬁle index) processes 23.7
questions per second per CPU thread.
On the other hand, the time required for building
an index for dense vectors is much longer. Com-
puting dense embeddings on 21-million passages
is resource intensive, but can be easily parallelized,
taking roughly 8.8 hours on 8 GPUs. However,
building the FAISS index on 21-million vectors
on a single server takes 8.5 hours. In comparison,
building an inverted index using Lucene is much
cheaper and takes only about 30 minutes in total.
6
Experiments: Question Answering
In this section, we experiment with how different
passage retrievers affect the ﬁnal QA accuracy.
6.1
End-to-end QA System
We implement an end-to-end question answering
system in which we can plug different retriever
systems directly. Besides the retriever, our QA sys-
tem consists of a neural reader that outputs the
answer to the question. Given the top k retrieved
passages (up to 100 in our experiments), the reader
assigns a passage selection score to each passage.
In addition, it extracts an answer span from each
passage and assigns a span score. The best span
from the passage with the highest passage selection
10FAISS conﬁguration: we used HNSW index type on CPU,
neighbors to store per node = 512, construction time search
depth = 200, search depth = 128.
score is chosen as the ﬁnal answer. The passage
selection model serves as a reranker through cross-
attention between the question and the passage. Al-
though cross-attention is not feasible for retrieving
relevant passages in a large corpus due to its non-
decomposable nature, it has more capacity than the
dual-encoder model sim(q, p) as in Eq. (1). Apply-
ing it to selecting the passage from a small number
of retrieved candidates has been shown to work
well (Wang et al., 2019, 2018; Lin et al., 2018).
Speciﬁcally, let Pi ∈RL×h (1 ≤i ≤k) be
a BERT (base, uncased in our experiments) rep-
resentation for the i-th passage, where L is the
maximum length of the passage and h the hidden
dimension. The probabilities of a token being the
starting/ending positions of an answer span and a
passage being selected are deﬁned as:
Pstart,i(s)
=
softmax
 Piwstart

s,
(3)
Pend,i(t)
=
softmax
 Piwend

t,
(4)
Pselected(i)
=
softmax
 ˆP⊺wselected

i, (5)
where ˆP = [P[CLS]
1
, . . . , P[CLS]
k
] ∈Rh×k and
wstart, wend, wselected ∈Rh are learnable vectors.
We compute a span score of the s-th to t-th words
from the i-th passage as Pstart,i(s) × Pend,i(t), and
a passage selection score of the i-th passage as
Pselected(i).
During training, we sample one positive and
˜m−1 negative passages from the top 100 passages
returned by the retrieval system (BM25 or DPR)
for each question. ˜m is a hyper-parameter and we
use ˜m = 24 in all the experiments. The training ob-
jective is to maximize the marginal log-likelihood
of all the correct answer spans in the positive pas-
sage (the answer string may appear multiple times
in one passage), combined with the log-likelihood
of the positive passage being selected. We use the
batch size of 16 for large (NQ, TriviaQA, SQuAD)
and 4 for small (TREC, WQ) datasets, and tune k
on the development set. For experiments on small
datasets under the Multi setting, in which using
other datasets is allowed, we ﬁne-tune the reader
trained on Natural Questions to the target dataset.
All experiments were done on eight 32GB GPUs.
6.2
Results
Table 4 summarizes our ﬁnal end-to-end QA re-
sults, measured by exact match with the reference
answer after minor normalization as in (Chen et al.,
2017; Lee et al., 2019). From the table, we can

72.0 | 65.39408111572266 | 190.58192443847656 | 76.30318450927734 | 5.3
Qualitative Analysis
 |  | 
71.60700225830078 | 83.53752136230469 | 292.08197021484375 | 175.7467803955078 | Although DPR performs better than BM25 in gen-
eral, passages retrieved by these two methods dif-
fer qualitatively.
Term-matching methods like
BM25 are sensitive to highly selective keywords
and phrases, while DPR captures lexical variations
or semantic relationships better. See Appendix C
for examples and more discussion.
 | 1 | 
72.0 | 188.12010192871094 | 189.5673828125 | 199.02919006347656 | 5.4
Run-time Efﬁciency
 | 2 | 
70.99600219726562 | 206.32260131835938 | 292.0789794921875 | 501.9987487792969 | The main reason that we require a retrieval compo-
nent for open-domain QA is to reduce the number
of candidate passages that the reader needs to con-
sider, which is crucial for answering user’s ques-
tions in real-time. We proﬁled the passage retrieval
speed on a server with Intel Xeon CPU E5-2698 v4
@ 2.20GHz and 512GB memory. With the help of
FAISS in-memory index for real-valued vectors10,
DPR can be made incredibly efﬁcient, processing
995.0 questions per second, returning top 100 pas-
sages per question. In contrast, BM25/Lucene (im-
plemented in Java, using ﬁle index) processes 23.7
questions per second per CPU thread.
On the other hand, the time required for building
an index for dense vectors is much longer. Com-
puting dense embeddings on 21-million passages
is resource intensive, but can be easily parallelized,
taking roughly 8.8 hours on 8 GPUs. However,
building the FAISS index on 21-million vectors
on a single server takes 8.5 hours. In comparison,
building an inverted index using Lucene is much
cheaper and takes only about 30 minutes in total.
 | 3 | 
72.0 | 514.9501342773438 | 266.6545715332031 | 526.9053344726562 | 6
Experiments: Question Answering
 | 4 | 
72.0 | 537.3963623046875 | 290.27276611328125 | 561.936767578125 | In this section, we experiment with how different
passage retrievers affect the ﬁnal QA accuracy.
 | 5 | 
72.0 | 574.3110961914062 | 204.4255828857422 | 585.2201538085938 | 6.1
End-to-end QA System
 | 6 | 
71.48699951171875 | 592.3763427734375 | 292.17559814453125 | 725.2843017578125 | We implement an end-to-end question answering
system in which we can plug different retriever
systems directly. Besides the retriever, our QA sys-
tem consists of a neural reader that outputs the
answer to the question. Given the top k retrieved
passages (up to 100 in our experiments), the reader
assigns a passage selection score to each passage.
In addition, it extracts an answer span from each
passage and assigns a span score. The best span
from the passage with the highest passage selection
 | 7 | 
72.0 | 734.6986694335938 | 291.39288330078125 | 765.1323852539062 | 10FAISS conﬁguration: we used HNSW index type on CPU,
neighbors to store per node = 512, construction time search
depth = 200, search depth = 128.
 | 8 | 
306.88299560546875 | 65.41136169433594 | 527.359375 | 293.5787353515625 | score is chosen as the ﬁnal answer. The passage
selection model serves as a reranker through cross-
attention between the question and the passage. Al-
though cross-attention is not feasible for retrieving
relevant passages in a large corpus due to its non-
decomposable nature, it has more capacity than the
dual-encoder model sim(q, p) as in Eq. (1). Apply-
ing it to selecting the passage from a small number
of retrieved candidates has been shown to work
well (Wang et al., 2019, 2018; Lin et al., 2018).
Speciﬁcally, let Pi ∈RL×h (1 ≤i ≤k) be
a BERT (base, uncased in our experiments) rep-
resentation for the i-th passage, where L is the
maximum length of the passage and h the hidden
dimension. The probabilities of a token being the
starting/ending positions of an answer span and a
passage being selected are deﬁned as:
 | 9 | 
331.9259948730469 | 307.09814453125 | 483.2684020996094 | 320.17864990234375 | Pstart,i(s)
=
softmax
 
Piwstart

 | 10 | 
483.27203369140625 | 308.09014892578125 | 525.5430908203125 | 322.6741943359375 | s,
(3)
 | 11 | 
335.3150634765625 | 324.67218017578125 | 481.0534362792969 | 337.794677734375 | Pend,i(t)
=
softmax
 
Piwend

 | 12 | 
481.05706787109375 | 325.6651916503906 | 525.5430908203125 | 340.24822998046875 | t,
(4)
 | 13 | 
326.5690612792969 | 340.22991943359375 | 499.83441162109375 | 356.30767822265625 | Pselected(i)
=
softmax
 ˆP⊺wselected

 | 14 | 
499.8380432128906 | 344.1781921386719 | 525.5430908203125 | 358.76220703125 | i, (5)
 | 15 | 
306.76300048828125 | 369.5543212890625 | 527.4527587890625 | 682.9747314453125 | where ˆP = [P[CLS]
1
, . . . , P[CLS]
k
] ∈Rh×k and
wstart, wend, wselected ∈Rh are learnable vectors.
We compute a span score of the s-th to t-th words
from the i-th passage as Pstart,i(s) × Pend,i(t), and
a passage selection score of the i-th passage as
Pselected(i).
During training, we sample one positive and
˜m−1 negative passages from the top 100 passages
returned by the retrieval system (BM25 or DPR)
for each question. ˜m is a hyper-parameter and we
use ˜m = 24 in all the experiments. The training ob-
jective is to maximize the marginal log-likelihood
of all the correct answer spans in the positive pas-
sage (the answer string may appear multiple times
in one passage), combined with the log-likelihood
of the positive passage being selected. We use the
batch size of 16 for large (NQ, TriviaQA, SQuAD)
and 4 for small (TREC, WQ) datasets, and tune k
on the development set. For experiments on small
datasets under the Multi setting, in which using
other datasets is allowed, we ﬁne-tune the reader
trained on Natural Questions to the target dataset.
All experiments were done on eight 32GB GPUs.
 | 16 | 
307.2760009765625 | 695.6981201171875 | 365.7596740722656 | 706.607177734375 | 6.2
Results
 | 17 | 
306.93701171875 | 713.9623413085938 | 527.3572998046875 | 765.6279907226562 | Table 4 summarizes our ﬁnal end-to-end QA re-
sults, measured by exact match with the reference
answer after minor normalization as in (Chen et al.,
2017; Lee et al., 2019). From the table, we can
 | 18 | 
Training
Model
NQ
TriviaQA
WQ
TREC
SQuAD
Single
BM25+BERT (Lee et al., 2019)
26.5
47.1
17.7
21.3
33.2
Single
ORQA (Lee et al., 2019)
33.3
45.0
36.4
30.1
20.2
Single
HardEM (Min et al., 2019a)
28.1
50.9
-
-
-
Single
GraphRetriever (Min et al., 2019b)
34.5
56.0
36.4
-
-
Single
PathRetriever (Asai et al., 2020)
32.6
-
-
-
56.5
Single
REALMWiki (Guu et al., 2020)
39.2
-
40.2
46.8
-
Single
REALMNews (Guu et al., 2020)
40.4
-
40.7
42.9
-
Single
BM25
32.6
52.4
29.9
24.9
38.1
DPR
41.5
56.8
34.6
25.9
29.8
BM25+DPR
39.0
57.0
35.2
28.0
36.7
Multi
DPR
41.5
56.8
42.4
49.4
24.1
BM25+DPR
38.8
57.9
41.1
50.6
35.8
Table 4: End-to-end QA (Exact Match) Accuracy. The ﬁrst block of results are copied from their cited papers.
REALMWiki and REALMNews are the same model but pretrained on Wikipedia and CC-News, respectively. Single
and Multi denote that our Dense Passage Retriever (DPR) is trained using individual or combined training datasets
(all except SQuAD). For WQ and TREC in the Multi setting, we ﬁne-tune the reader trained on NQ.
see that higher retriever accuracy typically leads to
better ﬁnal QA results: in all cases except SQuAD,
answers extracted from the passages retrieved by
DPR are more likely to be correct, compared to
those from BM25. For large datasets like NQ and
TriviaQA, models trained using multiple datasets
(Multi) perform comparably to those trained using
the individual training set (Single). Conversely,
on smaller datasets like WQ and TREC, the multi-
dataset setting has a clear advantage. Overall, our
DPR-based models outperform the previous state-
of-the-art results on four out of the ﬁve datasets,
with 1% to 12% absolute differences in exact match
accuracy. It is interesting to contrast our results to
those of ORQA (Lee et al., 2019) and also the
concurrently developed approach, REALM (Guu
et al., 2020). While both methods include addi-
tional pretraining tasks and employ an expensive
end-to-end training regime, DPR manages to out-
perform them on both NQ and TriviaQA, simply
by focusing on learning a strong passage retrieval
model using pairs of questions and answers. The
additional pretraining tasks are likely more useful
only when the target training sets are small. Al-
though the results of DPR on WQ and TREC in the
single-dataset setting are less competitive, adding
more question–answer pairs helps boost the perfor-
mance, achieving the new state of the art.
To compare our pipeline training approach with
joint learning, we run an ablation on Natural Ques-
tions where the retriever and reader are jointly
trained, following Lee et al. (2019). This approach
obtains a score of 39.8 EM, which suggests that our
strategy of training a strong retriever and reader in
isolation can leverage effectively available supervi-
sion, while outperforming a comparable joint train-
ing approach with a simpler design (Appendix D).
One thing worth noticing is that our reader does
consider more passages compared to ORQA, al-
though it is not completely clear how much more
time it takes for inference. While DPR processes
up to 100 passages for each question, the reader
is able to ﬁt all of them into one batch on a sin-
gle 32GB GPU, thus the latency remains almost
identical to the single passage case (around 20ms).
The exact impact on throughput is harder to mea-
sure: ORQA uses 2-3x longer passages compared
to DPR (288 word pieces compared to our 100
tokens) and the computational complexity is super-
linear in passage length. We also note that we
found k = 50 to be optimal for NQ, and k = 10
leads to only marginal loss in exact match accu-
racy (40.8 vs. 41.5 EM on NQ), which should be
roughly comparable to ORQA’s 5-passage setup.
7
Related Work
Passage retrieval has been an important compo-
nent for open-domain QA (Voorhees, 1999). It
not only effectively reduces the search space for
answer extraction, but also identiﬁes the support
context for users to verify the answer. Strong sparse
vector space models like TF-IDF or BM25 have

97.69499969482422 | 67.8460464477539 | 499.8480224609375 | 78.7551498413086 | Training
Model
NQ
TriviaQA
WQ
TREC
SQuAD
 |  | 
97.69499969482422 | 86.97164916992188 | 491.2089538574219 | 180.81178283691406 | Single
BM25+BERT (Lee et al., 2019)
26.5
47.1
17.7
21.3
33.2
Single
ORQA (Lee et al., 2019)
33.3
45.0
36.4
30.1
20.2
Single
HardEM (Min et al., 2019a)
28.1
50.9
-
-
-
Single
GraphRetriever (Min et al., 2019b)
34.5
56.0
36.4
-
-
Single
PathRetriever (Asai et al., 2020)
32.6
-
-
-
56.5
Single
REALMWiki (Guu et al., 2020)
39.2
-
40.2
46.8
-
Single
REALMNews (Guu et al., 2020)
40.4
-
40.7
42.9
-
 | 1 | 
97.69499969482422 | 187.29269409179688 | 491.20989990234375 | 225.30079650878906 | Single
BM25
32.6
52.4
29.9
24.9
38.1
DPR
41.5
56.8
34.6
25.9
29.8
BM25+DPR
39.0
57.0
35.2
28.0
36.7
 | 2 | 
97.69499969482422 | 233.3180389404297 | 491.20892333984375 | 257.875732421875 | Multi
DPR
41.5
56.8
42.4
49.4
24.1
BM25+DPR
38.8
57.9
41.1
50.6
35.8
 | 3 | 
71.67103576660156 | 272.83245849609375 | 527.2904663085938 | 318.6601257324219 | Table 4: End-to-end QA (Exact Match) Accuracy. The ﬁrst block of results are copied from their cited papers.
REALMWiki and REALMNews are the same model but pretrained on Wikipedia and CC-News, respectively. Single
and Multi denote that our Dense Passage Retriever (DPR) is trained using individual or combined training datasets
(all except SQuAD). For WQ and TREC in the Multi setting, we ﬁne-tune the reader trained on NQ.
 | 4 | 
71.60700225830078 | 342.51715087890625 | 292.0801696777344 | 719.187744140625 | see that higher retriever accuracy typically leads to
better ﬁnal QA results: in all cases except SQuAD,
answers extracted from the passages retrieved by
DPR are more likely to be correct, compared to
those from BM25. For large datasets like NQ and
TriviaQA, models trained using multiple datasets
(Multi) perform comparably to those trained using
the individual training set (Single). Conversely,
on smaller datasets like WQ and TREC, the multi-
dataset setting has a clear advantage. Overall, our
DPR-based models outperform the previous state-
of-the-art results on four out of the ﬁve datasets,
with 1% to 12% absolute differences in exact match
accuracy. It is interesting to contrast our results to
those of ORQA (Lee et al., 2019) and also the
concurrently developed approach, REALM (Guu
et al., 2020). While both methods include addi-
tional pretraining tasks and employ an expensive
end-to-end training regime, DPR manages to out-
perform them on both NQ and TriviaQA, simply
by focusing on learning a strong passage retrieval
model using pairs of questions and answers. The
additional pretraining tasks are likely more useful
only when the target training sets are small. Al-
though the results of DPR on WQ and TREC in the
single-dataset setting are less competitive, adding
more question–answer pairs helps boost the perfor-
mance, achieving the new state of the art.
 | 5 | 
72.0 | 727.6102294921875 | 292.08319091796875 | 765.6279907226562 | To compare our pipeline training approach with
joint learning, we run an ablation on Natural Ques-
tions where the retriever and reader are jointly
 | 6 | 
306.93701171875 | 342.504638671875 | 527.4498291015625 | 651.6917724609375 | trained, following Lee et al. (2019). This approach
obtains a score of 39.8 EM, which suggests that our
strategy of training a strong retriever and reader in
isolation can leverage effectively available supervi-
sion, while outperforming a comparable joint train-
ing approach with a simpler design (Appendix D).
One thing worth noticing is that our reader does
consider more passages compared to ORQA, al-
though it is not completely clear how much more
time it takes for inference. While DPR processes
up to 100 passages for each question, the reader
is able to ﬁt all of them into one batch on a sin-
gle 32GB GPU, thus the latency remains almost
identical to the single passage case (around 20ms).
The exact impact on throughput is harder to mea-
sure: ORQA uses 2-3x longer passages compared
to DPR (288 word pieces compared to our 100
tokens) and the computational complexity is super-
linear in passage length. We also note that we
found k = 50 to be optimal for NQ, and k = 10
leads to only marginal loss in exact match accu-
racy (40.8 vs. 41.5 EM on NQ), which should be
roughly comparable to ORQA’s 5-passage setup.
 | 7 | 
307.2760009765625 | 664.5121459960938 | 396.36614990234375 | 676.4673461914062 | 7
Related Work
 | 8 | 
307.00299072265625 | 686.8643798828125 | 527.3514404296875 | 765.6279907226562 | Passage retrieval has been an important compo-
nent for open-domain QA (Voorhees, 1999). It
not only effectively reduces the search space for
answer extraction, but also identiﬁes the support
context for users to verify the answer. Strong sparse
vector space models like TF-IDF or BM25 have
 | 9 | 
been used as the standard method applied broadly
to various QA tasks (e.g., Chen et al., 2017; Yang
et al., 2019a,b; Nie et al., 2019; Min et al., 2019a;
Wolfson et al., 2020). Augmenting text-based re-
trieval with external structured information, such
as knowledge graph and Wikipedia hyperlinks, has
also been explored recently (Min et al., 2019b; Asai
et al., 2020).
The use of dense vector representations for re-
trieval has a long history since Latent Semantic
Analysis (Deerwester et al., 1990). Using labeled
pairs of queries and documents, discriminatively
trained dense encoders have become popular re-
cently (Yih et al., 2011; Huang et al., 2013; Gillick
et al., 2019), with applications to cross-lingual
document retrieval, ad relevance prediction, Web
search and entity retrieval. Such approaches com-
plement the sparse vector methods as they can po-
tentially give high similarity scores to semantically
relevant text pairs, even without exact token match-
ing. The dense representation alone, however, is
typically inferior to the sparse one. While not the
focus of this work, dense representations from pre-
trained models, along with cross-attention mecha-
nisms, have also been shown effective in passage
or dialogue re-ranking tasks (Nogueira and Cho,
2019; Humeau et al., 2020). Finally, a concurrent
work (Khattab and Zaharia, 2020) demonstrates
the feasibility of full dense retrieval in IR tasks.
Instead of employing the dual-encoder framework,
they introduced a late-interaction operator on top
of the BERT encoders.
Dense retrieval for open-domain QA has been
explored by Das et al. (2019), who propose to re-
trieve relevant passages iteratively using reformu-
lated question vectors. As an alternative approach
that skips passage retrieval, Seo et al. (2019) pro-
pose to encode candidate answer phrases as vectors
and directly retrieve the answers to the input ques-
tions efﬁciently. Using additional pretraining with
the objective that matches surrogates of questions
and relevant passages, Lee et al. (2019) jointly train
the question encoder and reader. Their approach
outperforms the BM25 plus reader paradigm on
multiple open-domain QA datasets in QA accuracy,
and is further extended by REALM (Guu et al.,
2020), which includes tuning the passage encoder
asynchronously by re-indexing the passages dur-
ing training. The pretraining objective has also
recently been improved by Xiong et al. (2020b).
In contrast, our model provides a simple and yet
effective solution that shows stronger empirical per-
formance, without relying on additional pretraining
or complex joint training schemes.
DPR has also been used as an important mod-
ule in very recent work. For instance, extending
the idea of leveraging hard negatives, Xiong et al.
(2020a) use the retrieval model trained in the pre-
vious iteration to discover new negatives and con-
struct a different set of examples in each training
iteration. Starting from our trained DPR model,
they show that the retrieval performance can be
further improved. Recent work (Izacard and Grave,
2020; Lewis et al., 2020b) have also shown that
DPR can be combined with generation models
such as BART (Lewis et al., 2020a) and T5 (Raf-
fel et al., 2019), achieving good performance on
open-domain QA and other knowledge-intensive
tasks.
8
Conclusion
In this work, we demonstrated that dense retrieval
can outperform and potentially replace the tradi-
tional sparse retrieval component in open-domain
question answering. While a simple dual-encoder
approach can be made to work surprisingly well,
we showed that there are some critical ingredients
to training a dense retriever successfully. Moreover,
our empirical analysis and ablation studies indicate
that more complex model frameworks or similarity
functions do not necessarily provide additional val-
ues. As a result of improved retrieval performance,
we obtained new state-of-the-art results on multiple
open-domain question answering benchmarks.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments and suggestions.
References
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,
Richard Socher, and Caiming Xiong. 2020. Learn-
ing to retrieve reasoning paths over Wikipedia graph
for question answering. In International Conference
on Learning Representations (ICLR).
Petr Baudiˇs and Jan ˇSediv`y. 2015.
Modeling of the
question answering task in the yodaqa system. In In-
ternational Conference of the Cross-Language Eval-
uation Forum for European Languages, pages 222–
228. Springer.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013.
Semantic parsing on Freebase from

71.48699951171875 | 65.46477508544922 | 292.0742492675781 | 171.2477569580078 | been used as the standard method applied broadly
to various QA tasks (e.g., Chen et al., 2017; Yang
et al., 2019a,b; Nie et al., 2019; Min et al., 2019a;
Wolfson et al., 2020). Augmenting text-based re-
trieval with external structured information, such
as knowledge graph and Wikipedia hyperlinks, has
also been explored recently (Min et al., 2019b; Asai
et al., 2020).
 |  | 
71.60700225830078 | 179.6743621826172 | 292.17559814453125 | 502.2977600097656 | The use of dense vector representations for re-
trieval has a long history since Latent Semantic
Analysis (Deerwester et al., 1990). Using labeled
pairs of queries and documents, discriminatively
trained dense encoders have become popular re-
cently (Yih et al., 2011; Huang et al., 2013; Gillick
et al., 2019), with applications to cross-lingual
document retrieval, ad relevance prediction, Web
search and entity retrieval. Such approaches com-
plement the sparse vector methods as they can po-
tentially give high similarity scores to semantically
relevant text pairs, even without exact token match-
ing. The dense representation alone, however, is
typically inferior to the sparse one. While not the
focus of this work, dense representations from pre-
trained models, along with cross-attention mecha-
nisms, have also been shown effective in passage
or dialogue re-ranking tasks (Nogueira and Cho,
2019; Humeau et al., 2020). Finally, a concurrent
work (Khattab and Zaharia, 2020) demonstrates
the feasibility of full dense retrieval in IR tasks.
Instead of employing the dual-encoder framework,
they introduced a late-interaction operator on top
of the BERT encoders.
 | 1 | 
72.0 | 510.7243347167969 | 292.1755065917969 | 765.6279907226562 | Dense retrieval for open-domain QA has been
explored by Das et al. (2019), who propose to re-
trieve relevant passages iteratively using reformu-
lated question vectors. As an alternative approach
that skips passage retrieval, Seo et al. (2019) pro-
pose to encode candidate answer phrases as vectors
and directly retrieve the answers to the input ques-
tions efﬁciently. Using additional pretraining with
the objective that matches surrogates of questions
and relevant passages, Lee et al. (2019) jointly train
the question encoder and reader. Their approach
outperforms the BM25 plus reader paradigm on
multiple open-domain QA datasets in QA accuracy,
and is further extended by REALM (Guu et al.,
2020), which includes tuning the passage encoder
asynchronously by re-indexing the passages dur-
ing training. The pretraining objective has also
recently been improved by Xiong et al. (2020b).
In contrast, our model provides a simple and yet
 | 2 | 
306.9159851074219 | 65.57683563232422 | 527.4581298828125 | 306.73974609375 | effective solution that shows stronger empirical per-
formance, without relying on additional pretraining
or complex joint training schemes.
DPR has also been used as an important mod-
ule in very recent work. For instance, extending
the idea of leveraging hard negatives, Xiong et al.
(2020a) use the retrieval model trained in the pre-
vious iteration to discover new negatives and con-
struct a different set of examples in each training
iteration. Starting from our trained DPR model,
they show that the retrieval performance can be
further improved. Recent work (Izacard and Grave,
2020; Lewis et al., 2020b) have also shown that
DPR can be combined with generation models
such as BART (Lewis et al., 2020a) and T5 (Raf-
fel et al., 2019), achieving good performance on
open-domain QA and other knowledge-intensive
tasks.
 | 3 | 
307.2760009765625 | 318.6611328125 | 382.34271240234375 | 330.6163330078125 | 8
Conclusion
 | 4 | 
306.88299560546875 | 340.4271240234375 | 527.3584594726562 | 513.9427490234375 | In this work, we demonstrated that dense retrieval
can outperform and potentially replace the tradi-
tional sparse retrieval component in open-domain
question answering. While a simple dual-encoder
approach can be made to work surprisingly well,
we showed that there are some critical ingredients
to training a dense retriever successfully. Moreover,
our empirical analysis and ablation studies indicate
that more complex model frameworks or similarity
functions do not necessarily provide additional val-
ues. As a result of improved retrieval performance,
we obtained new state-of-the-art results on multiple
open-domain question answering benchmarks.
 | 5 | 
307.2760009765625 | 525.8641967773438 | 400.8015441894531 | 537.8193969726562 | Acknowledgments
 | 6 | 
306.76300048828125 | 547.7308349609375 | 525.5418701171875 | 572.105712890625 | We thank the anonymous reviewers for their helpful
comments and suggestions.
 | 7 | 
307.2760009765625 | 596.7271728515625 | 362.8198547363281 | 608.682373046875 | References
 | 8 | 
307.2760009765625 | 616.00146484375 | 527.2003784179688 | 669.800048828125 | Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,
Richard Socher, and Caiming Xiong. 2020. Learn-
ing to retrieve reasoning paths over Wikipedia graph
for question answering. In International Conference
on Learning Representations (ICLR).
 | 9 | 
307.2760009765625 | 678.4454345703125 | 527.2003173828125 | 734.5550537109375 | Petr Baudiˇs and Jan ˇSediv`y. 2015.
Modeling of the
question answering task in the yodaqa system. In In-
ternational Conference of the Cross-Language Eval-
uation Forum for European Languages, pages 222–
228. Springer.
 | 10 | 
307.27606201171875 | 744.4514770507812 | 525.8953857421875 | 765.373046875 | Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013.
Semantic parsing on Freebase from
 | 11 | 
question-answer pairs. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard
S¨ackinger, and Roopak Shah. 1994. Signature veriﬁ-
cation using a “Siamese” time delay neural network.
In NIPS, pages 737–744.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
Proceedings of the 22nd international conference on
Machine learning, pages 89–96.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions.
In Association for Computa-
tional Linguistics (ACL), pages 1870–1879.
Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,
and Andrew McCallum. 2019. Multi-step retriever-
reader interaction for scalable open-domain question
answering. In International Conference on Learn-
ing Representations (ICLR).
Scott Deerwester, Susan T Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391–407.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019.
BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In North American Association for Com-
putational Linguistics (NAACL).
David A Ferrucci. 2012. Introduction to “This is Wat-
son”. IBM Journal of Research and Development,
56(3.4):1–1.
Daniel
Gillick,
Sayali
Kulkarni,
Larry
Lansing,
Alessandro Presta, Jason Baldridge, Eugene Ie, and
Diego Garcia-Olano. 2019. Learning dense repre-
sentations for entity retrieval. In Computational Nat-
ural Language Learning (CoNLL).
Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and
David Simcha. 2016. Quantization based fast inner
product search. In Artiﬁcial Intelligence and Statis-
tics, pages 482–490.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-
supat, and Ming-Wei Chang. 2020.
REALM:
Retrieval-augmented language model pre-training.
ArXiv, abs/2002.08909.
Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-
hsuan Sung, L´aszl´o Luk´acs, Ruiqi Guo, Sanjiv Ku-
mar, Balint Miklos, and Ray Kurzweil. 2017. Efﬁ-
cient natural language response suggestion for smart
reply. ArXiv, abs/1705.00652.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for Web search using
clickthrough data.
In ACM International Confer-
ence on Information and Knowledge Management
(CIKM), pages 2333–2338.
Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,
and Jason Weston. 2020. Poly-encoders: Architec-
tures and pre-training strategies for fast and accurate
multi-sentence scoring. In International Conference
on Learning Representations (ICLR).
Gautier Izacard and Edouard Grave. 2020. Leveraging
passage retrieval with generative models for open do-
main question answering. ArXiv, abs/2007.01282.
Jeff Johnson, Matthijs Douze, and Herv´e J´egou. 2017.
Billion-scale similarity search with GPUs.
ArXiv,
abs/1702.08734.
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017.
TriviaQA: A large scale dis-
tantly supervised challenge dataset for reading com-
prehension. In Association for Computational Lin-
guistics (ACL), pages 1601–1611.
Omar Khattab and Matei Zaharia. 2020.
ColBERT:
Efﬁcient and effective passage search via contextu-
alized late interaction over BERT. In ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval (SIGIR), pages 39–48.
Brian Kulis. 2013. Metric learning: A survey. Foun-
dations and Trends in Machine Learning, 5(4):287–
364.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: a benchmark for question answering
research. Transactions of the Association of Compu-
tational Linguistics (TACL).
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open
domain question answering. In Association for Com-
putational Linguistics (ACL), pages 6086–6096.
Mike
Lewis,
Yinhan
Liu,
Naman
Goyal,
Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020a. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension.
In Association for Computa-
tional Linguistics (ACL), pages 7871–7880.
Patrick Lewis,
Ethan Perez,
Aleksandara Piktus,
Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich
K¨uttler,
Mike
Lewis,
Wen-tau
Yih,
Tim Rockt¨aschel, Sebastian Riedel, and Douwe
Kiela. 2020b. Retrieval-augmented generation for
knowledge-intensive NLP tasks.
In Advances in
Neural Information Processing Systems (NeurIPS).

82.90899658203125 | 66.0350341796875 | 291.9228210449219 | 87.13308715820312 | question-answer pairs. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
 |  | 
72.0 | 97.16047668457031 | 292.0140075683594 | 140.00003051757812 | Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard
S¨ackinger, and Roopak Shah. 1994. Signature veriﬁ-
cation using a “Siamese” time delay neural network.
In NIPS, pages 737–744.
 | 1 | 
72.0 | 150.0274200439453 | 292.01397705078125 | 203.82595825195312 | Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
Proceedings of the 22nd international conference on
Machine learning, pages 89–96.
 | 2 | 
71.99999237060547 | 213.8533477783203 | 291.9254455566406 | 256.6929016113281 | Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions.
In Association for Computa-
tional Linguistics (ACL), pages 1870–1879.
 | 3 | 
71.99999237060547 | 266.72027587890625 | 291.92498779296875 | 320.5188293457031 | Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,
and Andrew McCallum. 2019. Multi-step retriever-
reader interaction for scalable open-domain question
answering. In International Conference on Learn-
ing Representations (ICLR).
 | 4 | 
71.99999237060547 | 330.54620361328125 | 292.0140075683594 | 384.3447570800781 | Scott Deerwester, Susan T Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391–407.
 | 5 | 
71.99996948242188 | 394.37213134765625 | 291.9243469238281 | 448.1706848144531 | Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019.
BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In North American Association for Com-
putational Linguistics (NAACL).
 | 6 | 
71.99995422363281 | 458.19805908203125 | 291.9242858886719 | 490.0786437988281 | David A Ferrucci. 2012. Introduction to “This is Wat-
son”. IBM Journal of Research and Development,
56(3.4):1–1.
 | 7 | 
71.99993896484375 | 500.10601806640625 | 291.9242248535156 | 553.904541015625 | Daniel
Gillick,
Sayali
Kulkarni,
Larry
Lansing,
Alessandro Presta, Jason Baldridge, Eugene Ie, and
Diego Garcia-Olano. 2019. Learning dense repre-
sentations for entity retrieval. In Computational Nat-
ural Language Learning (CoNLL).
 | 8 | 
71.99995422363281 | 563.9319458007812 | 291.92413330078125 | 606.7715454101562 | Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and
David Simcha. 2016. Quantization based fast inner
product search. In Artiﬁcial Intelligence and Statis-
tics, pages 482–490.
 | 9 | 
71.99993896484375 | 616.7989501953125 | 292.01385498046875 | 659.6384887695312 | Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-
supat, and Ming-Wei Chang. 2020.
REALM:
Retrieval-augmented language model pre-training.
ArXiv, abs/2002.08909.
 | 10 | 
71.99993133544922 | 669.6659545898438 | 291.9243469238281 | 723.4645385742188 | Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-
hsuan Sung, L´aszl´o Luk´acs, Ruiqi Guo, Sanjiv Ku-
mar, Balint Miklos, and Ray Kurzweil. 2017. Efﬁ-
cient natural language response suggestion for smart
reply. ArXiv, abs/1705.00652.
 | 11 | 
71.99995422363281 | 733.491943359375 | 291.5158386230469 | 765.3724975585938 | Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for Web search using
 | 12 | 
317.5169982910156 | 66.03448486328125 | 527.1951904296875 | 98.09152221679688 | clickthrough data.
In ACM International Confer-
ence on Information and Knowledge Management
(CIKM), pages 2333–2338.
 | 13 | 
307.2760009765625 | 109.92591857910156 | 527.2003173828125 | 163.72348022460938 | Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,
and Jason Weston. 2020. Poly-encoders: Architec-
tures and pre-training strategies for fast and accurate
multi-sentence scoring. In International Conference
on Learning Representations (ICLR).
 | 14 | 
307.2760009765625 | 175.55787658691406 | 527.2003784179688 | 207.43844604492188 | Gautier Izacard and Edouard Grave. 2020. Leveraging
passage retrieval with generative models for open do-
main question answering. ArXiv, abs/2007.01282.
 | 15 | 
307.2760314941406 | 219.2218780517578 | 527.2894287109375 | 251.15243530273438 | Jeff Johnson, Matthijs Douze, and Herv´e J´egou. 2017.
Billion-scale similarity search with GPUs.
ArXiv,
abs/1702.08734.
 | 16 | 
307.2759704589844 | 262.98583984375 | 527.2002563476562 | 316.7843933105469 | Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017.
TriviaQA: A large scale dis-
tantly supervised challenge dataset for reading com-
prehension. In Association for Computational Lin-
guistics (ACL), pages 1601–1611.
 | 17 | 
307.2759704589844 | 328.6187744140625 | 527.2003784179688 | 382.4163513183594 | Omar Khattab and Matei Zaharia. 2020.
ColBERT:
Efﬁcient and effective passage search via contextu-
alized late interaction over BERT. In ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval (SIGIR), pages 39–48.
 | 18 | 
307.27593994140625 | 394.07427978515625 | 527.1975708007812 | 426.1313171386719 | Brian Kulis. 2013. Metric learning: A survey. Foun-
dations and Trends in Machine Learning, 5(4):287–
364.
 | 19 | 
307.27593994140625 | 437.9646911621094 | 527.20068359375 | 535.5982055664062 | Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: a benchmark for question answering
research. Transactions of the Association of Compu-
tational Linguistics (TACL).
 | 20 | 
307.2759094238281 | 547.4326171875 | 527.2899169921875 | 590.2721557617188 | Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open
domain question answering. In Association for Com-
putational Linguistics (ACL), pages 6086–6096.
 | 21 | 
307.2759094238281 | 602.1055908203125 | 527.2899780273438 | 677.8222045898438 | Mike
Lewis,
Yinhan
Liu,
Naman
Goyal,
Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020a. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension.
In Association for Computa-
tional Linguistics (ACL), pages 7871–7880.
 | 22 | 
307.27587890625 | 689.6555786132812 | 526.7918090820312 | 765.3721923828125 | Patrick Lewis,
Ethan Perez,
Aleksandara Piktus,
Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich
K¨uttler,
Mike
Lewis,
Wen-tau
Yih,
Tim Rockt¨aschel, Sebastian Riedel, and Douwe
Kiela. 2020b. Retrieval-augmented generation for
knowledge-intensive NLP tasks.
In Advances in
Neural Information Processing Systems (NeurIPS).
 | 23 | 
Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun.
2018. Denoising distantly supervised open-domain
question answering.
In Association for Computa-
tional Linguistics (ACL), pages 1736–1745.
Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and
Luke Zettlemoyer. 2019a. A discrete hard EM ap-
proach for weakly supervised question answering.
In Empirical Methods in Natural Language Process-
ing (EMNLP).
Sewon Min, Danqi Chen, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2019b. Knowledge guided text re-
trieval and reading for open domain question answer-
ing. ArXiv, abs/1911.03868.
Dan Moldovan, Marius Pas¸ca, Sanda Harabagiu, and
Mihai Surdeanu. 2003. Performance issues and er-
ror analysis in an open-domain question answering
system. ACM Transactions on Information Systems
(TOIS), 21(2):133–154.
Stephen Mussmann and Stefano Ermon. 2016. Learn-
ing and inference via maximum inner product search.
In International Conference on Machine Learning
(ICML), pages 2587–2596.
Yixin Nie, Songhe Wang, and Mohit Bansal. 2019. Re-
vealing the importance of semantic retrieval for ma-
chine reading at scale. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage
re-ranking with BERT. ArXiv, abs/1901.04085.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former. ArXiv, abs/1910.10683.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016.
SQuAD: 100,000+ questions
for machine comprehension of text.
In Empirical
Methods in Natural Language Processing (EMNLP),
pages 2383–2392.
Parikshit Ram and Alexander G Gray. 2012. Maximum
inner-product search using cone trees. In Proceed-
ings of the 18th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 931–939.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the param-
eters of a language model? ArXiv, abs/2002.08910.
Stephen Robertson and Hugo Zaragoza. 2009.
The
probabilistic relevance framework: BM25 and be-
yond. Foundations and Trends in Information Re-
trieval, 3(4):333–389.
Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur
Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.
Real-time open-domain question answering with
dense-sparse phrase index. In Association for Com-
putational Linguistics (ACL).
Anshumali Shrivastava and Ping Li. 2014. Asymmet-
ric LSH (ALSH) for sublinear time maximum inner
product search (MIPS). In Advances in Neural In-
formation Processing Systems (NIPS), pages 2321–
2329.
Ellen M Voorhees. 1999.
The TREC-8 question an-
swering track report. In TREC, volume 99, pages
77–82.
Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,
Tim Klinger, Wei Zhang, Shiyu Chang, Gerry
Tesauro, Bowen Zhou, and Jing Jiang. 2018. Rˆ3:
Reinforced ranker-reader for open-domain question
answering. In Conference on Artiﬁcial Intelligence
(AAAI).
Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nalla-
pati, and Bing Xiang. 2019. Multi-passage BERT:
A globally normalized bert model for open-domain
question answering. In Empirical Methods in Natu-
ral Language Processing (EMNLP).
Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-
ner, Yoav Goldberg, Daniel Deutch, and Jonathan
Berant. 2020.
Break it down: A question under-
standing benchmark. Transactions of the Associa-
tion of Computational Linguistics (TACL).
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold
Overwijk. 2020a.
Approximate nearest neighbor
negative contrastive learning for dense text retrieval.
ArXiv, abs/2007.00808.
Wenhan Xiong, Hankang Wang, and William Yang
Wang. 2020b. Progressively pretrained dense corpus
index for open-domain question answering. ArXiv,
abs/2005.00038.
Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen
Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a.
End-to-end open-domain question answering with
bertserini. In North American Association for Com-
putational Linguistics (NAACL), pages 72–77.
Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming
Li, and Jimmy Lin. 2019b. Data augmentation for
bert ﬁne-tuning in open-domain question answering.
ArXiv, abs/1904.06652.
Wen-tau Yih, Kristina Toutanova, John C Platt, and
Christopher Meek. 2011.
Learning discriminative
projections for text similarity measures.
In Com-
putational Natural Language Learning (CoNLL),
pages 247–256.

72.0 | 66.21150207519531 | 292.0140075683594 | 109.05105590820312 | Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun.
2018. Denoising distantly supervised open-domain
question answering.
In Association for Computa-
tional Linguistics (ACL), pages 1736–1745.
 |  | 
71.99999237060547 | 118.16548156738281 | 292.01397705078125 | 171.96401977539062 | Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and
Luke Zettlemoyer. 2019a. A discrete hard EM ap-
proach for weakly supervised question answering.
In Empirical Methods in Natural Language Process-
ing (EMNLP).
 | 1 | 
71.99999237060547 | 181.0784454345703 | 291.9243469238281 | 223.91702270507812 | Sewon Min, Danqi Chen, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2019b. Knowledge guided text re-
trieval and reading for open domain question answer-
ing. ArXiv, abs/1911.03868.
 | 2 | 
72.0 | 233.0314483642578 | 291.92431640625 | 286.8299865722656 | Dan Moldovan, Marius Pas¸ca, Sanda Harabagiu, and
Mihai Surdeanu. 2003. Performance issues and er-
ror analysis in an open-domain question answering
system. ACM Transactions on Information Systems
(TOIS), 21(2):133–154.
 | 3 | 
72.0 | 295.94439697265625 | 292.01397705078125 | 338.7839660644531 | Stephen Mussmann and Stefano Ermon. 2016. Learn-
ing and inference via maximum inner product search.
In International Conference on Machine Learning
(ICML), pages 2587–2596.
 | 4 | 
71.99998474121094 | 347.8983459472656 | 291.9244079589844 | 390.7379455566406 | Yixin Nie, Songhe Wang, and Mohit Bansal. 2019. Re-
vealing the importance of semantic retrieval for ma-
chine reading at scale. In Empirical Methods in Nat-
ural Language Processing (EMNLP).
 | 5 | 
72.0 | 399.8523254394531 | 290.2705078125 | 420.7739562988281 | Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage
re-ranking with BERT. ArXiv, abs/1901.04085.
 | 6 | 
72.0 | 429.8883361816406 | 291.92425537109375 | 483.6859436035156 | Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former. ArXiv, abs/1910.10683.
 | 7 | 
72.0 | 492.8003234863281 | 291.5126647949219 | 546.598876953125 | Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016.
SQuAD: 100,000+ questions
for machine comprehension of text.
In Empirical
Methods in Natural Language Processing (EMNLP),
pages 2383–2392.
 | 8 | 
72.0 | 555.7132568359375 | 291.92425537109375 | 609.5119018554688 | Parikshit Ram and Alexander G Gray. 2012. Maximum
inner-product search using cone trees. In Proceed-
ings of the 18th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 931–939.
 | 9 | 
72.0 | 618.6262817382812 | 292.0140380859375 | 650.505859375 | Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the param-
eters of a language model? ArXiv, abs/2002.08910.
 | 10 | 
72.0 | 659.6203002929688 | 291.9254455566406 | 702.4598999023438 | Stephen Robertson and Hugo Zaragoza. 2009.
The
probabilistic relevance framework: BM25 and be-
yond. Foundations and Trends in Information Re-
trieval, 3(4):333–389.
 | 11 | 
72.0 | 711.5742797851562 | 292.0139465332031 | 765.3728637695312 | Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur
Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.
Real-time open-domain question answering with
dense-sparse phrase index. In Association for Com-
putational Linguistics (ACL).
 | 12 | 
307.2760009765625 | 66.21131896972656 | 527.200439453125 | 120.00985717773438 | Anshumali Shrivastava and Ping Li. 2014. Asymmet-
ric LSH (ALSH) for sublinear time maximum inner
product search (MIPS). In Advances in Neural In-
formation Processing Systems (NIPS), pages 2321–
2329.
 | 13 | 
307.2760009765625 | 129.9722442626953 | 527.2003784179688 | 161.85281372070312 | Ellen M Voorhees. 1999.
The TREC-8 question an-
swering track report. In TREC, volume 99, pages
77–82.
 | 14 | 
307.2760009765625 | 171.81520080566406 | 526.9312133789062 | 236.57272338867188 | Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,
Tim Klinger, Wei Zhang, Shiyu Chang, Gerry
Tesauro, Bowen Zhou, and Jing Jiang. 2018. Rˆ3:
Reinforced ranker-reader for open-domain question
answering. In Conference on Artiﬁcial Intelligence
(AAAI).
 | 15 | 
307.2760009765625 | 246.5351104736328 | 527.2003784179688 | 300.3336486816406 | Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nalla-
pati, and Bing Xiang. 2019. Multi-passage BERT:
A globally normalized bert model for open-domain
question answering. In Empirical Methods in Natu-
ral Language Processing (EMNLP).
 | 16 | 
307.2759704589844 | 310.2960205078125 | 527.2003784179688 | 364.0945739746094 | Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-
ner, Yoav Goldberg, Daniel Deutch, and Jonathan
Berant. 2020.
Break it down: A question under-
standing benchmark. Transactions of the Associa-
tion of Computational Linguistics (TACL).
 | 17 | 
307.27593994140625 | 374.05694580078125 | 527.289794921875 | 427.8545227050781 | Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold
Overwijk. 2020a.
Approximate nearest neighbor
negative contrastive learning for dense text retrieval.
ArXiv, abs/2007.00808.
 | 18 | 
307.27593994140625 | 437.8179016113281 | 526.78857421875 | 480.6565246582031 | Wenhan Xiong, Hankang Wang, and William Yang
Wang. 2020b. Progressively pretrained dense corpus
index for open-domain question answering. ArXiv,
abs/2005.00038.
 | 19 | 
307.2759094238281 | 490.6199035644531 | 527.2897338867188 | 544.41748046875 | Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen
Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a.
End-to-end open-domain question answering with
bertserini. In North American Association for Com-
putational Linguistics (NAACL), pages 72–77.
 | 20 | 
307.27587890625 | 554.380859375 | 527.289794921875 | 597.219482421875 | Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming
Li, and Jimmy Lin. 2019b. Data augmentation for
bert ﬁne-tuning in open-domain question answering.
ArXiv, abs/1904.06652.
 | 21 | 
307.27587890625 | 607.1829223632812 | 527.19677734375 | 660.9805297851562 | Wen-tau Yih, Kristina Toutanova, John C Platt, and
Christopher Meek. 2011.
Learning discriminative
projections for text similarity measures.
In Com-
putational Natural Language Learning (CoNLL),
pages 247–256.
 | 22 | 
A
Distant Supervision
When training our ﬁnal DPR model using Natural
Questions, we use the passages in our collection
that best match the gold context as the positive
passages. As some QA datasets contain only the
question and answer pairs, it is thus interesting
to see when using the passages that contain the
answers as positives (i.e., the distant supervision
setting), whether there is a signiﬁcant performance
degradation. Using the question and answer to-
gether as the query, we run Lucene-BM25 and pick
the top passage that contains the answer as the pos-
itive passage. Table 5 shows the performance of
DPR when trained using the original setting and
the distant supervision setting.
B
Alternative Similarity Functions &
Triplet Loss
In addition to dot product (DP) and negative log-
likelihood based on softmax (NLL), we also exper-
iment with Euclidean distance (L2) and the triplet
loss. We negate L2 similarity scores before ap-
plying softmax and change signs of question-to-
positive and question-to-negative similarities when
applying the triplet loss on dot product scores. The
margin value of the triplet loss is set to 1. Ta-
ble 6 summarizes the results. All these additional
experiments are conducted using the same hyper-
parameters tuned for the baseline (DP, NLL).
Note that the retrieval accuracy for our “baseline”
settings reported in Table 5 (Gold) and Table 6
(DP, NLL) is slightly better than those reported in
Table 3. This is due to a better hyper-parameter
setting used in these analysis experiments, which
is documented in our code release.
C
Qualitative Analysis
Although DPR performs better than BM25 in gen-
eral, the retrieved passages of these two retrievers
actually differ qualitatively. Methods like BM25
are sensitive to highly selective keywords and
phrases, but cannot capture lexical variations or se-
mantic relationships well. In contrast, DPR excels
at semantic representation, but might lack sufﬁcient
capacity to represent salient phrases which appear
rarely. Table 7 illustrates this phenomenon with
two examples. In the ﬁrst example, the top scor-
ing passage from BM25 is irrelevant, even though
keywords such as England and Ireland appear mul-
tiple times. In comparison, DPR is able to return
Top-1
Top-5
Top-20
Top-100
Gold
44.9
66.8
78.1
85.0
Dist. Sup.
43.9
65.3
77.1
84.4
Table 5: Retrieval accuracy on the development set of
Natural Questions, trained on passages that match the
gold context (Gold) or the top BM25 passage that con-
tains the answer (Dist. Sup.).
Sim Loss
Retrieval Accuracy
Top-1 Top-5 Top-20 Top-100
DP
NLL
44.9
66.8
78.1
85.0
Triplet
41.6
65.0
77.2
84.5
L2
NLL
43.5
64.7
76.1
83.1
Triplet
42.2
66.0
78.1
84.9
Table 6: Retrieval Top-k accuracy on the development
set of Natural Questions using different similarity and
loss functions.
the correct answer, presumably by matching “body
of water” with semantic neighbors such as sea and
channel, even though no lexical overlap exists. The
second example is one where BM25 does better.
The salient phrase “Thoros of Myr” is critical, and
DPR is unable to capture it.
D
Joint Training of Retriever and
Reader
We ﬁx the passage encoder in our joint-training
scheme while allowing only the question encoder
to receive backpropagation signal from the com-
bined (retriever + reader) loss function. This allows
us to leverage the HNSW-based FAISS index for
efﬁcient low-latency retrieving, without reindexing
the passages during model updates. Our loss func-
tion largely follows ORQA’s approach, which uses
log probabilities of positive passages selected from
the retriever model, and correct spans and passages
selected from the reader model. Since the passage
encoder is ﬁxed, we could use larger amount of
retrieved passages when calculating the retriever
loss. Speciﬁcally, we get top 100 passages for each
question in a mini-batch and use the method similar
to in-batch negative training: all retrieved passages’
vectors participate in the loss calculation for all
questions in a batch. Our training batch size is set
to 16, which effectively gives 1,600 passages per
question to calculate retriever loss. The reader still
uses 24 passages per question, which are selected

72.0 | 64.59117889404297 | 193.10618591308594 | 76.54637908935547 | A
Distant Supervision
 |  | 
71.48699951171875 | 87.91303253173828 | 292.0831604003906 | 274.98175048828125 | When training our ﬁnal DPR model using Natural
Questions, we use the passages in our collection
that best match the gold context as the positive
passages. As some QA datasets contain only the
question and answer pairs, it is thus interesting
to see when using the passages that contain the
answers as positives (i.e., the distant supervision
setting), whether there is a signiﬁcant performance
degradation. Using the question and answer to-
gether as the query, we run Lucene-BM25 and pick
the top passage that contains the answer as the pos-
itive passage. Table 5 shows the performance of
DPR when trained using the original setting and
the distant supervision setting.
 | 1 | 
72.0 | 289.0701904296875 | 269.83465576171875 | 314.973388671875 | B
Alternative Similarity Functions &
Triplet Loss
 | 2 | 
71.63999938964844 | 326.27734375 | 292.205078125 | 554.6717529296875 | In addition to dot product (DP) and negative log-
likelihood based on softmax (NLL), we also exper-
iment with Euclidean distance (L2) and the triplet
loss. We negate L2 similarity scores before ap-
plying softmax and change signs of question-to-
positive and question-to-negative similarities when
applying the triplet loss on dot product scores. The
margin value of the triplet loss is set to 1. Ta-
ble 6 summarizes the results. All these additional
experiments are conducted using the same hyper-
parameters tuned for the baseline (DP, NLL).
Note that the retrieval accuracy for our “baseline”
settings reported in Table 5 (Gold) and Table 6
(DP, NLL) is slightly better than those reported in
Table 3. This is due to a better hyper-parameter
setting used in these analysis experiments, which
is documented in our code release.
 | 3 | 
72.0 | 568.7601928710938 | 195.64068603515625 | 580.7153930664062 | C
Qualitative Analysis
 | 4 | 
71.60700225830078 | 592.0975341796875 | 292.08343505859375 | 765.6279907226562 | Although DPR performs better than BM25 in gen-
eral, the retrieved passages of these two retrievers
actually differ qualitatively. Methods like BM25
are sensitive to highly selective keywords and
phrases, but cannot capture lexical variations or se-
mantic relationships well. In contrast, DPR excels
at semantic representation, but might lack sufﬁcient
capacity to represent salient phrases which appear
rarely. Table 7 illustrates this phenomenon with
two examples. In the ﬁrst example, the top scor-
ing passage from BM25 is irrelevant, even though
keywords such as England and Ireland appear mul-
tiple times. In comparison, DPR is able to return
 | 5 | 
366.50299072265625 | 67.8460464477539 | 520.2777099609375 | 78.7551498413086 | Top-1
Top-5
Top-20
Top-100
 | 6 | 
312.5450134277344 | 86.97164916992188 | 510.9270935058594 | 111.42976379394531 | Gold
44.9
66.8
78.1
85.0
Dist. Sup.
43.9
65.3
77.1
84.4
 | 7 | 
306.9670104980469 | 126.38648986816406 | 527.2002563476562 | 172.21414184570312 | Table 5: Retrieval accuracy on the development set of
Natural Questions, trained on passages that match the
gold context (Gold) or the top BM25 passage that con-
tains the answer (Dist. Sup.).
 | 8 | 
312.7760009765625 | 191.3160858154297 | 520.0448608398438 | 215.87379455566406 | Sim Loss
Retrieval Accuracy
Top-1 Top-5 Top-20 Top-100
 | 9 | 
312.7760009765625 | 223.89109802246094 | 511.2421569824219 | 248.4488067626953 | DP
NLL
44.9
66.8
78.1
85.0
Triplet
41.6
65.0
77.2
84.5
 | 10 | 
312.7760009765625 | 256.5666809082031 | 511.2421569824219 | 281.0247802734375 | L2
NLL
43.5
64.7
76.1
83.1
Triplet
42.2
66.0
78.1
84.9
 | 11 | 
306.9670104980469 | 295.75091552734375 | 525.5464477539062 | 329.8540954589844 | Table 6: Retrieval Top-k accuracy on the development
set of Natural Questions using different similarity and
loss functions.
 | 12 | 
306.93701171875 | 354.4655456542969 | 527.451416015625 | 433.228759765625 | the correct answer, presumably by matching “body
of water” with semantic neighbors such as sea and
channel, even though no lexical overlap exists. The
second example is one where BM25 does better.
The salient phrase “Thoros of Myr” is critical, and
DPR is unable to capture it.
 | 13 | 
307.2760009765625 | 446.7931823730469 | 487.2257080078125 | 472.6963806152344 | D
Joint Training of Retriever and
Reader
 | 14 | 
306.76300048828125 | 483.6263427734375 | 527.3592529296875 | 765.6188354492188 | We ﬁx the passage encoder in our joint-training
scheme while allowing only the question encoder
to receive backpropagation signal from the com-
bined (retriever + reader) loss function. This allows
us to leverage the HNSW-based FAISS index for
efﬁcient low-latency retrieving, without reindexing
the passages during model updates. Our loss func-
tion largely follows ORQA’s approach, which uses
log probabilities of positive passages selected from
the retriever model, and correct spans and passages
selected from the reader model. Since the passage
encoder is ﬁxed, we could use larger amount of
retrieved passages when calculating the retriever
loss. Speciﬁcally, we get top 100 passages for each
question in a mini-batch and use the method similar
to in-batch negative training: all retrieved passages’
vectors participate in the loss calculation for all
questions in a batch. Our training batch size is set
to 16, which effectively gives 1,600 passages per
question to calculate retriever loss. The reader still
uses 24 passages per question, which are selected
 | 15 | 
Question
Passage received by BM25
Passage retrieved by DPR
What is the body of water
between England and Ireland?
Title:British Cycling
Title: Irish Sea
...England is not recognised as a region by the UCI, and
there is no English cycling team outside the Commonwealth
Games. For those occasions, British Cycling selects and sup-
ports the England team. Cycling is represented on the Isle
of Man by the Isle of Man Cycling Association. Cycling in
Northern Ireland is organised under Cycling Ulster, part of
the all-Ireland governing body Cycling Ireland. Until 2006,
a rival governing body existed, . . .
. . . Annual trafﬁc between Great Britain and Ireland amounts
to over 12 million passengers and of traded goods. The Irish
Sea is connected to the North Atlantic at both its northern
and southern ends. To the north, the connection is through
the North Channel between Scotland and Northern Ireland
and the Malin Sea. The southern end is linked to the Atlantic
through the St George’s Channel between Ireland and Pem-
brokeshire, and the Celtic Sea. . . .
Who plays Thoros of Myr in
Game of Thrones?
Title: No One (Game of Thrones)
Title: P˚al Sverre Hagen
...He may be ”no one,” but there’s still enough of a person
left in him to respect, and admire who this girl is and what
she’s become. Arya ﬁnally tells us something that we’ve kind
of known all along, that she’s not no one, she’s Arya Stark
of Winterfell.” ”No One” saw the reintroduction of Richard
Dormer and Paul Kaye, who portrayed Beric Dondarrion and
Thoros of Myr, respectively, in the third season, . . .
P˚al Sverre Valheim Hagen (born 6 November 1980) is a Nor-
wegian stage and screen actor. He appeared in the Norwe-
gian ﬁlm ”Max Manus” and played Thor Heyerdahl in the
Oscar-nominated 2012 ﬁlm ”Kon-Tiki”. Pl Hagen was born
in Stavanger, Norway, the son of Roar Hagen, a Norwegian
cartoonist who has long been associated with Norway´s largest
daily, ”VG”. He lived in Jtten, a neighborhood in the city of
Stavanger in south-western Norway. . . .
Table 7: Examples of passages returned from BM25 and DPR. Correct answers are written in blue and the content
words in the question are written in bold.
from the top 5 positive and top 30 negative passages
(from the set of top 100 passages retrieved from
the same question). The question encoder’s initial
state is taken from a DPR model previously trained
on the NQ dataset. The reader’s initial state is a
BERT-base model. In terms of the end-to-end QA
results, our joint-training scheme does not provide
better results compared to the usual retriever/reader
training pipeline, resulting in the same 39.8 exact
match score on NQ dev as in our regular reader
model training.

77.97799682617188 | 66.96046447753906 | 434.71014404296875 | 73.93425750732422 | Question
Passage received by BM25
Passage retrieved by DPR
 |  | 
77.97799682617188 | 80.47212982177734 | 162.60507202148438 | 95.41590118408203 | What is the body of water
between England and Ireland?
 | 1 | 
175.3709716796875 | 80.47212982177734 | 399.42962646484375 | 151.2057647705078 | Title:British Cycling
Title: Irish Sea
...England is not recognised as a region by the UCI, and
there is no English cycling team outside the Commonwealth
Games. For those occasions, British Cycling selects and sup-
ports the England team. Cycling is represented on the Isle
of Man by the Isle of Man Cycling Association. Cycling in
Northern Ireland is organised under Cycling Ulster, part of
the all-Ireland governing body Cycling Ireland. Until 2006,
a rival governing body existed, . . .
 | 2 | 
357.8030090332031 | 88.37849426269531 | 529.0379638671875 | 151.2057647705078 | . . . Annual trafﬁc between Great Britain and Ireland amounts
to over 12 million passengers and of traded goods. The Irish
Sea is connected to the North Atlantic at both its northern
and southern ends. To the north, the connection is through
the North Channel between Scotland and Northern Ireland
and the Malin Sea. The southern end is linked to the Atlantic
through the St George’s Channel between Ireland and Pem-
brokeshire, and the Celtic Sea. . . .
 | 3 | 
77.97799682617188 | 157.68014526367188 | 157.97445678710938 | 172.62391662597656 | Who plays Thoros of Myr in
Game of Thrones?
 | 4 | 
175.1400146484375 | 157.64517211914062 | 423.5306701660156 | 220.44483947753906 | Title: No One (Game of Thrones)
Title: P˚al Sverre Hagen
...He may be ”no one,” but there’s still enough of a person
left in him to respect, and admire who this girl is and what
she’s become. Arya ﬁnally tells us something that we’ve kind
of known all along, that she’s not no one, she’s Arya Stark
of Winterfell.” ”No One” saw the reintroduction of Richard
Dormer and Paul Kaye, who portrayed Beric Dondarrion and
Thoros of Myr, respectively, in the third season, . . .
 | 5 | 
357.5520324707031 | 165.61520385742188 | 529.0379638671875 | 228.41481018066406 | P˚al Sverre Valheim Hagen (born 6 November 1980) is a Nor-
wegian stage and screen actor. He appeared in the Norwe-
gian ﬁlm ”Max Manus” and played Thor Heyerdahl in the
Oscar-nominated 2012 ﬁlm ”Kon-Tiki”. Pl Hagen was born
in Stavanger, Norway, the son of Roar Hagen, a Norwegian
cartoonist who has long been associated with Norway´s largest
daily, ”VG”. He lived in Jtten, a neighborhood in the city of
Stavanger in south-western Norway. . . .
 | 6 | 
71.64102172851562 | 242.5565185546875 | 525.5474243164062 | 264.5660705566406 | Table 7: Examples of passages returned from BM25 and DPR. Correct answers are written in blue and the content
words in the question are written in bold.
 | 7 | 
71.63999938964844 | 288.4388427734375 | 290.6589050292969 | 434.7567443847656 | from the top 5 positive and top 30 negative passages
(from the set of top 100 passages retrieved from
the same question). The question encoder’s initial
state is taken from a DPR model previously trained
on the NQ dataset. The reader’s initial state is a
BERT-base model. In terms of the end-to-end QA
results, our joint-training scheme does not provide
better results compared to the usual retriever/reader
training pipeline, resulting in the same 39.8 exact
match score on NQ dev as in our regular reader
model training.
 | 8 | 


### Extracted from LangChainDoc.pdf ###

Introduction: 
LangChain is a framework for developing applications powered by large language models 
(LLMs). 
 
LangChain simplifies every stage of the LLM application lifecycle: 
- DevelopmentBuild your applications using LangChain's open-source 
[components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/). 
Use [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-
class streaming and human-in-the-loop support. 
- ProductionizationUse [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor 
and evaluate your applications, so that you can continuously optimize and deploy with 
confidence. 
- Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with 
[LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/). 
LangChain implements a standard interface for large language models and related 
technologies, such as embedding models and vector stores, and integrates with hundreds of 
providers. See the integrations page for more. 
 
pip install -qU "langchain[openai]" 
import getpass 
import os 
 
if not os.environ.get("OPENAI_API_KEY"): 
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ") 
 
from langchain.chat_models import init_chat_model 
 
model = init_chat_model("gpt-4o-mini", model_provider="openai") 
model.invoke("Hello, world!") 
 
Architecture: 
 
The LangChain framework consists of multiple open-source libraries. Read more in the 
[Architecture](/docs/concepts/architecture/) page. 
 
- langchain-core: Base abstractions for chat models and other components. 

72.02400207519531 | 71.95344543457031 | 135.9411163330078 | 85.43328857421875 | Introduction: 
 |  | 
72.02400207519531 | 94.41340637207031 | 492.0252990722656 | 122.41326904296875 | LangChain is a framework for developing applications powered by large language models 
(LLMs). 
 | 1 | 
72.02400207519531 | 131.49342346191406 | 74.26512145996094 | 144.9732666015625 |  
 | 2 | 
72.02400207519531 | 153.9334259033203 | 380.7911071777344 | 167.41326904296875 | LangChain simplifies every stage of the LLM application lifecycle: 
 | 3 | 
72.02400207519531 | 176.37342834472656 | 505.7411193847656 | 204.373291015625 | - DevelopmentBuild your applications using LangChain's open-source 
[components](/docs/concepts) and [third-party integrations](/docs/integrations/providers/). 
 | 4 | 
72.02400207519531 | 213.4534454345703 | 506.5335998535156 | 241.333251953125 | Use [LangGraph](/docs/concepts/architecture/#langgraph) to build stateful agents with first-
class streaming and human-in-the-loop support. 
 | 5 | 
72.02400207519531 | 250.4134063720703 | 504.92529296875 | 292.84326171875 | - ProductionizationUse [LangSmith](https://docs.smith.langchain.com/) to inspect, monitor 
and evaluate your applications, so that you can continuously optimize and deploy with 
confidence. 
 | 6 | 
72.02400207519531 | 301.9234313964844 | 525.3831176757812 | 329.80328369140625 | - Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with 
[LangGraph Platform](https://langchain-ai.github.io/langgraph/cloud/). 
 | 7 | 
72.02400207519531 | 338.8834228515625 | 508.7111511230469 | 381.28326416015625 | LangChain implements a standard interface for large language models and related 
technologies, such as embedding models and vector stores, and integrates with hundreds of 
providers. See the integrations page for more. 
 | 8 | 
72.02400207519531 | 390.3634338378906 | 74.26512145996094 | 403.84326171875 |  
 | 9 | 
72.02400207519531 | 412.8034362792969 | 236.17111206054688 | 426.28326416015625 | pip install -qU "langchain[openai]" 
 | 10 | 
72.02400207519531 | 435.3834228515625 | 445.7411193847656 | 564.7832641601562 | import getpass 
import os 
 
if not os.environ.get("OPENAI_API_KEY"): 
  os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ") 
 
from langchain.chat_models import init_chat_model 
 
model = init_chat_model("gpt-4o-mini", model_provider="openai") 
 | 11 | 
72.02400207519531 | 573.743408203125 | 212.89111328125 | 587.2232666015625 | model.invoke("Hello, world!") 
 | 12 | 
72.02400207519531 | 596.3034057617188 | 74.26512145996094 | 609.7832641601562 |  
 | 13 | 
72.02400207519531 | 618.7734375 | 136.06112670898438 | 632.2532958984375 | Architecture: 
 | 14 | 
72.02400207519531 | 641.21337890625 | 74.26512145996094 | 654.6932373046875 |  
 | 15 | 
72.02400207519531 | 663.7734375 | 481.5011291503906 | 677.2532958984375 | The LangChain framework consists of multiple open-source libraries. Read more in the 
 | 16 | 
72.02400207519531 | 686.21337890625 | 312.4911193847656 | 699.6932373046875 | [Architecture](/docs/concepts/architecture/) page. 
 | 17 | 
72.02400207519531 | 708.7734375 | 74.26512145996094 | 722.2532958984375 |  
 | 18 | 
72.02400207519531 | 731.21337890625 | 432.151123046875 | 744.6932373046875 | - langchain-core: Base abstractions for chat models and other components. 
 | 19 | 
- Integration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations 
have been split into lightweight packages that are co-maintained by the LangChain team and 
the integration developers. 
- langchain: Chains, agents, and retrieval strategies that make up an application's cognitive 
architecture. 
- langchain-community: Third-party integrations that are community maintained. 
- langgraph: Orchestration framework for combining LangChain components into production-
ready applications with persistence, streaming, and other key features. See [LangGraph 
documentation](https://langchain-ai.github.io/langgraph/). 
 
Guides 
 
 [Tutorials](/docs/tutorials) 
 
If you're looking to build something specific or are more of a hands-on learner, check out our 
[tutorials section](/docs/tutorials). 
This is the best place to get started. 
 
These are the best ones to get started with: 
 
- [Build a Simple LLM Application](/docs/tutorials/llm_chain) 
- [Build a Chatbot](/docs/tutorials/chatbot) 
- [Build an Agent](/docs/tutorials/agents) 
- [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/) 
 
Explore the full list of LangChain tutorials [here](/docs/tutorials), and check out other 
[LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more 
about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, 
available [here](https://academy.langchain.com/courses/intro-to-langgraph). 
 
 
 [How-to guides](/docs/how_to) 
 
[Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions. 

72.02400207519531 | 71.95344543457031 | 524.5741577148438 | 114.4932861328125 | - Integration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations 
have been split into lightweight packages that are co-maintained by the LangChain team and 
the integration developers. 
 |  | 
72.02400207519531 | 123.45344543457031 | 501.3310241699219 | 151.4532470703125 | - langchain: Chains, agents, and retrieval strategies that make up an application's cognitive 
architecture. 
 | 1 | 
72.02400207519531 | 160.4134063720703 | 453.901123046875 | 173.89324951171875 | - langchain-community: Third-party integrations that are community maintained. 
 | 2 | 
72.02400207519531 | 182.97340393066406 | 510.7336120605469 | 225.373291015625 | - langgraph: Orchestration framework for combining LangChain components into production-
ready applications with persistence, streaming, and other key features. See [LangGraph 
documentation](https://langchain-ai.github.io/langgraph/). 
 | 3 | 
72.02400207519531 | 234.3334503173828 | 74.26512145996094 | 247.81329345703125 |  
 | 4 | 
72.02400207519531 | 256.9234313964844 | 108.10111999511719 | 270.40325927734375 | Guides 
 | 5 | 
72.02400207519531 | 279.3634338378906 | 74.26512145996094 | 292.84326171875 |  
 | 6 | 
72.02400207519531 | 301.9234313964844 | 199.81112670898438 | 315.40325927734375 |  [Tutorials](/docs/tutorials) 
 | 7 | 
72.02400207519531 | 324.3634338378906 | 74.26512145996094 | 337.84326171875 |  
 | 8 | 
72.02400207519531 | 346.9234313964844 | 507.16693115234375 | 374.8032531738281 | If you're looking to build something specific or are more of a hands-on learner, check out our 
[tutorials section](/docs/tutorials). 
 | 9 | 
72.02400207519531 | 383.8834228515625 | 240.9711151123047 | 397.3632507324219 | This is the best place to get started. 
 | 10 | 
72.02400207519531 | 406.32342529296875 | 74.26512145996094 | 419.8032531738281 |  
 | 11 | 
72.02400207519531 | 428.8834228515625 | 276.1311340332031 | 442.3632507324219 | These are the best ones to get started with: 
 | 12 | 
72.02400207519531 | 451.34344482421875 | 74.26512145996094 | 464.8232727050781 |  
 | 13 | 
72.02400207519531 | 473.783447265625 | 358.95111083984375 | 487.2632751464844 | - [Build a Simple LLM Application](/docs/tutorials/llm_chain) 
 | 14 | 
72.02400207519531 | 496.34344482421875 | 276.1311340332031 | 509.8232727050781 | - [Build a Chatbot](/docs/tutorials/chatbot) 
 | 15 | 
72.02400207519531 | 518.783447265625 | 265.6911315917969 | 532.2633056640625 | - [Build an Agent](/docs/tutorials/agents) 
 | 16 | 
72.02400207519531 | 541.3433837890625 | 515.5811767578125 | 554.8232421875 | - [Introduction to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/) 
 | 17 | 
72.02400207519531 | 563.783447265625 | 74.26512145996094 | 577.2633056640625 |  
 | 18 | 
72.02400207519531 | 586.3433837890625 | 523.169189453125 | 643.2932739257812 | Explore the full list of LangChain tutorials [here](/docs/tutorials), and check out other 
[LangGraph tutorials here](https://langchain-ai.github.io/langgraph/tutorials/). To learn more 
about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, 
available [here](https://academy.langchain.com/courses/intro-to-langgraph). 
 | 19 | 
72.02400207519531 | 652.25341796875 | 74.26512145996094 | 665.7332763671875 |  
 | 20 | 
72.02400207519531 | 674.8134155273438 | 74.26512145996094 | 688.2932739257812 |  
 | 21 | 
72.02400207519531 | 697.25341796875 | 222.7311248779297 | 710.7332763671875 |  [How-to guides](/docs/how_to) 
 | 22 | 
72.02400207519531 | 719.8134155273438 | 74.26512145996094 | 733.2932739257812 |  
 | 23 | 
72.02400207519531 | 742.2493896484375 | 465.7811279296875 | 755.729248046875 | [Here](/docs/how_to) you’ll find short answers to “How do I….?” types of questions. 
 | 24 | 
These how-to guides don’t cover topics in depth – you’ll find that material in the 
[Tutorials](/docs/tutorials) and the [API 
Reference](https://python.langchain.com/api_reference/). 
However, these guides will help you quickly accomplish common tasks using [chat 
models](/docs/how_to/#chat-models), 
[vector stores](/docs/how_to/#vector-stores), and other common LangChain components. 
 
Check out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-
tos/). 
 
 [Conceptual guide](/docs/concepts) 
 
Introductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) 
you'll find high level explanations of all LangChain concepts. 
 
For a deeper dive into LangGraph concepts, check out [this page](https://langchain-
ai.github.io/langgraph/concepts/). 
 
 [Integrations](integrations/providers/index.mdx) 
 
LangChain is part of a rich ecosystem of tools that integrate with our framework and build on 
top of it. 
If you're looking to get up and running quickly with [chat models](/docs/integrations/chat/), 
[vector stores](/docs/integrations/vectorstores/), 
or other LangChain components from a specific provider, check out our growing list of 
[integrations](/docs/integrations/providers/). 
 
 [API reference](https://python.langchain.com/api_reference/) 
Head to the reference section for full documentation of all classes and methods in the 
LangChain Python packages. 
 
Ecosystem 
 
 LangSmith](https://docs.smith.langchain.com) 

72.02400207519531 | 71.95344543457031 | 446.06451416015625 | 114.4932861328125 | These how-to guides don’t cover topics in depth – you’ll find that material in the 
[Tutorials](/docs/tutorials) and the [API 
Reference](https://python.langchain.com/api_reference/). 
 |  | 
72.02400207519531 | 123.45344543457031 | 462.2548522949219 | 151.4532470703125 | However, these guides will help you quickly accomplish common tasks using [chat 
models](/docs/how_to/#chat-models), 
 | 1 | 
72.02400207519531 | 160.4134063720703 | 500.1011047363281 | 173.89324951171875 | [vector stores](/docs/how_to/#vector-stores), and other common LangChain components. 
 | 2 | 
72.02400207519531 | 182.97340393066406 | 74.26512145996094 | 196.4532470703125 |  
 | 3 | 
72.02400207519531 | 205.4134063720703 | 506.5335998535156 | 233.41326904296875 | Check out [LangGraph-specific how-tos here](https://langchain-ai.github.io/langgraph/how-
tos/). 
 | 4 | 
72.02400207519531 | 242.37342834472656 | 74.26512145996094 | 255.853271484375 |  
 | 5 | 
72.02400207519531 | 264.9634094238281 | 247.4511260986328 | 278.4432373046875 |  [Conceptual guide](/docs/concepts) 
 | 6 | 
72.02400207519531 | 287.4034118652344 | 74.26512145996094 | 300.88323974609375 |  
 | 7 | 
72.02400207519531 | 309.8434143066406 | 499.3382873535156 | 337.84326171875 | Introductions to all the key parts of LangChain you’ll need to know! [Here](/docs/concepts) 
you'll find high level explanations of all LangChain concepts. 
 | 8 | 
72.02400207519531 | 346.9234313964844 | 74.26512145996094 | 360.40325927734375 |  
 | 9 | 
72.02400207519531 | 369.3634338378906 | 466.5736083984375 | 397.3632507324219 | For a deeper dive into LangGraph concepts, check out [this page](https://langchain-
ai.github.io/langgraph/concepts/). 
 | 10 | 
72.02400207519531 | 406.32342529296875 | 74.26512145996094 | 419.8032531738281 |  
 | 11 | 
72.02400207519531 | 428.8834228515625 | 302.1711120605469 | 442.3632507324219 |  [Integrations](integrations/providers/index.mdx) 
 | 12 | 
72.02400207519531 | 451.34344482421875 | 74.26512145996094 | 464.8232727050781 |  
 | 13 | 
72.02400207519531 | 473.783447265625 | 507.4525451660156 | 501.78326416015625 | LangChain is part of a rich ecosystem of tools that integrate with our framework and build on 
top of it. 
 | 14 | 
72.02400207519531 | 510.8634338378906 | 499.21673583984375 | 538.7432861328125 | If you're looking to get up and running quickly with [chat models](/docs/integrations/chat/), 
[vector stores](/docs/integrations/vectorstores/), 
 | 15 | 
72.02400207519531 | 547.8233642578125 | 476.8276672363281 | 575.8232421875 | or other LangChain components from a specific provider, check out our growing list of 
[integrations](/docs/integrations/providers/). 
 | 16 | 
72.02400207519531 | 584.7833862304688 | 74.26512145996094 | 598.2632446289062 |  
 | 17 | 
72.02400207519531 | 607.223388671875 | 365.7911071777344 | 620.7032470703125 |  [API reference](https://python.langchain.com/api_reference/) 
 | 18 | 
72.02400207519531 | 629.8134155273438 | 478.6494140625 | 657.813232421875 | Head to the reference section for full documentation of all classes and methods in the 
LangChain Python packages. 
 | 19 | 
72.02400207519531 | 666.7734375 | 74.26512145996094 | 680.2532958984375 |  
 | 20 | 
72.02400207519531 | 689.21337890625 | 126.22112274169922 | 702.6932373046875 | Ecosystem 
 | 21 | 
72.02400207519531 | 711.7734375 | 74.26512145996094 | 725.2532958984375 |  
 | 22 | 
72.02400207519531 | 734.2094116210938 | 297.97113037109375 | 747.6892700195312 |  LangSmith](https://docs.smith.langchain.com) 
 | 23 | 
Trace and evaluate your language model applications and intelligent agents to help you move 
from prototype to production. 
 
LangGraph](https://langchain-ai.github.io/langgraph) 
Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can 
be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, 
Klarna, GitLab, and many more. 
 
Additional resources 
 
 [Versions](/docs/versions/v0_3/) 
See what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, 
and more. 
 
 [Security](/docs/security) 
Read up on [security](/docs/security) best practices to make sure you're developing safely with 
LangChain. 
 
 [Contributing](contributing/index.mdx) 
Check out the developer's guide for guidelines on contributing and help getting your dev 
environment set up. 

72.02400207519531 | 71.95344543457031 | 510.3229675292969 | 99.9732666015625 | Trace and evaluate your language model applications and intelligent agents to help you move 
from prototype to production. 
 |  | 
72.02400207519531 | 108.93342590332031 | 74.26512145996094 | 122.41326904296875 |  
 | 1 | 
72.02400207519531 | 131.49342346191406 | 324.8711242675781 | 144.9732666015625 | LangGraph](https://langchain-ai.github.io/langgraph) 
 | 2 | 
72.02400207519531 | 153.9334259033203 | 521.814697265625 | 196.4532470703125 | Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can 
be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, 
Klarna, GitLab, and many more. 
 | 3 | 
72.02400207519531 | 205.4134063720703 | 74.26512145996094 | 218.89324951171875 |  
 | 4 | 
72.02400207519531 | 227.85340881347656 | 172.06112670898438 | 241.333251953125 | Additional resources 
 | 5 | 
72.02400207519531 | 250.4134063720703 | 74.26512145996094 | 263.89324951171875 |  
 | 6 | 
72.02400207519531 | 272.8834533691406 | 229.33111572265625 | 286.36328125 |  [Versions](/docs/versions/v0_3/) 
 | 7 | 
72.02400207519531 | 295.4434509277344 | 520.5240478515625 | 323.3232421875 | See what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, 
and more. 
 | 8 | 
72.02400207519531 | 332.4034423828125 | 74.26512145996094 | 345.8832702636719 |  
 | 9 | 
72.02400207519531 | 354.84344482421875 | 195.7311248779297 | 368.3232727050781 |  [Security](/docs/security) 
 | 10 | 
72.02400207519531 | 377.4034423828125 | 517.7529296875 | 405.28326416015625 | Read up on [security](/docs/security) best practices to make sure you're developing safely with 
LangChain. 
 | 11 | 
72.02400207519531 | 414.3634338378906 | 74.26512145996094 | 427.84326171875 |  
 | 12 | 
72.02400207519531 | 436.82342529296875 | 259.21112060546875 | 450.3032531738281 |  [Contributing](contributing/index.mdx) 
 | 13 | 
72.02400207519531 | 459.3834228515625 | 485.62646484375 | 487.2632751464844 | Check out the developer's guide for guidelines on contributing and help getting your dev 
environment set up. 
 | 14 | 


### Extracted from Retrieval-Augmented Generation for knowledge intensive nlp tasks.pdf ###

Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks
Patrick Lewis†‡, Ethan Perez⋆,
Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,
Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†
†Facebook AI Research; ‡University College London; ⋆New York University;
plewis@fb.com
Abstract
Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-
stream NLP tasks. However, their ability to access and precisely manipulate knowl-
edge is still limited, and hence on knowledge-intensive tasks, their performance
lags behind task-speciﬁc architectures. Additionally, providing provenance for their
decisions and updating their world knowledge remain open research problems. Pre-
trained models with a differentiable access mechanism to explicit non-parametric
memory have so far been only investigated for extractive downstream tasks. We
explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation
(RAG) — models which combine pre-trained parametric and non-parametric mem-
ory for language generation. We introduce RAG models where the parametric
memory is a pre-trained seq2seq model and the non-parametric memory is a dense
vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-
pare two RAG formulations, one which conditions on the same retrieved passages
across the whole generated sequence, and another which can use different passages
per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-
intensive NLP tasks and set the state of the art on three open domain QA tasks,
outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract
architectures. For language generation tasks, we ﬁnd that RAG models generate
more speciﬁc, diverse and factual language than a state-of-the-art parametric-only
seq2seq baseline.
1
Introduction
Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-
edge from data [47]. They can do so without any access to an external memory, as a parameterized
implicit knowledge base [51, 52]. While this development is exciting, such models do have down-
sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into
their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric
memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these
issues because knowledge can be directly revised and expanded, and accessed knowledge can be
inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that
combine masked language models [8] with a differentiable retriever, have shown promising results,
arXiv:2005.11401v4  [cs.CL]  12 Apr 2021

170.79100036621094 | 99.8338394165039 | 441.58941650390625 | 136.97422790527344 | Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks
 |  | 
240.4459991455078 | 179.57632446289062 | 371.55364990234375 | 191.04908752441406 | Patrick Lewis†‡, Ethan Perez⋆,
 | 1 | 
113.97798156738281 | 207.75637817382812 | 513.1696166992188 | 219.2281036376953 | Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,
 | 2 | 
131.47299194335938 | 235.93539428710938 | 480.0302429199219 | 247.40711975097656 | Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†
 | 3 | 
151.08697509765625 | 262.6103820800781 | 460.9129943847656 | 287.3230895996094 | †Facebook AI Research; ‡University College London; ⋆New York University;
plewis@fb.com
 | 4 | 
283.75799560546875 | 316.6741638183594 | 328.2432861328125 | 328.6293640136719 | Abstract
 | 5 | 
143.53700256347656 | 343.2843017578125 | 469.7906188964844 | 571.5030517578125 | Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-
stream NLP tasks. However, their ability to access and precisely manipulate knowl-
edge is still limited, and hence on knowledge-intensive tasks, their performance
lags behind task-speciﬁc architectures. Additionally, providing provenance for their
decisions and updating their world knowledge remain open research problems. Pre-
trained models with a differentiable access mechanism to explicit non-parametric
memory have so far been only investigated for extractive downstream tasks. We
explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation
(RAG) — models which combine pre-trained parametric and non-parametric mem-
ory for language generation. We introduce RAG models where the parametric
memory is a pre-trained seq2seq model and the non-parametric memory is a dense
vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-
pare two RAG formulations, one which conditions on the same retrieved passages
across the whole generated sequence, and another which can use different passages
per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-
intensive NLP tasks and set the state of the art on three open domain QA tasks,
outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract
architectures. For language generation tasks, we ﬁnd that RAG models generate
more speciﬁc, diverse and factual language than a state-of-the-art parametric-only
seq2seq baseline.
 | 6 | 
108.0 | 596.9281616210938 | 190.8136749267578 | 608.8833618164062 | 1
Introduction
 | 7 | 
108.0 | 623.04638671875 | 505.65142822265625 | 720.2122802734375 | Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-
edge from data [47]. They can do so without any access to an external memory, as a parameterized
implicit knowledge base [51, 52]. While this development is exciting, such models do have down-
sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into
their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric
memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these
issues because knowledge can be directly revised and expanded, and accessed knowledge can be
inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that
combine masked language models [8] with a differentiable retriever, have shown promising results,
 | 8 | 
10.940000534057617 | 212.260009765625 | 37.619998931884766 | 560.0 | arXiv:2005.11401v4  [cs.CL]  12 Apr 2021
 | 9 | 
The Divine
Comedy (x)
q
Query
Encoder
q(x)
MIPS
pθ
Generator pθ
(Parametric)
Margin-
alize
This 14th century work
is divided into 3
sections: "Inferno",
"Purgatorio" &
"Paradiso"         (y)
End-to-End Backprop through q and pθ
Barack Obama was
born in Hawaii.(x)
Fact Veriﬁcation: Fact Query
supports (y)
Question Generation
Fact Veriﬁcation:
Label Generation
Document
Index
Define "middle ear"(x)
Question Answering:
Question Query
The middle ear includes
the tympanic cavity and
the three ossicles.  (y)
Question Answering:
Answer Generation
Retriever pη
(Non-Parametric)
z4
z3
z2
z1
d(z)
Jeopardy Question
Generation:
Answer Query
Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document
Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use
Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we
treat z as a latent variable and marginalize over seq2seq predictions given different documents.
but have only explored open-domain extractive question answering. Here, we bring hybrid parametric
and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.
We endow pre-trained, parametric-memory generation models with a non-parametric memory through
a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).
We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the
non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural
retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The
retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on
the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with
the input to generate the output. We marginalize the latent documents with a top-K approximation,
either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token
basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG
can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.
There has been extensive previous work proposing architectures to enrich systems with non-parametric
memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack-
augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both
parametric and non-parametric memory components are pre-trained and pre-loaded with extensive
knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is
present without additional training.
Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera-
tion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform
without access to an external knowledge source. Our RAG models achieve state-of-the-art results
on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform
recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being
extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.
For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question
generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and
diverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of
state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that
the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1
2
Methods
We explore RAG models, which use the input sequence x to retrieve text documents z and use them
as additional context when generating the target sequence y. As shown in Figure 1, our models
leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/
2

131.82244873046875 | 137.72134399414062 | 194.86895751953125 | 149.4656982421875 | The Divine
Comedy (x)
q
 |  | 
180.57760620117188 | 95.28038787841797 | 205.05062866210938 | 109.656005859375 | Query
Encoder
 | 1 | 
214.58489990234375 | 115.59516906738281 | 228.13658142089844 | 122.98834228515625 | q(x)
 | 2 | 
236.96337890625 | 139.82835388183594 | 397.62677001953125 | 147.88694763183594 | MIPS
pθ
 | 3 | 
371.3753662109375 | 95.24749755859375 | 422.9337463378906 | 105.51579284667969 | Generator pθ
 | 4 | 
381.3356018066406 | 107.00269317626953 | 412.9619445800781 | 112.75293731689453 | (Parametric)
 | 5 | 
405.74847412109375 | 124.75861358642578 | 427.9957580566406 | 130.91958618164062 | Margin-
 | 6 | 
410.1959533691406 | 132.15179443359375 | 423.5529479980469 | 138.31277465820312 | alize
 | 7 | 
434.9425354003906 | 140.59646606445312 | 500.0075988769531 | 170.82376098632812 | This 14th century work
is divided into 3
sections: "Inferno",
"Purgatorio" &
"Paradiso"         (y)
 | 8 | 
238.51644897460938 | 81.50442504882812 | 366.8539733886719 | 89.56303405761719 | End-to-End Backprop through q and pθ
 | 9 | 
116.6253662109375 | 110.20230865478516 | 169.86264038085938 | 121.94667053222656 | Barack Obama was
born in Hawaii.(x)
 | 10 | 
110.17559814453125 | 125.70742797851562 | 172.35690307617188 | 130.63621520996094 | Fact Veriﬁcation: Fact Query
 | 11 | 
448.4966735839844 | 113.4881591796875 | 483.9826354980469 | 119.07154846191406 | supports (y)
 | 12 | 
445.2621765136719 | 174.17379760742188 | 490.7400207519531 | 179.1025848388672 | Question Generation
 | 13 | 
447.03985595703125 | 123.65377807617188 | 484.8485412597656 | 134.3328094482422 | Fact Veriﬁcation:
Label Generation
 | 14 | 
308.03924560546875 | 95.28038787841797 | 337.98651123046875 | 101.85209655761719 | Document
 | 15 | 
314.97674560546875 | 103.08429718017578 | 331.0511474609375 | 109.656005859375 | Index
 | 16 | 
112.92877960205078 | 77.34375762939453 | 177.99383544921875 | 82.9271469116211 | Define "middle ear"(x)
 | 17 | 
120.37329864501953 | 88.33084106445312 | 165.8462371826172 | 93.2596206665039 | Question Answering:
 | 18 | 
125.85399627685547 | 94.08108520507812 | 160.37025451660156 | 99.0098648071289 | Question Query
 | 19 | 
430.835205078125 | 74.8793716430664 | 501.8173828125 | 92.78471374511719 | The middle ear includes
the tympanic cavity and
the three ossicles.  (y)
 | 20 | 
444.036376953125 | 95.72401428222656 | 489.50933837890625 | 100.65279388427734 | Question Answering:
 | 21 | 
232.72769165039062 | 95.6582260131836 | 487.86474609375 | 106.40303802490234 | Answer Generation
Retriever pη
 | 22 | 
234.2230224609375 | 106.59195709228516 | 278.31011962890625 | 112.34220123291016 | (Non-Parametric)
 | 23 | 
327.83135986328125 | 115.84986877441406 | 357.6340637207031 | 146.05511474609375 | z4
z3
z2
z1
 | 24 | 
301.0439453125 | 111.48785400390625 | 314.18157958984375 | 118.88102722167969 | d(z)
 | 25 | 
127.15679168701172 | 152.81573486328125 | 168.5240478515625 | 157.74452209472656 | Jeopardy Question
 | 26 | 
132.2267608642578 | 158.56597900390625 | 163.45059204101562 | 169.24501037597656 | Generation:
Answer Query
 | 27 | 
107.83100128173828 | 192.63172912597656 | 504.0003662109375 | 235.42007446289062 | Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document
Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use
Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we
treat z as a latent variable and marginalize over seq2seq predictions given different documents.
 | 28 | 
108.0 | 257.4353942871094 | 505.58575439453125 | 278.21051025390625 | but have only explored open-domain extractive question answering. Here, we bring hybrid parametric
and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.
 | 29 | 
107.53199768066406 | 284.7334289550781 | 505.74713134765625 | 403.7110595703125 | We endow pre-trained, parametric-memory generation models with a non-parametric memory through
a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).
We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the
non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural
retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The
retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on
the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with
the input to generate the output. We marginalize the latent documents with a top-K approximation,
either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token
basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG
can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.
 | 30 | 
107.69100189208984 | 410.21240234375 | 505.65380859375 | 474.64508056640625 | There has been extensive previous work proposing architectures to enrich systems with non-parametric
memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack-
augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both
parametric and non-parametric memory components are pre-trained and pre-loaded with extensive
knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is
present without additional training.
 | 31 | 
107.64099884033203 | 481.0969543457031 | 505.7451171875 | 600.104736328125 | Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera-
tion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform
without access to an external knowledge source. Our RAG models achieve state-of-the-art results
on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform
recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being
extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.
For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question
generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and
diverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of
state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that
the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1
 | 32 | 
108.0 | 616.6521606445312 | 170.4300537109375 | 628.6073608398438 | 2
Methods
 | 33 | 
107.53199768066406 | 640.8719482421875 | 504.6724548339844 | 684.402587890625 | We explore RAG models, which use the input sequence x to retrieve text documents z and use them
as additional context when generating the target sequence y. As shown in Figure 1, our models
leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated)
distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized
 | 34 | 
108.0 | 691.732666015625 | 505.489501953125 | 722.3895874023438 | 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/
examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/
 | 35 | 
303.5090026855469 | 742.3324584960938 | 308.49029541015625 | 752.2950439453125 | 2
 | 36 | 
by θ that generates a current token based on a context of the previous i −1 tokens y1:i−1, the original
input x and a retrieved passage z.
To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.
We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pη and pθ components, as well as the training and decoding procedure.
2.1
Models
RAG-Sequence Model
The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) ≈
X
z∈top-k(p(·|x))
pη(z|x)pθ(y|x, z) =
X
z∈top-k(p(·|x))
pη(z|x)
N
Y
i
pθ(yi|x, z, y1:i−1)
RAG-Token Model
In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:
pRAG-Token(y|x) ≈
N
Y
i
X
z∈top-k(p(·|x))
pη(z|x)pθ(yi|x, z, y1:i−1)
Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.
2.2
Retriever: DPR
The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
pη(z|x) ∝exp
 d(z)⊤q(x)

d(z) = BERTd(z), q(x) = BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This
retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and
Natural Questions [29]. We refer to the document index as the non-parametric memory.
2.3
Generator: BART
The generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use
BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input
x with the retrieved content z when generating from BART, we simply concatenate them. BART was
pre-trained using a denoising objective and a variety of different noising functions. It has obtained
state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5
models [32]. We refer to the BART generator parameters θ as the parametric memory henceforth.
2.4
Training
We jointly train the retriever and generator components without any direct supervision on what
document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj, yj), we
3

108.0 | 74.04781341552734 | 504.0007629394531 | 95.27908325195312 | by θ that generates a current token based on a context of the previous i −1 tokens y1:i−1, the original
input x and a retrieved passage z.
 |  | 
107.53199768066406 | 101.66304016113281 | 505.7437744140625 | 167.70712280273438 | To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.
We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach, RAG-Sequence, the model uses the same document
to predict each target token. The second approach, RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
pη and pθ components, as well as the training and decoding procedure.
 | 1 | 
108.0 | 180.16851806640625 | 161.4095001220703 | 190.13111877441406 | 2.1
Models
 | 2 | 
108.0 | 200.14349365234375 | 503.9996643066406 | 253.83407592773438 | RAG-Sequence Model
The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
 | 3 | 
143.36199951171875 | 267.1873474121094 | 240.81394958496094 | 280.2132873535156 | pRAG-Sequence(y|x) ≈
X
 | 4 | 
208.15899658203125 | 267.1872863769531 | 347.3009033203125 | 291.3299255371094 | z∈top-k(p(·|x))
pη(z|x)pθ(y|x, z) =
X
 | 5 | 
314.64593505859375 | 269.3597106933594 | 380.1044006347656 | 291.3298645019531 | z∈top-k(p(·|x))
pη(z|x)
 | 6 | 
381.7639465332031 | 259.37066650390625 | 394.49615478515625 | 277.1498718261719 | N
Y
 | 7 | 
386.71893310546875 | 269.3597106933594 | 468.6393737792969 | 290.552490234375 | i
pθ(yi|x, z, y1:i−1)
 | 8 | 
107.99990844726562 | 302.55841064453125 | 505.24267578125 | 356.22509765625 | RAG-Token Model
In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:
 | 9 | 
181.8780059814453 | 371.7738037109375 | 251.23089599609375 | 382.62823486328125 | pRAG-Token(y|x) ≈
 | 10 | 
256.76300048828125 | 361.7857360839844 | 269.4952087402344 | 379.56494140625 | N
Y
 | 11 | 
261.718994140625 | 385.9937438964844 | 264.5364074707031 | 392.9675598144531 | i
 | 12 | 
292.1809997558594 | 369.60235595703125 | 306.57696533203125 | 379.56494140625 | X
 | 13 | 
273.9209899902344 | 371.7748107910156 | 430.1234130859375 | 393.74493408203125 | z∈top-k(p(·|x))
pη(z|x)pθ(yi|x, z, y1:i−1)
 | 14 | 
108.0 | 405.0304260253906 | 504.0014343261719 | 425.8260803222656 | Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.
 | 15 | 
108.0 | 439.7815246582031 | 197.42431640625 | 449.7441101074219 | 2.2
Retriever: DPR
 | 16 | 
107.69100189208984 | 459.4878234863281 | 487.8671875 | 470.42156982421875 | The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
 | 17 | 
154.67300415039062 | 475.0323791503906 | 457.3283996582031 | 487.7045593261719 | pη(z|x) ∝exp
 
d(z)⊤q(x)

d(z) = BERTd(z), q(x) = BERTq(x)
 | 18 | 
107.64099884033203 | 493.3359375 | 505.2452697753906 | 568.9840698242188 | where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],
and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating
top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This
retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and
Natural Questions [29]. We refer to the document index as the non-parametric memory.
 | 19 | 
108.0 | 582.9395141601562 | 208.06436157226562 | 592.902099609375 | 2.3
Generator: BART
 | 20 | 
107.69100189208984 | 602.6458129882812 | 504.0042724609375 | 667.5130615234375 | The generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use
BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input
x with the retrieved content z when generating from BART, we simply concatenate them. BART was
pre-trained using a denoising objective and a variety of different noising functions. It has obtained
state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5
models [32]. We refer to the BART generator parameters θ as the parametric memory henceforth.
 | 21 | 
108.00003051757812 | 681.4695434570312 | 167.32730102539062 | 691.43212890625 | 2.4
Training
 | 22 | 
107.53199768066406 | 701.4602661132812 | 504.00482177734375 | 723.017578125 | We jointly train the retriever and generator components without any direct supervision on what
document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj, yj), we
 | 23 | 
303.5090026855469 | 742.3324584960938 | 308.49029541015625 | 752.2950439453125 | 3
 | 24 | 
minimize the negative marginal log-likelihood of each target, P
j −log p(yj|xj) using stochastic
gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as
it requires the document index to be periodically updated as REALM does during pre-training [20].
We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and
index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.
2.5
Decoding
At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x).
RAG-Token
The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-
tor with transition probability: p′
θ(yi|x, y1:i−1) = P
z∈top-k(p(·|x)) pη(zi|x)pθ(yi|x, zi, y1:i−1) To
decode, we can plug p′
θ(yi|x, y1:i−1) into a standard beam decoder.
RAG-Sequence
For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-
token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for
each document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses
Y , some of which may not have appeared in the beams of all documents. To estimate the probability
of an hypothesis y we run an additional forward pass for each document z for which y does not
appear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across
beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer
output sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding,
we can make a further approximation that pθ(y|x, zi) ≈0 where y was not generated during beam
search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has
been generated. We refer to this decoding procedure as “Fast Decoding.”
3
Experiments
We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use
a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
k documents for each query. We consider k ∈{5, 10} for training and set k for test time using dev
data. We now discuss experimental details for each task.
3.1
Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book
QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.
3.2
Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,
we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages
retrieved from a search engine for each question, and a full sentence answer annotated from the
retrieved passages. We do not use the supplied passages, only the questions and answers, to treat
4

107.53199768066406 | 73.86836242675781 | 505.7430419921875 | 130.68710327148438 | minimize the negative marginal log-likelihood of each target, P
j −log p(yj|xj) using stochastic
gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as
it requires the document index to be periodically updated as REALM does during pre-training [20].
We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and
index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.
 |  | 
107.99998474121094 | 143.57952880859375 | 170.26622009277344 | 153.54212951660156 | 2.5
Decoding
 | 1 | 
107.64099884033203 | 163.28578186035156 | 505.74383544921875 | 175.8125 | At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x).
 | 2 | 
108.0 | 186.6495361328125 | 505.6536560058594 | 214.54495239257812 | RAG-Token
The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-
tor with transition probability: p′
θ(yi|x, y1:i−1) = P
 | 3 | 
108.0 | 197.28981018066406 | 504.0041809082031 | 222.7685546875 | z∈top-k(p(·|x)) pη(zi|x)pθ(yi|x, zi, y1:i−1) To
decode, we can plug p′
θ(yi|x, y1:i−1) into a standard beam decoder.
 | 4 | 
107.64099884033203 | 233.60279846191406 | 505.65814208984375 | 353.0160827636719 | RAG-Sequence
For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-
token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for
each document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of hypotheses
Y , some of which may not have appeared in the beams of all documents. To estimate the probability
of an hypothesis y we run an additional forward pass for each document z for which y does not
appear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across
beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer
output sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding,
we can make a further approximation that pθ(y|x, zi) ≈0 where y was not generated during beam
search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has
been generated. We refer to this decoding procedure as “Fast Decoding.”
 | 5 | 
108.0 | 369.80816650390625 | 191.01690673828125 | 381.76336669921875 | 3
Experiments
 | 6 | 
107.25299835205078 | 394.5651550292969 | 504.2507019042969 | 480.8500671386719 | We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use
a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and
Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
k documents for each query. We consider k ∈{5, 10} for training and set k for test time using dev
data. We now discuss experimental details for each task.
 | 7 | 
108.0 | 495.23651123046875 | 278.21099853515625 | 505.1990966796875 | 3.1
Open-domain Question Answering
 | 8 | 
108.0 | 515.302490234375 | 505.1664733886719 | 634.3560180664062 | Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book
QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.
 | 9 | 
108.0 | 648.7425537109375 | 268.57720947265625 | 658.7051391601562 | 3.2
Abstractive Question Answering
 | 10 | 
107.64099884033203 | 668.8084716796875 | 505.2497863769531 | 722.4310302734375 | RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,
we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages
retrieved from a search engine for each question, and a full sentence answer annotated from the
retrieved passages. We do not use the supplied passages, only the questions and answers, to treat
 | 11 | 
303.5090026855469 | 742.3324584960938 | 308.49029541015625 | 752.2950439453125 | 4
 | 12 | 
MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be
answered in a way that matches the reference answer without access to the gold passages, such as
“What is the weather in Volcano, CA?” so performance will be lower without using gold passages.
We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,
RAG can rely on parametric knowledge to generate reasonable responses.
3.3
Jeopardy Question Generation
To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-
eration. Rather than use questions from standard open-domain QA tasks, which typically consist
of short, simple questions, we propose the more demanding task of generating Jeopardy questions.
Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.
For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst
country to host this international sports competition twice.” As Jeopardy questions are precise,
factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a
challenging knowledge-intensive generation task.
We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As
this is a new task, we train a BART model for comparison. Following [67], we evaluate using the
SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for
matching entities and has higher correlation with human judgment for question generation than
standard metrics. We also perform two human evaluations, one to assess generation factuality, and
one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external
sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow
best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two
generated questions, one from BART and one from RAG. They are then asked to pick one of four
options—quuestion A is better, question B is better, both are good, or neither is good.
3.4
Fact Veriﬁcation
FEVER [56] requires classifying whether a natural language claim is supported or refuted by
Wikipedia, or whether there is not enough information to decide. The task requires retrieving
evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify
whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem
coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for
exploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER
class labels (supports, refutes, or not enough info) to single output tokens and directly train with
claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on
retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and
models that do not require such supervision will be applicable to a wider range of tasks. We explore
two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way
(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.
4
Results
4.1
Open-domain Question Answering
Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA
tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines
the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of
"open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results
without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s
retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-
encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.
There are several advantages to generating answers even when it is possible to extract them. Docu-
ments with clues about the answer but do not contain the answer verbatim can still contribute towards
a correct answer being generated, which is not possible with standard extractive approaches, leading
5

106.67500305175781 | 74.4834213256836 | 505.7395935058594 | 128.00607299804688 | MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be
answered in a way that matches the reference answer without access to the gold passages, such as
“What is the weather in Volcano, CA?” so performance will be lower without using gold passages.
We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,
RAG can rely on parametric knowledge to generate reasonable responses.
 |  | 
108.0 | 141.82049560546875 | 261.4439392089844 | 151.78309631347656 | 3.3
Jeopardy Question Generation
 | 1 | 
107.69100189208984 | 161.81130981445312 | 505.7471618652344 | 248.21310424804688 | To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-
eration. Rather than use questions from standard open-domain QA tasks, which typically consist
of short, simple questions, we propose the more demanding task of generating Jeopardy questions.
Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.
For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst
country to host this international sports competition twice.” As Jeopardy questions are precise,
factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a
challenging knowledge-intensive generation task.
 | 2 | 
107.53199768066406 | 254.56326293945312 | 504.35772705078125 | 362.7830810546875 | We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As
this is a new task, we train a BART model for comparison. Following [67], we evaluate using the
SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for
matching entities and has higher correlation with human judgment for question generation than
standard metrics. We also perform two human evaluations, one to assess generation factuality, and
one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external
sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow
best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two
generated questions, one from BART and one from RAG. They are then asked to pick one of four
options—quuestion A is better, question B is better, both are good, or neither is good.
 | 3 | 
108.0 | 376.5975341796875 | 200.8314971923828 | 386.56011962890625 | 3.4
Fact Veriﬁcation
 | 4 | 
107.53199768066406 | 396.5882873535156 | 504.34979248046875 | 526.6260375976562 | FEVER [56] requires classifying whether a natural language claim is supported or refuted by
Wikipedia, or whether there is not enough information to decide. The task requires retrieving
evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify
whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem
coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for
exploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER
class labels (supports, refutes, or not enough info) to single output tokens and directly train with
claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on
retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and
models that do not require such supervision will be applicable to a wider range of tasks. We explore
two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way
(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.
 | 5 | 
108.0 | 542.8461303710938 | 163.12542724609375 | 554.8013305664062 | 4
Results
 | 6 | 
108.0 | 566.8984985351562 | 278.21099853515625 | 576.861083984375 | 4.1
Open-domain Question Answering
 | 7 | 
107.64099884033203 | 586.8902587890625 | 505.6514587402344 | 684.2000122070312 | Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA
tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines
the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of
"open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results
without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s
retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-
encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.
 | 8 | 
107.69100189208984 | 690.5850219726562 | 505.65325927734375 | 722.396240234375 | There are several advantages to generating answers even when it is possible to extract them. Docu-
ments with clues about the answer but do not contain the answer verbatim can still contribute towards
a correct answer being generated, which is not possible with standard extractive approaches, leading
 | 9 | 
303.5090026855469 | 742.3324584960938 | 308.49029541015625 | 752.2950439453125 | 5
 | 10 | 
Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details.
Model
NQ
TQA
WQ
CT
Closed
Book
T5-11B [52]
34.5
- /50.1
37.4
-
T5-11B+SSM[52]
36.6
- /60.5
44.7
-
Open
Book
REALM [20]
40.4
- / -
40.7
46.8
DPR [26]
41.5
57.9/ -
41.1
50.6
RAG-Token
44.1
55.2/66.1
45.5
50.0
RAG-Seq.
44.5
56.8/68.0
45.2
52.2
Table 2: Generation and classiﬁcation Test Scores.
MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [57] *Uses gold context/evidence.
Best model without gold access underlined.
Model
Jeopardy
MSMARCO FVR3 FVR2
B-1 QB-1
R-L
B-1
Label Acc.
SotA
-
-
49.8* 49.9*
76.8
92.2*
BART
15.1
19.7
38.2
41.6
64.0
81.1
RAG-Tok. 17.3
22.2
40.1
41.5
72.5
89.5
RAG-Seq. 14.7
21.4
40.8
44.2
to more effective marginalization over documents. Furthermore, RAG can generate correct answers
even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such
cases for NQ, where an extractive model would score 0%.
4.2
Abstractive Question Answering
As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu
points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is
impressive given that (i) those models access gold passages with speciﬁc information required to
generate the reference answer , (ii) many questions are unanswerable without the gold passages, and
(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers
from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually
correct text more often than BART. Later, we also show that RAG generations are more diverse than
BART generations (see §4.5).
4.3
Jeopardy Question Generation
Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,
with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452
pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual
than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and
BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on
the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more
speciﬁc by a large margin. Table 3 shows typical generations from each model.
Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform
best because it can generate responses that combine content from several documents. Figure 2 shows
an example. When generating “Sun”, the posterior is high for document 2 which mentions “The
Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is
generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.
This observation suggests that the generator can complete the titles without depending on speciﬁc
documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We
ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The
Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun
Also Rises" indicating the title "The Sun Also Rises" is stored in BART’s parameters. Similarly,
BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A
with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows
how parametric and non-parametric memories work together—the non-parametric component helps
to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.
4.4
Fact Veriﬁcation
Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of
state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and
substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.
6

107.69100189208984 | 79.3233871459961 | 303.697021484375 | 121.93710327148438 | Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details.
 |  | 
141.39306640625 | 131.41307067871094 | 294.24969482421875 | 139.78831481933594 | Model
NQ
TQA
WQ
CT
 | 1 | 
113.81651306152344 | 145.58103942871094 | 290.29119873046875 | 163.26145935058594 | Closed
Book
T5-11B [52]
34.5
- /50.1
37.4
-
T5-11B+SSM[52]
36.6
- /60.5
44.7
-
 | 2 | 
113.81651306152344 | 169.0542449951172 | 296.228759765625 | 186.7356414794922 | Open
Book
REALM [20]
40.4
- / -
40.7
46.8
DPR [26]
41.5
57.9/ -
41.1
50.6
 | 3 | 
143.00991821289062 | 192.45184326171875 | 296.2265930175781 | 210.20970153808594 | RAG-Token
44.1
55.2/66.1
45.5
50.0
RAG-Seq.
44.5
56.8/68.0
45.2
52.2
 | 4 | 
309.64898681640625 | 79.1303939819336 | 505.745849609375 | 121.74404907226562 | Table 2: Generation and classiﬁcation Test Scores.
MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [57] *Uses gold context/evidence.
Best model without gold access underlined.
 | 5 | 
321.6646728515625 | 139.26351928710938 | 499.01568603515625 | 157.29788208007812 | Model
Jeopardy
MSMARCO FVR3 FVR2
B-1 QB-1
R-L
B-1
Label Acc.
 | 6 | 
314.9419250488281 | 163.12863159179688 | 498.18267822265625 | 171.74954223632812 | SotA
-
-
49.8* 49.9*
76.8
92.2*
 | 7 | 
314.9419250488281 | 177.65829467773438 | 496.04327392578125 | 186.20120239257812 | BART
15.1
19.7
38.2
41.6
64.0
81.1
 | 8 | 
314.9419250488281 | 192.03182983398438 | 496.04986572265625 | 210.14523315429688 | RAG-Tok. 17.3
22.2
40.1
41.5
72.5
89.5
RAG-Seq. 14.7
21.4
40.8
44.2
 | 9 | 
108.0 | 235.9834747314453 | 504.00335693359375 | 267.7650451660156 | to more effective marginalization over documents. Furthermore, RAG can generate correct answers
even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such
cases for NQ, where an extractive model would score 0%.
 | 10 | 
108.0 | 282.00250244140625 | 268.57720947265625 | 291.965087890625 | 4.2
Abstractive Question Answering
 | 11 | 
107.64099884033203 | 302.0307922363281 | 504.3529357910156 | 388.39508056640625 | As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu
points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is
impressive given that (i) those models access gold passages with speciﬁc information required to
generate the reference answer , (ii) many questions are unanswerable without the gold passages, and
(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers
from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually
correct text more often than BART. Later, we also show that RAG generations are more diverse than
BART generations (see §4.5).
 | 12 | 
108.0 | 402.63250732421875 | 261.4439392089844 | 412.5950927734375 | 4.3
Jeopardy Question Generation
 | 13 | 
107.64099884033203 | 422.7744140625 | 505.24481201171875 | 498.1160888671875 | Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,
with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452
pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual
than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and
BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on
the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more
speciﬁc by a large margin. Table 3 shows typical generations from each model.
 | 14 | 
107.64099884033203 | 504.5339050292969 | 505.7395324707031 | 656.322021484375 | Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform
best because it can generate responses that combine content from several documents. Figure 2 shows
an example. When generating “Sun”, the posterior is high for document 2 which mentions “The
Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is
generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.
This observation suggests that the generator can complete the titles without depending on speciﬁc
documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We
ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding "The
Sun. BART completes the generation "The Sun Also Rises" is a novel by this author of "The Sun
Also Rises" indicating the title "The Sun Also Rises" is stored in BART’s parameters. Similarly,
BART will complete the partial decoding "The Sun Also Rises" is a novel by this author of "A
with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows
how parametric and non-parametric memories work together—the non-parametric component helps
to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.
 | 15 | 
108.0 | 670.5604858398438 | 200.8314971923828 | 680.5230712890625 | 4.4
Fact Veriﬁcation
 | 16 | 
107.69100189208984 | 690.55126953125 | 505.7393798828125 | 722.3828735351562 | Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of
state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and
substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.
 | 17 | 
303.5090026855469 | 742.3324584960938 | 308.49029541015625 | 752.2950439453125 | 6
 | 18 | 
Document 1: his works are considered classics of American
literature ... His wartime experiences formed the basis for his novel
”A Farewell to Arms” (1929) ...
Document 2: ... artists of the 1920s ”Lost Generation” expatriate
community. His debut novel, ”The Sun Also Rises”, was published
in 1926.
BOS ”
The
Sun
Also
R
ises
”
is
a
novel
by
this
authorof
”
A
Fare
well
to
Arms”
Doc 1
Doc 2
Doc 3
Doc 4
Doc 5
Figure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hem-
ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high
when generating “A Farewell to Arms" and for document 2 when generating “The Sun Also Rises".
Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate
responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.
Task
Input
Model
Generation
MS-
MARCO
deﬁne middle
ear
BART
?The middle ear is the part of the ear between the middle ear and the nose.
RAG-T The middle ear is the portion of the ear internal to the eardrum.
RAG-S The middle ear includes the tympanic cavity and the three ossicles.
what currency
needed in
scotland
BART
The currency needed in Scotland is Pound sterling.
RAG-T Pound is the currency needed in Scotland.
RAG-S The currency needed in Scotland is the pound sterling.
Jeopardy
Question
Gener
-ation
Washington
BART
?This state has the largest number of counties in the U.S.
RAG-T It’s the only U.S. state named for a U.S. president
RAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park
The Divine
Comedy
BART
*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio
RAG-T Dante’s "Inferno" is the ﬁrst part of this epic poem
RAG-S This 14th century work is divided into 3 sections: "Inferno", "Purgatorio" & "Paradiso"
For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]
to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy
within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.
We also analyze whether documents retrieved by RAG correspond to documents annotated as gold
evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved
by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article
in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.
4.5
Additional Results
Generation Diversity
Section 4.3 shows that RAG models are more factual and speciﬁc than
BART for Jeopardy question generation. Following recent work on diversity-promoting decoding
[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to
total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are
more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing
any diversity-promoting decoding.
Retrieval Ablations
A key feature of RAG is learning to retrieve relevant information for the task.
To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever
during training. As shown in Table 6, learned retrieval improves results for all tasks.
We compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace
RAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating
p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are
heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval
improves results on all other tasks, especially for Open-Domain QA, where it is crucial.
Index hot-swapping
An advantage of non-parametric memory models like RAG is that knowledge
can be easily updated at test time. Parametric-only models like T5 or BART need further training to
update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]
Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer
index from our main results (December 2018). We prepare a list of 82 world leaders who had changed
7

110.38406372070312 | 76.00355529785156 | 281.9526062011719 | 96.43010711669922 | Document 1: his works are considered classics of American
literature ... His wartime experiences formed the basis for his novel
”A Farewell to Arms” (1929) ...
 |  | 
110.38406372070312 | 98.76768493652344 | 284.1763610839844 | 119.08995056152344 | Document 2: ... artists of the 1920s ”Lost Generation” expatriate
community. His debut novel, ”The Sun Also Rises”, was published
in 1926.
 | 1 | 
307.3744812011719 | 121.39527130126953 | 325.9622802734375 | 134.49302673339844 | BOS ”
 | 2 | 
325.1069641113281 | 121.40399932861328 | 336.7765197753906 | 133.39208984375 | The
 | 3 | 
333.67523193359375 | 121.40401458740234 | 345.4788818359375 | 133.55191040039062 | Sun
 | 4 | 
341.6978454589844 | 121.40841674804688 | 354.72320556640625 | 135.01223754882812 | Also
 | 5 | 
352.8863525390625 | 121.39524841308594 | 360.81640625 | 128.9267578125 | R
 | 6 | 
359.7525634765625 | 121.40848541259766 | 371.2098083496094 | 133.1435546875 | ises
 | 7 | 
370.4698181152344 | 121.39527130126953 | 377.7741394042969 | 128.18106079101562 | ”
 | 8 | 
378.7048034667969 | 121.3996353149414 | 386.8061828613281 | 129.13531494140625 | is
 | 9 | 
387.7405090332031 | 121.39527130126953 | 395.0448303222656 | 128.18106079101562 | a
 | 10 | 
392.72015380859375 | 121.41283416748047 | 407.32098388671875 | 136.89425659179688 | novel
 | 11 | 
403.8770446777344 | 121.39959716796875 | 413.4459228515625 | 130.88418579101562 | by
 | 12 | 
411.55511474609375 | 121.40845489501953 | 423.0309753417969 | 133.16571044921875 | this
 | 13 | 
417.7154846191406 | 121.3996810913086 | 438.92352294921875 | 139.06484985351562 | authorof
 | 14 | 
439.5523986816406 | 121.39527130126953 | 446.8567199707031 | 128.18106079101562 | ”
 | 15 | 
447.6700134277344 | 121.39517974853516 | 456.009765625 | 129.4149627685547 | A
 | 16 | 
454.1934509277344 | 121.40409088134766 | 466.7494812011719 | 134.4486083984375 | Fare
 | 17 | 
462.8604431152344 | 121.4084243774414 | 475.34942626953125 | 134.373046875 | well
 | 18 | 
473.40093994140625 | 121.39962768554688 | 482.0870666503906 | 129.8322296142578 | to
 | 19 | 
479.0416564941406 | 121.39527130126953 | 498.6685485839844 | 136.96978759765625 | Arms”
 | 20 | 
291.2212829589844 | 75.92112731933594 | 305.77117919921875 | 117.56640625 | Doc 1
Doc 2
Doc 3
Doc 4
Doc 5
 | 21 | 
107.64099884033203 | 139.9497833251953 | 505.65869140625 | 172.09109497070312 | Figure 2: RAG-Token document posterior p(zi|x, yi, y−i) for each generated token for input “Hem-
ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high
when generating “A Farewell to Arms" and for document 2 when generating “The Sun Also Rises".
 | 22 | 
107.69100189208984 | 192.68896484375 | 504.0037841796875 | 213.49710083007812 | Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate
responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.
 | 23 | 
112.93011474609375 | 222.55709838867188 | 251.94635009765625 | 229.97445678710938 | Task
Input
Model
Generation
 | 24 | 
112.93011474609375 | 252.32205200195312 | 140.1295928955078 | 267.981201171875 | MS-
MARCO
 | 25 | 
145.5200653076172 | 239.95980834960938 | 186.10787963867188 | 255.61895751953125 | deﬁne middle
ear
 | 26 | 
192.1788787841797 | 234.56346130371094 | 439.1220397949219 | 259.73944091796875 | BART
?The middle ear is the part of the ear between the middle ear and the nose.
RAG-T The middle ear is the portion of the ear internal to the eardrum.
RAG-S The middle ear includes the tympanic cavity and the three ossicles.
 | 27 | 
145.5200653076172 | 264.7049865722656 | 187.62843322753906 | 288.60595703125 | what currency
needed in
scotland
 | 28 | 
192.1788787841797 | 264.7049865722656 | 380.5947570800781 | 288.60595703125 | BART
The currency needed in Scotland is Pound sterling.
RAG-T Pound is the currency needed in Scotland.
RAG-S The currency needed in Scotland is the pound sterling.
 | 29 | 
111.69918823242188 | 302.7117004394531 | 139.71421813964844 | 334.8536376953125 | Jeopardy
Question
Gener
-ation
 | 30 | 
145.5200653076172 | 293.1940612792969 | 395.778076171875 | 318.37091064453125 | Washington
BART
?This state has the largest number of counties in the U.S.
RAG-T It’s the only U.S. state named for a U.S. president
RAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park
 | 31 | 
145.5200653076172 | 328.1915283203125 | 178.91302490234375 | 343.849853515625 | The Divine
Comedy
 | 32 | 
192.1788787841797 | 322.79437255859375 | 496.8319091796875 | 347.97119140625 | BART
*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio
RAG-T Dante’s "Inferno" is the ﬁrst part of this epic poem
RAG-S This 14th century work is divided into 3 sections: "Inferno", "Purgatorio" & "Paradiso"
 | 33 | 
107.53199768066406 | 373.7102966308594 | 505.7417297363281 | 449.20208740234375 | For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]
to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy
within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.
We also analyze whether documents retrieved by RAG correspond to documents annotated as gold
evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved
by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article
in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.
 | 34 | 
108.0 | 464.3485412597656 | 209.29971313476562 | 474.3111267089844 | 4.5
Additional Results
 | 35 | 
108.0 | 484.6195373535156 | 504.0016784667969 | 549.2190551757812 | Generation Diversity
Section 4.3 shows that RAG models are more factual and speciﬁc than
BART for Jeopardy question generation. Following recent work on diversity-promoting decoding
[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to
total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are
more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing
any diversity-promoting decoding.
 | 36 | 
107.69100189208984 | 563.01953125 | 505.7386474609375 | 594.8910522460938 | Retrieval Ablations
A key feature of RAG is learning to retrieve relevant information for the task.
To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever
during training. As shown in Table 6, learned retrieval improves results for all tasks.
 | 37 | 
107.53199768066406 | 601.2947998046875 | 504.002685546875 | 654.9160766601562 | We compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace
RAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating
p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are
heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval
improves results on all other tasks, especially for Open-Domain QA, where it is crucial.
 | 38 | 
107.53199768066406 | 668.717529296875 | 504.16912841796875 | 722.3828735351562 | Index hot-swapping
An advantage of non-parametric memory models like RAG is that knowledge
can be easily updated at test time. Parametric-only models like T5 or BART need further training to
update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]
Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer
index from our main results (December 2018). We prepare a list of 82 world leaders who had changed
 | 39 | 
303.5090026855469 | 742.3324584960938 | 308.49029541015625 | 752.2950439453125 | 7
 | 40 | 
Table 4: Human assessments for the Jeopardy
Question Generation Task.
Factuality
Speciﬁcity
BART better
7.1%
16.8%
RAG better
42.7%
37.4%
Both good
11.7%
11.8%
Both poor
17.7%
6.9%
No majority
20.8%
20.1%
Table 5: Ratio of distinct to total tri-grams for
generation tasks.
MSMARCO
Jeopardy QGen
Gold
89.6%
90.0%
BART
70.7%
32.4%
RAG-Token
77.8%
46.8%
RAG-Seq.
83.5%
53.8%
Table 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent.
Model
NQ
TQA
WQ
CT
Jeopardy-QGen
MSMarco
FVR-3
FVR-2
Exact Match
B-1
QB-1
R-L
B-1
Label Accuracy
RAG-Token-BM25
29.7
41.5
32.1
33.1
17.5
22.3
55.5
48.4
75.1
91.6
RAG-Sequence-BM25
31.8
44.1
36.6
33.8
11.1
19.5
56.5
46.9
RAG-Token-Frozen
37.8
50.1
37.1
51.1
16.7
21.7
55.9
49.4
72.9
89.4
RAG-Sequence-Frozen
41.2
52.1
41.8
52.6
11.8
19.6
56.7
47.3
RAG-Token
43.5
54.8
46.5
51.9
17.9
22.6
56.2
49.4
74.5
90.6
RAG-Sequence
44.0
55.8
44.9
53.4
15.3
21.5
57.2
47.5
between these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)
to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for
2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched
indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).
This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.
Effect of Retrieving more documents
Models are trained with either 5 or 10 retrieved latent
documents, and we do not observe signiﬁcant differences in performance between them. We have the
ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and
runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves
Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved
documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for
RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.
10
20
30
40
50
K Retrieved Docs
39
40
41
42
43
44
NQ Exact Match
RAG-Tok
RAG-Seq
10
20
30
40
50
K Retrieved Docs
40
50
60
70
80
NQ Answer Recall @ K
RAG-Tok
RAG-Seq
Fixed DPR
BM25
10
20
30
40
50
K Retrieved Docs
48
50
52
54
56
Bleu-1 / Rouge-L score
RAG-Tok R-L
RAG-Tok B-1
RAG-Seq R-L
RAG-Seq B-1
Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-
mance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.
5
Related Work
Single-Task Retrieval
Prior work has shown that retrieval improves performance across a variety of
NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],
fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article
generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our
work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single
retrieval-based architecture is capable of achieving strong performance across several tasks.
8

107.69100189208984 | 78.97927856445312 | 302.39215087890625 | 99.92605590820312 | Table 4: Human assessments for the Jeopardy
Question Generation Task.
 |  | 
190.83700561523438 | 109.65798950195312 | 276.8785400390625 | 118.6243896484375 | Factuality
Speciﬁcity
 | 1 | 
133.16200256347656 | 124.82595825195312 | 270.0319519042969 | 173.64337158203125 | BART better
7.1%
16.8%
RAG better
42.7%
37.4%
Both good
11.7%
11.8%
Both poor
17.7%
6.9%
No majority
20.8%
20.1%
 | 2 | 
309.64898681640625 | 78.97927856445312 | 504.1670837402344 | 99.92605590820312 | Table 5: Ratio of distinct to total tri-grams for
generation tasks.
 | 3 | 
377.5429992675781 | 114.63998413085938 | 497.11895751953125 | 123.60638427734375 | MSMARCO
Jeopardy QGen
 | 4 | 
316.50201416015625 | 129.80795288085938 | 480.6858825683594 | 168.662353515625 | Gold
89.6%
90.0%
BART
70.7%
32.4%
RAG-Token
77.8%
46.8%
RAG-Seq.
83.5%
53.8%
 | 5 | 
107.69100189208984 | 198.30738830566406 | 503.9950866699219 | 208.16986083984375 | Table 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent.
 | 6 | 
144.1719970703125 | 217.92599487304688 | 509.63360595703125 | 236.8543701171875 | Model
NQ
TQA
WQ
CT
Jeopardy-QGen
MSMarco
FVR-3
FVR-2
Exact Match
B-1
QB-1
R-L
B-1
Label Accuracy
 | 7 | 
113.97799682617188 | 243.05599975585938 | 505.02313232421875 | 261.98541259765625 | RAG-Token-BM25
29.7
41.5
32.1
33.1
17.5
22.3
55.5
48.4
75.1
91.6
RAG-Sequence-BM25
31.8
44.1
36.6
33.8
11.1
19.5
56.5
46.9
 | 8 | 
113.97799682617188 | 268.1869812011719 | 505.02313232421875 | 287.11639404296875 | RAG-Token-Frozen
37.8
50.1
37.1
51.1
16.7
21.7
55.9
49.4
72.9
89.4
RAG-Sequence-Frozen
41.2
52.1
41.8
52.6
11.8
19.6
56.7
47.3
 | 9 | 
113.97799682617188 | 293.2361145019531 | 505.02313232421875 | 312.2463684082031 | RAG-Token
43.5
54.8
46.5
51.9
17.9
22.6
56.2
49.4
74.5
90.6
RAG-Sequence
44.0
55.8
44.9
53.4
15.3
21.5
57.2
47.5
 | 10 | 
107.69100189208984 | 340.18017578125 | 505.73956298828125 | 393.74884033203125 | between these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)
to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for
2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched
indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).
This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.
 | 11 | 
108.0 | 409.25250244140625 | 504.17022705078125 | 484.76007080078125 | Effect of Retrieving more documents
Models are trained with either 5 or 10 retrieved latent
documents, and we do not observe signiﬁcant differences in performance between them. We have the
ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and
runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves
Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved
documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for
RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.
 | 12 | 
144.14117431640625 | 568.2509155273438 | 239.41981506347656 | 580.9435424804688 | 10
20
30
40
50
K Retrieved Docs
 | 13 | 
116.494873046875 | 558.1387329101562 | 120.98833465576172 | 563.647705078125 | 39
 | 14 | 
116.494873046875 | 546.9177856445312 | 120.98833465576172 | 552.4267578125 | 40
 | 15 | 
116.494873046875 | 535.69677734375 | 120.98833465576172 | 541.2057495117188 | 41
 | 16 | 
116.494873046875 | 524.4757690429688 | 120.98833465576172 | 529.9847412109375 | 42
 | 17 | 
116.494873046875 | 513.2548217773438 | 120.98833465576172 | 518.7637939453125 | 43
 | 18 | 
116.494873046875 | 502.0338134765625 | 120.98833465576172 | 507.54278564453125 | 44
 | 19 | 
108.08541107177734 | 513.3269653320312 | 114.69617462158203 | 552.705810546875 | NQ Exact Match
 | 20 | 
214.39881896972656 | 549.81494140625 | 233.08705139160156 | 562.0612182617188 | RAG-Tok
RAG-Seq
 | 21 | 
276.3834228515625 | 568.2509155273438 | 371.662109375 | 580.9435424804688 | 10
20
30
40
50
K Retrieved Docs
 | 22 | 
248.7371368408203 | 555.6923828125 | 253.2305908203125 | 561.2013549804688 | 40
 | 23 | 
248.7371368408203 | 542.7027587890625 | 253.2305908203125 | 548.2117309570312 | 50
 | 24 | 
248.7371368408203 | 529.7130737304688 | 253.2305908203125 | 535.2220458984375 | 60
 | 25 | 
248.7371368408203 | 516.723388671875 | 253.2305908203125 | 522.2323608398438 | 70
 | 26 | 
248.7371368408203 | 503.73370361328125 | 253.2305908203125 | 509.24267578125 | 80
 | 27 | 
240.32766723632812 | 505.4193115234375 | 246.9384307861328 | 560.6402587890625 | NQ Answer Recall @ K
 | 28 | 
344.01690673828125 | 536.3402709960938 | 365.3247985839844 | 562.0612182617188 | RAG-Tok
RAG-Seq
Fixed DPR
BM25
 | 29 | 
408.6257019042969 | 568.2509155273438 | 503.90435791015625 | 580.9435424804688 | 10
20
30
40
50
K Retrieved Docs
 | 30 | 
380.9794006347656 | 557.20751953125 | 385.4728698730469 | 562.7164916992188 | 48
 | 31 | 
380.9794006347656 | 544.92626953125 | 385.4728698730469 | 550.4352416992188 | 50
 | 32 | 
380.9794006347656 | 532.64501953125 | 385.4728698730469 | 538.1539916992188 | 52
 | 33 | 
380.9794006347656 | 520.36376953125 | 385.4728698730469 | 525.8727416992188 | 54
 | 34 | 
380.9794006347656 | 508.0824890136719 | 385.4728698730469 | 513.5914916992188 | 56
 | 35 | 
372.5699462890625 | 505.4786682128906 | 379.1806945800781 | 560.5971069335938 | Bleu-1 / Rouge-L score
 | 36 | 
470.5166015625 | 519.6229858398438 | 497.55816650390625 | 545.3439331054688 | RAG-Tok R-L
RAG-Tok B-1
RAG-Seq R-L
RAG-Seq B-1
 | 37 | 
108.0 | 581.7532958984375 | 505.6538391113281 | 602.7000122070312 | Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-
mance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.
 | 38 | 
108.0 | 631.7041625976562 | 197.09014892578125 | 643.6593627929688 | 5
Related Work
 | 39 | 
107.64099884033203 | 657.8085327148438 | 505.2448425292969 | 722.4070434570312 | Single-Task Retrieval
Prior work has shown that retrieval improves performance across a variety of
NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],
fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article
generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our
work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single
retrieval-based architecture is capable of achieving strong performance across several tasks.
 | 40 | 
303.5090026855469 | 742.3324584960938 | 308.49029541015625 | 752.2950439453125 | 8
 | 41 | 
General-Purpose Architectures for NLP
Prior work on general-purpose architectures for NLP
tasks has shown great success without the use of retrieval. A single, pre-trained language model
has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-
marks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained
language model could achieve strong performance across both discriminative and generative tasks.
For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder
model that leverages bi-directional attention to achieve stronger performance on discriminative
and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed
architecture, by learning a retrieval module to augment pre-trained, generative language models.
Learned Retrieval
There is signiﬁcant work on learning to retrieve documents in information
retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some
work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,
using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our
work. These successes leverage different retrieval-based architectures and optimization techniques to
achieve strong performance on a single task, while we show that a single retrieval-based architecture
can be ﬁne-tuned for strong performance on a variety of tasks.
Memory-based Architectures
Our document index can be seen as a large external memory for
neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns
to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our
work. Other work improves the ability of dialog models to generate factual text by attending over
fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather
distributed representations, which makes the memory both (i) human-readable, lending a form of
interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s
memory by editing the document index. This approach has also been used in knowledge-intensive
dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF
rather than end-to-end learnt retrieval [9].
Retrieve-and-Edit approaches
Our method shares some similarities with retrieve-and-edit style
approaches, where a similar training input-output pair is retrieved for a given input, and then edited
to provide a ﬁnal output. These approaches have proved successful in a number of domains including
Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,
including less of emphasis on lightly editing a retrieved item, but on aggregating content from several
pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents
rather than related training pairs. This said, RAG techniques may work well in these settings, and
could represent promising future work.
6
Discussion
In this work, we presented hybrid generation models with access to parametric and non-parametric
memory. We showed that our RAG models obtain state of the art results on open-domain QA. We
found that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual
and speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating
its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model
without requiring any retraining. In future work, it may be fruitful to investigate if the two components
can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some
another objective. Our work opens up new research directions on how parametric and non-parametric
memories interact and how to most effectively combine them, showing promise in being applied to a
wide variety of NLP tasks.
9

108.0 | 74.31652069091797 | 505.7387390136719 | 171.64205932617188 | General-Purpose Architectures for NLP
Prior work on general-purpose architectures for NLP
tasks has shown great success without the use of retrieval. A single, pre-trained language model
has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-
marks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained
language model could achieve strong performance across both discriminative and generative tasks.
For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder
model that leverages bi-directional attention to achieve stronger performance on discriminative
and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed
architecture, by learning a retrieval module to augment pre-trained, generative language models.
 |  | 
107.64099884033203 | 184.7025146484375 | 505.2486877441406 | 260.2100524902344 | Learned Retrieval
There is signiﬁcant work on learning to retrieve documents in information
retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some
work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,
using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our
work. These successes leverage different retrieval-based architectures and optimization techniques to
achieve strong performance on a single task, while we show that a single retrieval-based architecture
can be ﬁne-tuned for strong performance on a variety of tasks.
 | 1 | 
107.64099884033203 | 273.2695007324219 | 504.27490234375 | 381.50506591796875 | Memory-based Architectures
Our document index can be seen as a large external memory for
neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns
to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our
work. Other work improves the ability of dialog models to generate factual text by attending over
fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather
distributed representations, which makes the memory both (i) human-readable, lending a form of
interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s
memory by editing the document index. This approach has also been used in knowledge-intensive
dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF
rather than end-to-end learnt retrieval [9].
 | 2 | 
108.0 | 394.5655212402344 | 505.24603271484375 | 480.9820861816406 | Retrieve-and-Edit approaches
Our method shares some similarities with retrieve-and-edit style
approaches, where a similar training input-output pair is retrieved for a given input, and then edited
to provide a ﬁnal output. These approaches have proved successful in a number of domains including
Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,
including less of emphasis on lightly editing a retrieved item, but on aggregating content from several
pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents
rather than related training pairs. This said, RAG techniques may work well in these settings, and
could represent promising future work.
 | 3 | 
108.0 | 497.7931823730469 | 179.7431640625 | 509.7483825683594 | 6
Discussion
 | 4 | 
107.64099884033203 | 522.496337890625 | 504.003662109375 | 630.6710815429688 | In this work, we presented hybrid generation models with access to parametric and non-parametric
memory. We showed that our RAG models obtain state of the art results on open-domain QA. We
found that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual
and speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating
its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model
without requiring any retraining. In future work, it may be fruitful to investigate if the two components
can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some
another objective. Our work opens up new research directions on how parametric and non-parametric
memories interact and how to most effectively combine them, showing promise in being applied to a
wide variety of NLP tasks.
 | 5 | 
303.5090026855469 | 742.3324584960938 | 308.49029541015625 | 752.2950439453125 | 9
 | 6 | 
Broader Impact
This work offers several positive societal beneﬁts over previous work: the fact that it is more
strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less
with generations that are more factual, and offers more control and interpretability. RAG could be
employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it
with a medical index and asking it open-domain questions on that topic, or by helping people be more
effective at their jobs.
With these advantages also come potential downsides: Wikipedia, or any potential external knowledge
source, will probably never be entirely factual and completely devoid of bias. Since RAG can be
employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably
to a lesser extent, including that it might be used to generate abuse, faked or misleading content in
the news or on social media; to impersonate others; or to automate the production of spam/phishing
content [54]. Advanced language models may also lead to the automation of various jobs in the
coming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against
misleading content and automated spam/phishing.
Acknowledgments
The authors would like to thank the reviewers for their thoughtful and constructive feedback on this
paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors
would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP
thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD
program.
References
[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan
Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina
Stoica, Saurabh Tiwary, and Tong Wang.
MS MARCO: A Human Generated MAchine
Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:
//arxiv.org/abs/1611.09268. arXiv: 1611.09268.
[2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In
International Conference of the Cross-Language Evaluation Forum for European Languages,
pages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%
2F978-3-319-24027-5_20.
[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase
from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013.
Association for Computational Linguistics. URL http://www.aclweb.org/anthology/
D13-1160.
[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-
ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,
2020. URL https://arxiv.org/abs/2004.07159.
[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer
Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada,
July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL
https://www.aclweb.org/anthology/P17-1171.
[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and
Jonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:
10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.
10

108.0 | 72.78716278076172 | 189.79747009277344 | 84.74236297607422 | Broader Impact
 |  | 
107.64099884033203 | 97.52627563476562 | 504.0037841796875 | 162.11007690429688 | This work offers several positive societal beneﬁts over previous work: the fact that it is more
strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less
with generations that are more factual, and offers more control and interpretability. RAG could be
employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it
with a medical index and asking it open-domain questions on that topic, or by helping people be more
effective at their jobs.
 | 1 | 
107.53199768066406 | 168.6114044189453 | 504.3446044921875 | 254.86209106445312 | With these advantages also come potential downsides: Wikipedia, or any potential external knowledge
source, will probably never be entirely factual and completely devoid of bias. Since RAG can be
employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably
to a lesser extent, including that it might be used to generate abuse, faked or misleading content in
the news or on social media; to impersonate others; or to automate the production of spam/phishing
content [54]. Advanced language models may also lead to the automation of various jobs in the
coming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against
misleading content and automated spam/phishing.
 | 2 | 
108.0 | 271.80816650390625 | 201.52552795410156 | 283.76336669921875 | Acknowledgments
 | 3 | 
107.64099884033203 | 296.6262512207031 | 504.0017395019531 | 350.2210693359375 | The authors would like to thank the reviewers for their thoughtful and constructive feedback on this
paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors
would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP
thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD
program.
 | 4 | 
108.0 | 367.1671447753906 | 163.54383850097656 | 379.1223449707031 | References
 | 5 | 
112.98100280761719 | 386.4272766113281 | 506.6208801269531 | 440.35009765625 | [1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan
Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina
Stoica, Saurabh Tiwary, and Tong Wang.
MS MARCO: A Human Generated MAchine
Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:
//arxiv.org/abs/1611.09268. arXiv: 1611.09268.
 | 6 | 
112.98100280761719 | 450.42645263671875 | 505.24505615234375 | 493.4490966796875 | [2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In
International Conference of the Cross-Language Evaluation Forum for European Languages,
pages 222–228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%
2F978-3-319-24027-5_20.
 | 7 | 
112.98100280761719 | 502.34613037109375 | 505.7414245605469 | 556.205078125 | [3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase
from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013.
Association for Computational Linguistics. URL http://www.aclweb.org/anthology/
D13-1160.
 | 8 | 
112.98100280761719 | 565.072998046875 | 505.654296875 | 597.1431274414062 | [4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-
ing&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159,
2020. URL https://arxiv.org/abs/2004.07159.
 | 9 | 
112.98097229003906 | 606.0223388671875 | 505.24176025390625 | 659.8991088867188 | [5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer
Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada,
July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL
https://www.aclweb.org/anthology/P17-1171.
 | 10 | 
112.98100280761719 | 668.7332763671875 | 505.3839111328125 | 722.6550903320312 | [6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and
Jonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:
10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.
 | 11 | 
301.01898193359375 | 742.3324584960938 | 310.9815673828125 | 752.2950439453125 | 10
 | 12 | 
[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-
hension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.
arXiv: 1710.10723.
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://www.aclweb.org/anthology/N19-1423.
[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-
ard of wikipedia: Knowledge-powered conversational agents. In International Conference on
Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.
[10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun
Cho.
SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.
arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:
1704.05179.
[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-
ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational
Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/
P18-1082.
[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:
Long form question answering. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/
anthology/P19-1346.
[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers
with KNN-based composite memory, 2020. URL https://openreview.net/forum?id=
H1gx1CNKPH.
[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.
Entities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202,
2020. URL https://arxiv.org/abs/2004.07202.
[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen
tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI
Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/
AAAI/AAAI18/paper/view/16710.
[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI
exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL
http://arxiv.org/abs/1705.08807.
[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural
machine translation.
In AAAI Conference on Artiﬁcial Intelligence, 2018.
URL https:
//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.
[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural
machine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, 32nd
AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018.
32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018
Through 07-02-2018.
[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by
editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450,
2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.
11

112.98100280761719 | 74.4037094116211 | 505.74383544921875 | 106.18807983398438 | [7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-
hension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723.
arXiv: 1710.10723.
 |  | 
112.98099517822266 | 115.71330261230469 | 505.7413635253906 | 180.5150909423828 | [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://www.aclweb.org/anthology/N19-1423.
 | 1 | 
112.98098754882812 | 189.8374786376953 | 505.74127197265625 | 221.86610412597656 | [9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-
ard of wikipedia: Knowledge-powered conversational agents. In International Conference on
Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.
 | 2 | 
108.0 | 231.12551879882812 | 505.74041748046875 | 273.8790588378906 | [10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun
Cho.
SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.
arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:
1704.05179.
 | 3 | 
108.0 | 283.3195495605469 | 505.6517333984375 | 337.2970886230469 | [11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-
ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational
Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/
P18-1082.
 | 4 | 
107.99996948242188 | 346.61944580078125 | 505.38006591796875 | 400.46710205078125 | [12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:
Long form question answering. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/
anthology/P19-1346.
 | 5 | 
107.99999237060547 | 409.71429443359375 | 504.0301208496094 | 441.8180847167969 | [13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers
with KNN-based composite memory, 2020. URL https://openreview.net/forum?id=
H1gx1CNKPH.
 | 6 | 
107.99999237060547 | 451.1414489746094 | 505.7412414550781 | 483.17010498046875 | [14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.
Entities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202,
2020. URL https://arxiv.org/abs/2004.07202.
 | 7 | 
107.99998474121094 | 492.41729736328125 | 505.08026123046875 | 535.4301147460938 | [15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen
tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI
Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/
AAAI/AAAI18/paper/view/16710.
 | 8 | 
108.0 | 544.67822265625 | 504.4878234863281 | 576.7821044921875 | [16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI
exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL
http://arxiv.org/abs/1705.08807.
 | 9 | 
108.0 | 586.029296875 | 506.62225341796875 | 618.1340942382812 | [17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural
machine translation.
In AAAI Conference on Artiﬁcial Intelligence, 2018.
URL https:
//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.
 | 10 | 
108.0 | 627.3812255859375 | 505.7386169433594 | 681.0550537109375 | [18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural
machine translation. In 32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, 32nd
AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018.
32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018
Through 07-02-2018.
 | 11 | 
108.0 | 690.62646484375 | 505.74383544921875 | 722.6550903320312 | [19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by
editing prototypes. Transactions of the Association for Computational Linguistics, 6:437–450,
2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031.
 | 12 | 
301.0190124511719 | 742.3324584960938 | 310.9815979003906 | 752.2950439453125 | 11
 | 13 | 
[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:
Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:
//arxiv.org/abs/2002.08909.
[21] Tatsunori B Hashimoto,
Kelvin Guu,
Yonatan Oren,
and Percy S Liang.
A
retrieve-and-edit
framework
for
predicting
structured
outputs.
In
S.
Bengio,
H. Wallach,
H. Larochelle,
K. Grauman,
N. Cesa-Bianchi,
and R. Garnett,
ed-
itors,
Advances
in
Neural
Information
Processing
Systems
31,
pages
10052–
10062.
Curran
Associates,
Inc.,
2018.
URL
http://papers.nips.cc/paper/
8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.
pdf.
[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-
edit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/
anthology/2020.acl-main.228.
[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv
preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.
[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale
Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.
doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.
[25] Armand Joulin and Tomas Mikolov.
Inferring algorithmic patterns with stack-
augmented recurrent nets.
In Proceedings of the 28th International Conference on
Neural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cam-
bridge,
MA, USA, 2015. MIT Press.
URL https://papers.nips.cc/paper/
5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.
[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint
arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.
[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-
tion through memorization: Nearest neighbor language models. In International Conference on
Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.
[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arxiv.org/abs/1412.6980.
[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,
Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-
ton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov.
Natural Questions:
a Benchmark for Ques-
tion Answering Research.
Transactions of the Association of Computational Lin-
guistics, 2019.
URL https://tomkwiat.users.x20web.corp.google.com/papers/
natural-questions/main-1455-kwiatkowski.pdf.
[30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and
Herve Jegou.
Large memory layers with product keys.
In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-
formation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http:
//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.
[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised
open domain question answering. In Proceedings of the 57th Annual Meeting of the Association
12

108.0 | 74.33230590820312 | 506.62225341796875 | 106.43610382080078 | [20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:
Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:
//arxiv.org/abs/2002.08909.
 |  | 
108.0 | 115.47628784179688 | 505.6489562988281 | 191.21607971191406 | [21] Tatsunori B Hashimoto,
Kelvin Guu,
Yonatan Oren,
and Percy S Liang.
A
retrieve-and-edit
framework
for
predicting
structured
outputs.
In
S.
Bengio,
H. Wallach,
H. Larochelle,
K. Grauman,
N. Cesa-Bianchi,
and R. Garnett,
ed-
itors,
Advances
in
Neural
Information
Processing
Systems
31,
pages
10052–
10062.
Curran
Associates,
Inc.,
2018.
URL
http://papers.nips.cc/paper/
8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.
pdf.
 | 1 | 
108.00000762939453 | 200.25625610351562 | 505.6553649902344 | 254.17811584472656 | [22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-
edit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/
anthology/2020.acl-main.228.
 | 2 | 
108.0 | 263.1947021484375 | 504.0001525878906 | 284.4130859375 | [23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. arXiv
preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734.
 | 3 | 
108.0 | 293.4532775878906 | 505.74456787109375 | 347.3750915527344 | [24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale
Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.
doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147.
 | 4 | 
108.0 | 356.415283203125 | 507.2718200683594 | 410.33709716796875 | [25] Armand Joulin and Tomas Mikolov.
Inferring algorithmic patterns with stack-
augmented recurrent nets.
In Proceedings of the 28th International Conference on
Neural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cam-
bridge,
MA, USA, 2015. MIT Press.
URL https://papers.nips.cc/paper/
5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.
 | 5 | 
108.0 | 419.4072570800781 | 504.0040283203125 | 451.4810791015625 | [26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint
arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906.
 | 6 | 
107.99996948242188 | 460.596435546875 | 505.74127197265625 | 492.6250915527344 | [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-
tion through memorization: Nearest neighbor language models. In International Conference on
Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.
 | 7 | 
108.0 | 501.687744140625 | 505.73974609375 | 544.6781005859375 | [28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arxiv.org/abs/1412.6980.
 | 8 | 
108.0 | 553.71826171875 | 505.6513977050781 | 629.4580688476562 | [29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,
Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-
ton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov.
Natural Questions:
a Benchmark for Ques-
tion Answering Research.
Transactions of the Association of Computational Lin-
guistics, 2019.
URL https://tomkwiat.users.x20web.corp.google.com/papers/
natural-questions/main-1455-kwiatkowski.pdf.
 | 9 | 
107.99998474121094 | 638.498291015625 | 506.6208801269531 | 692.4201049804688 | [30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and
Herve Jegou.
Large memory layers with product keys.
In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-
formation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URL http:
//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.
 | 10 | 
108.0 | 701.5354614257812 | 504.00372314453125 | 722.3828735351562 | [31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised
open domain question answering. In Proceedings of the 57th Annual Meeting of the Association
 | 11 | 
301.0190124511719 | 742.3324584960938 | 310.9815979003906 | 752.2950439453125 | 12
 | 12 | 
for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/
anthology/P19-1612.
[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehension. arXiv preprint
arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.
[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting
objective function for neural conversation models. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational
Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/
N16-1014.
[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation
with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL
https://arxiv.org/abs/1909.03087.
[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine
translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy,
July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL
https://www.aclweb.org/anthology/P19-1291.
[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,
and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=Hyg0vbWC-.
[37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search
using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.
[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv
preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.
[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis
Plachouras, Fabrizio Silvestri, and Sebastian Riedel.
How decoding strategies affect the
veriﬁability of generated text.
arXiv preprint arXiv:1911.03587, 2019.
URL https:
//arxiv.org/abs/1911.03587.
[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed
precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.
[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-
ing background knowledge for building conversation systems. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus-
sels, Belgium, October-November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.
[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation
systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/
anthology/D18-1429.
[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,
and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In
Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,
Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic
13

129.57899475097656 | 74.15408325195312 | 505.0647277832031 | 106.43610382080078 | for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/
anthology/P19-1612.
 |  | 
107.99999237060547 | 114.77432250976562 | 505.24249267578125 | 157.78712463378906 | [32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehension. arXiv preprint
arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461.
 | 1 | 
107.99996948242188 | 166.19393920898438 | 505.0774841308594 | 230.9571075439453 | [33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting
objective function for neural conversation models. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational
Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/
N16-1014.
 | 2 | 
107.99996948242188 | 239.29531860351562 | 504.48553466796875 | 271.40008544921875 | [34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation
with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL
https://arxiv.org/abs/1909.03087.
 | 3 | 
107.99996948242188 | 279.8134460449219 | 505.2491149902344 | 333.66009521484375 | [35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine
translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy,
July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL
https://www.aclweb.org/anthology/P19-1291.
 | 4 | 
108.00001525878906 | 341.9992980957031 | 505.2425842285156 | 385.0120849609375 | [36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,
and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=Hyg0vbWC-.
 | 5 | 
107.99999237060547 | 393.4254455566406 | 504.0032653808594 | 425.4541015625 | [37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search
using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 42:824–836, 2016. URL https://arxiv.org/abs/1603.09320.
 | 6 | 
108.0 | 433.6150817871094 | 504.0006408691406 | 454.98809814453125 | [38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence. arXiv
preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177.
 | 7 | 
108.0 | 463.3262939453125 | 506.62225341796875 | 506.340087890625 | [39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis
Plachouras, Fabrizio Silvestri, and Sebastian Riedel.
How decoding strategies affect the
veriﬁability of generated text.
arXiv preprint arXiv:1911.03587, 2019.
URL https:
//arxiv.org/abs/1911.03587.
 | 8 | 
108.0 | 514.75341796875 | 505.6748352050781 | 546.7821044921875 | [40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed
precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.
 | 9 | 
108.0 | 555.1202392578125 | 505.6558532714844 | 609.0430908203125 | [41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-
ing background knowledge for building conversation systems. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus-
sels, Belgium, October-November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.
 | 10 | 
108.0 | 617.4564208984375 | 505.0647277832031 | 671.3031005859375 | [42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation
systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/
anthology/D18-1429.
 | 11 | 
107.99999237060547 | 679.6422729492188 | 505.2486572265625 | 722.2528076171875 | [43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,
and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In
Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,
Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic
 | 12 | 
301.0190124511719 | 742.3324584960938 | 310.9815979003906 | 752.2950439453125 | 13
 | 13 | 
approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing
Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop
Proceedings. CEUR-WS.org, 2016.
URL http://ceur-ws.org/Vol-1773/CoCoNIPS_
2016_paper9.pdf.
[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint
arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.
[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association
for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.
org/anthology/N19-4009.
[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun
Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics.
doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.
[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong
Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/
D19-1250. URL https://www.aclweb.org/anthology/D19-1250.
[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H.
Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In
Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?
id=025X0zPfn.
[49] Alec
Radford,
Karthik
Narasimhan,
Tim
Salimans,
and
Ilya
Sutskever.
Im-
proving
Language
Understanding
by
Generative
Pre-Training,
2018.
URL
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/
language-unsupervised/language_understanding_paper.pdf.
[50] Alec
Radford,
Jeff
Wu,
Rewon
Child,
David
Luan,
Dario
Amodei,
and
Ilya
Sutskever.
Language models are unsupervised multitask learners,
2019.
URL
https://d4mucfpksywv.cloudfront.net/better-language-models/language_
models_are_unsupervised_multitask_learners.pdf.
[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.
[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into
the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/
2002.08910.
[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and
beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/
1500000019. URL https://doi.org/10.1561/1500000019.
[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec
Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models.
ArXiv, abs/1908.09203, 2019.
[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-
works. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances
in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.
URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.
14

129.27000427246094 | 74.30873107910156 | 504.5602722167969 | 117.34510040283203 | approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing
Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop
Proceedings. CEUR-WS.org, 2016.
URL http://ceur-ws.org/Vol-1773/CoCoNIPS_
2016_paper9.pdf.
 |  | 
107.99999237060547 | 124.59609985351562 | 503.9974670410156 | 145.96913146972656 | [44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint
arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085.
 | 1 | 
107.99996948242188 | 153.47352600097656 | 507.6841735839844 | 218.23011779785156 | [45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association
for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.
org/anthology/N19-4009.
 | 2 | 
108.00001525878906 | 225.73451232910156 | 505.7397155761719 | 290.4910888671875 | [46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun
Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics.
doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.
 | 3 | 
107.99996948242188 | 297.9954528808594 | 505.2475280761719 | 362.7510986328125 | [47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong
Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/
D19-1250. URL https://www.aclweb.org/anthology/D19-1250.
 | 4 | 
108.0 | 370.2564697265625 | 505.7405090332031 | 413.194091796875 | [48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H.
Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In
Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum?
id=025X0zPfn.
 | 5 | 
108.0 | 420.623291015625 | 505.64892578125 | 463.6360778808594 | [49] Alec
Radford,
Karthik
Narasimhan,
Tim
Salimans,
and
Ilya
Sutskever.
Im-
proving
Language
Understanding
by
Generative
Pre-Training,
2018.
URL
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/
language-unsupervised/language_understanding_paper.pdf.
 | 6 | 
107.99996948242188 | 471.0662841796875 | 504.49053955078125 | 514.0791015625 | [50] Alec
Radford,
Jeff
Wu,
Rewon
Child,
David
Luan,
Dario
Amodei,
and
Ilya
Sutskever.
Language models are unsupervised multitask learners,
2019.
URL
https://d4mucfpksywv.cloudfront.net/better-language-models/language_
models_are_unsupervised_multitask_learners.pdf.
 | 7 | 
107.99996948242188 | 521.5608520507812 | 505.72869873046875 | 553.6121215820312 | [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683.
 | 8 | 
108.0 | 561.041259765625 | 505.0633544921875 | 593.1460571289062 | [52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into
the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/
2002.08910.
 | 9 | 
108.00000762939453 | 600.5752563476562 | 504.56256103515625 | 632.6790771484375 | [53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and
beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/
1500000019. URL https://doi.org/10.1561/1500000019.
 | 10 | 
107.99996948242188 | 640.1834716796875 | 505.7405090332031 | 671.9640502929688 | [54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec
Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models.
ArXiv, abs/1908.09203, 2019.
 | 11 | 
107.99998474121094 | 679.7174682617188 | 505.7422180175781 | 722.6550903320312 | [55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-
works. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances
in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.
URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.
 | 12 | 
301.0190124511719 | 742.3324584960938 | 310.9815979003906 | 752.2950439453125 | 14
 | 13 | 
[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a
large-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana,
June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL
https://www.aclweb.org/anthology/N18-1074.
[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model
biases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv, abs/2004.14366,
2020. URL https://arxiv.org/abs/2004.14366.
[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.
[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.
AAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/17329.
[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding.
In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/
anthology/W18-5446.
[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-
Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information
Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://
arxiv.org/abs/1905.00537.
[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,
Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain
question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of
the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative
Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,
2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/16712.
[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,
Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-
ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.
net/forum?id=rJl3yM-Ab.
[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio
and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arxiv.org/abs/1410.3916.
[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence
generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd
International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium,
October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL
https://www.aclweb.org/anthology/W18-5713.
15

108.0 | 74.33230590820312 | 505.2409362792969 | 139.16310119628906 | [56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a
large-scale dataset for fact extraction and VERiﬁcation. In Proceedings of the 2018 Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana,
June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL
https://www.aclweb.org/anthology/N18-1074.
 |  | 
108.00001525878906 | 149.82830810546875 | 505.24603271484375 | 181.88710021972656 | [57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model
biases in sentence-pair classiﬁcation with elastic weight consolidation. ArXiv, abs/2004.14366,
2020. URL https://arxiv.org/abs/2004.14366.
 | 1 | 
107.99998474121094 | 192.5560760498047 | 505.24249267578125 | 246.42909240722656 | [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.
 | 2 | 
108.0 | 257.12445068359375 | 507.68975830078125 | 300.06207275390625 | [59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.
AAAI Conference on Artiﬁcial Intelligence, 2018. URL https://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/17329.
 | 3 | 
108.0 | 310.6812744140625 | 505.7405700683594 | 375.5130920410156 | [60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding.
In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/
anthology/W18-5446.
 | 4 | 
107.99999237060547 | 386.13330078125 | 505.64886474609375 | 450.9640808105469 | [61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-
Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information
Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://
arxiv.org/abs/1905.00537.
 | 5 | 
108.0 | 461.6594543457031 | 507.68975830078125 | 550.8471069335938 | [62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,
Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain
question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of
the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative
Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,
2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/16712.
 | 6 | 
108.0 | 561.4935302734375 | 507.6841735839844 | 604.4801025390625 | [63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,
Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-
ranking in open-domain question answering. In ICLR, 2018. URL https://openreview.
net/forum?id=rJl3yM-Ab.
 | 7 | 
108.00001525878906 | 615.1002807617188 | 504.4930725097656 | 658.1130981445312 | [64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio
and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arxiv.org/abs/1410.3916.
 | 8 | 
108.0 | 668.7332763671875 | 505.24517822265625 | 722.6550903320312 | [65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence
generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd
International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium,
October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL
https://www.aclweb.org/anthology/W18-5713.
 | 9 | 
301.0190124511719 | 742.3324584960938 | 310.9815979003906 | 752.2950439453125 | 15
 | 10 | 
[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers:
State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.
[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-
supervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, Novem-
ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL
https://www.aclweb.org/anthology/D19-1253.
[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and
Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019.
URL https://arxiv.org/abs/1909.03745.
16

108.0 | 74.34353637695312 | 505.3800354003906 | 128.00607299804688 | [66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers:
State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.
 |  | 
108.00001525878906 | 136.84732055664062 | 505.658447265625 | 201.67909240722656 | [67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-
supervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, Novem-
ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL
https://www.aclweb.org/anthology/D19-1253.
 | 1 | 
108.00001525878906 | 210.33993530273438 | 505.7428283691406 | 242.37611389160156 | [68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and
Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019.
URL https://arxiv.org/abs/1909.03745.
 | 2 | 
301.01898193359375 | 742.3324584960938 | 310.9815673828125 | 752.2950439453125 | 16
 | 3 | 
Appendices for Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks
A
Implementation Details
For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.
For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the
Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as
we did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,
we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,
and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast
Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.
B
Human Evaluation
Figure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions
and a worked example appear when clicking "view tool guide".
Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position,
which model corresponded to sentence A and sentence B was randomly selected for each example.
Annotators were encouraged to research the topic using the internet, and were given detailed instruc-
tions and worked examples in a full instructions tab. We included some gold sentences in order to
assess the accuracy of the annotators. Two annotators did not perform well on these examples and
their annotations were removed from the results.
C
Training setup Details
We train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision
ﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though
training and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search
with FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring ∼100
GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace
Transformers [66]3, which achieves equivalent performance to the previous version but is a cleaner
and easier to use implementation. This version is also open-sourced. We also compress the document
index using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to
run experiments with RAG can be found at https://github.com/huggingface/transformers/
blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found
at https://huggingface.co/rag/
2https://github.com/pytorch/fairseq
3https://github.com/huggingface/transformers
17

155.3159942626953 | 70.82889556884766 | 456.9295349121094 | 101.36920928955078 | Appendices for Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks
 |  | 
107.99998474121094 | 124.46318817138672 | 248.4616241455078 | 136.41839599609375 | A
Implementation Details
 | 1 | 
107.64099884033203 | 149.4071807861328 | 505.74713134765625 | 224.77685546875 | For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.
For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the
Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as
we did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation,
we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,
and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast
Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.
 | 2 | 
108.0 | 241.92413330078125 | 225.13706970214844 | 253.87933349609375 | B
Human Evaluation
 | 3 | 
108.0 | 429.2389831542969 | 503.9993591308594 | 450.0470886230469 | Figure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions
and a worked example appear when clicking "view tool guide".
 | 4 | 
107.64099884033203 | 466.2803039550781 | 505.74407958984375 | 530.863037109375 | Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position,
which model corresponded to sentence A and sentence B was randomly selected for each example.
Annotators were encouraged to research the topic using the internet, and were given detailed instruc-
tions and worked examples in a full instructions tab. We included some gold sentences in order to
assess the accuracy of the annotators. Two annotators did not perform well on these examples and
their annotations were removed from the results.
 | 5 | 
108.0 | 547.9861450195312 | 241.2885284423828 | 559.9413452148438 | C
Training setup Details
 | 6 | 
107.53199768066406 | 569.2162475585938 | 505.0916442871094 | 692.2080688476562 | We train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision
ﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though
training and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search
with FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring ∼100
GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace
Transformers [66]3, which achieves equivalent performance to the previous version but is a cleaner
and easier to use implementation. This version is also open-sourced. We also compress the document
index using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to
run experiments with RAG can be found at https://github.com/huggingface/transformers/
blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found
at https://huggingface.co/rag/
 | 7 | 
120.65299987792969 | 700.806640625 | 326.54779052734375 | 722.3895874023438 | 2https://github.com/pytorch/fairseq
3https://github.com/huggingface/transformers
 | 8 | 
301.01898193359375 | 742.3324584960938 | 310.9815673828125 | 752.2950439453125 | 17
 | 9 | 
D
Further Details on Open-Domain QA
For open-domain QA, multiple answer annotations are often available for a given question. These
answer annotations are exploited by extractive models during training as typically all the answer
annotations are used to ﬁnd matches within documents when preparing training data. For RAG, we
also make use of multiple annotation examples for Natural Questions and WebQuestions by training
the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA,
there are often many valid answers to a given question, some of which are not suitable training targets,
such as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur
in top 1000 documents for the query.
CuratedTrec preprocessing
The answers for CuratedTrec are given in the form of regular expres-
sions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].
To overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for
each query, and use the answer that most frequently matches the regex pattern as the supervision
target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for
each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.
TriviaQA Evaluation setups
The open-domain QA community customarily uses public develop-
ment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading
compehension purposes. We report our results using the datasets splits used in DPR [26], which are
consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public
TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set
instead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See
appendix of [14]). We report results on both test sets to enable fair comparison to both approaches.
We ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more
conventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being
simpler to answer from Wikipedia.
E
Further Details on FEVER
For FEVER classiﬁcation, we follow the practice from [32], and ﬁrst re-generate the claim, and
then classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across
documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The
ﬁrst is to classify the claim as either "Supported", "Refuted" or "Not Enough Info", which is the task
we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia
as evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to
us, directly tackling this task is not straightforward. We hope to address this in future work.
F
Null Document Probabilities
We experimented with adding "Null document" mechanism to RAG, similar to REALM [20] in order
to model cases where no useful information could be retrieved for a given input. Here, if k documents
were retrieved, we would additionally "retrieve" an empty document and predict a logit for the null
document, before marginalizing over k + 1 predictions. We explored modelling this null document
logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or
(iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in
the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents
cannot always be retrieved, we observe that the model learns to always retrieve a particular set of
documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document
mechanisms may not be necessary for RAG.
G
Parameters
Our RAG models contain the trainable parameters for the BERT-base query and document encoder of
DPR, with 110M parameters each (although we do not train the document encoder ourselves) and
406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable
18

108.0 | 72.78716278076172 | 319.2005615234375 | 84.74236297607422 | D
Further Details on Open-Domain QA
 |  | 
108.0 | 97.41175079345703 | 505.2416076660156 | 183.80606079101562 | For open-domain QA, multiple answer annotations are often available for a given question. These
answer annotations are exploited by extractive models during training as typically all the answer
annotations are used to ﬁnd matches within documents when preparing training data. For RAG, we
also make use of multiple annotation examples for Natural Questions and WebQuestions by training
the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA,
there are often many valid answers to a given question, some of which are not suitable training targets,
such as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur
in top 1000 documents for the query.
 | 1 | 
107.69100189208984 | 196.82452392578125 | 505.7466125488281 | 261.4110107421875 | CuratedTrec preprocessing
The answers for CuratedTrec are given in the form of regular expres-
sions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].
To overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for
each query, and use the answer that most frequently matches the regex pattern as the supervision
target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for
each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.
 | 2 | 
107.53199768066406 | 274.4425048828125 | 505.7420959472656 | 382.6780700683594 | TriviaQA Evaluation setups
The open-domain QA community customarily uses public develop-
ment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading
compehension purposes. We report our results using the datasets splits used in DPR [26], which are
consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public
TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set
instead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See
appendix of [14]). We report results on both test sets to enable fair comparison to both approaches.
We ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more
conventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being
simpler to answer from Wikipedia.
 | 3 | 
108.0 | 399.4471435546875 | 265.7369079589844 | 411.40234375 | E
Further Details on FEVER
 | 4 | 
107.64099884033203 | 424.0653076171875 | 504.24505615234375 | 499.55706787109375 | For FEVER classiﬁcation, we follow the practice from [32], and ﬁrst re-generate the claim, and
then classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across
documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The
ﬁrst is to classify the claim as either "Supported", "Refuted" or "Not Enough Info", which is the task
we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia
as evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to
us, directly tackling this task is not straightforward. We hope to address this in future work.
 | 5 | 
108.0 | 516.3271484375 | 272.5035705566406 | 528.2823486328125 | F
Null Document Probabilities
 | 6 | 
107.53199768066406 | 541.0194702148438 | 504.17022705078125 | 649.1640625 | We experimented with adding "Null document" mechanism to RAG, similar to REALM [20] in order
to model cases where no useful information could be retrieved for a given input. Here, if k documents
were retrieved, we would additionally "retrieve" an empty document and predict a logit for the null
document, before marginalizing over k + 1 predictions. We explored modelling this null document
logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or
(iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in
the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents
cannot always be retrieved, we observe that the model learns to always retrieve a particular set of
documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document
mechanisms may not be necessary for RAG.
 | 7 | 
108.0 | 665.9331665039062 | 188.21937561035156 | 677.8883666992188 | G
Parameters
 | 8 | 
107.6510009765625 | 690.702392578125 | 504.00274658203125 | 722.425048828125 | Our RAG models contain the trainable parameters for the BERT-base query and document encoder of
DPR, with 110M parameters each (although we do not train the document encoder ourselves) and
406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable
 | 9 | 
301.0190124511719 | 742.3324584960938 | 310.9815979003906 | 752.2950439453125 | 18
 | 10 | 
Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation
Task
Train
Development
Test
Natural Questions
79169
8758
3611
TriviaQA
78786
8838
11314
WebQuestions
3418
362
2033
CuratedTrec
635
134
635
Jeopardy Question Generation
97392
13714
26849
MS-MARCO
153726
12468
101093*
FEVER-3-way
145450
10000
10000
FEVER-2-way
96966
6666
6666
parameters. The best performing "closed-book" (parametric only) open-domain QA model is T5-11B
with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our
models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],
substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-
parametric models require far fewer trainable parameters for strong open-domain QA performance.
The non-parametric memory index does not consist of trainable parameters, but does consists of 21M
728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating
point precision to manage memory and disk footprints.
H
Retrieval Collapse
In preliminary experiments, we observed that for some tasks such as story generation [11], the
retrieval component would “collapse” and learn to retrieve the same documents regardless of the
input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,
and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit
requirement for factual knowledge in some tasks, or the longer target sequences, which could result
in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results
when optimizing a retrieval component in order to improve performance on downstream tasks.
I
Number of instances per dataset
The number of training, development and test datapoints in each of our datasets is shown in Table 7.
19

107.69100189208984 | 79.1303939819336 | 503.99505615234375 | 88.99286651611328 | Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation
 |  | 
179.2969970703125 | 98.74899291992188 | 430.4637756347656 | 107.71539306640625 | Task
Train
Development
Test
 | 1 | 
179.2969970703125 | 113.91696166992188 | 430.46405029296875 | 192.62237548828125 | Natural Questions
79169
8758
3611
TriviaQA
78786
8838
11314
WebQuestions
3418
362
2033
CuratedTrec
635
134
635
Jeopardy Question Generation
97392
13714
26849
MS-MARCO
153726
12468
101093*
FEVER-3-way
145450
10000
10000
FEVER-2-way
96966
6666
6666
 | 2 | 
107.64099884033203 | 218.44337463378906 | 505.7419738769531 | 304.6930847167969 | parameters. The best performing "closed-book" (parametric only) open-domain QA model is T5-11B
with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our
models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],
substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-
parametric models require far fewer trainable parameters for strong open-domain QA performance.
The non-parametric memory index does not consist of trainable parameters, but does consists of 21M
728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating
point precision to manage memory and disk footprints.
 | 3 | 
108.0 | 321.5041809082031 | 222.9253387451172 | 333.4593811035156 | H
Retrieval Collapse
 | 4 | 
107.64099884033203 | 346.16229248046875 | 505.2414855957031 | 421.6550598144531 | In preliminary experiments, we observed that for some tasks such as story generation [11], the
retrieval component would “collapse” and learn to retrieve the same documents regardless of the
input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,
and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit
requirement for factual knowledge in some tasks, or the longer target sequences, which could result
in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results
when optimizing a retrieval component in order to improve performance on downstream tasks.
 | 5 | 
108.0 | 438.46514892578125 | 289.30059814453125 | 450.42034912109375 | I
Number of instances per dataset
 | 6 | 
107.69100189208984 | 463.2591552734375 | 500.8976135253906 | 473.1417236328125 | The number of training, development and test datapoints in each of our datasets is shown in Table 7.
 | 7 | 
301.0190124511719 | 742.3324584960938 | 310.9815979003906 | 752.2950439453125 | 19
 | 8 | 

View full instructions

View tool guide

Note: Some questions are
control questions. We require
good accuracy on our control
questions to accept
responses.

Indicate which one of the
following sentences is more
factually true with respect to
the subject. Using the
internet to check whether
the sentences are true is
encouraged.

Which sentence is more factually true?

Subject : Hemingway

Sentence A : "The Sun Also Rises" is a novel by this author of "A
Farewell to Arms"

Sentence B : This author of "The Sun Also Rises" was born in
Havana, Cuba, the son of Spanish immigrants

Select an option

Sentence A is more
true

Sentence B is more
true

Both sentences are
true

Both sentences are
completely untrue




### Extracted from Scaling Laws for Neural Language Models.pdf ###

Scaling Laws for Neural Language Models
Jared Kaplan ∗
Johns Hopkins University, OpenAI
jaredk@jhu.edu
Sam McCandlish∗
OpenAI
sam@openai.com
Tom Henighan
OpenAI
henighan@openai.com
Tom B. Brown
OpenAI
tom@openai.com
Benjamin Chess
OpenAI
bchess@openai.com
Rewon Child
OpenAI
rewon@openai.com
Scott Gray
OpenAI
scott@openai.com
Alec Radford
OpenAI
alec@openai.com
Jeffrey Wu
OpenAI
jeffwu@openai.com
Dario Amodei
OpenAI
damodei@openai.com
Abstract
We study empirical scaling laws for language model performance on the cross-entropy loss.
The loss scales as a power-law with model size, dataset size, and the amount of compute
used for training, with some trends spanning more than seven orders of magnitude. Other
architectural details such as network width or depth have minimal effects within a wide
range. Simple equations govern the dependence of overﬁtting on model/dataset size and the
dependence of training speed on model size. These relationships allow us to determine the
optimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sample-
efﬁcient, such that optimally compute-efﬁcient training involves training very large models
on a relatively modest amount of data and stopping signiﬁcantly before convergence.
∗Equal contribution.
Contributions:
Jared Kaplan and Sam McCandlish led the research.
Tom Henighan contributed the LSTM ex-
periments.
Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer
implementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided
guidance throughout the project.
arXiv:2001.08361v1  [cs.LG]  23 Jan 2020

149.5989990234375 | 99.8338394165039 | 462.4029541015625 | 117.04924011230469 | Scaling Laws for Neural Language Models
 |  | 
200.25100708007812 | 159.65139770507812 | 264.8936767578125 | 171.1231231689453 | Jared Kaplan ∗
 | 1 | 
159.52398681640625 | 177.61549377441406 | 299.0501403808594 | 187.57809448242188 | Johns Hopkins University, OpenAI
 | 2 | 
192.6829833984375 | 194.2275390625 | 265.9080810546875 | 204.1901397705078 | jaredk@jhu.edu
 | 3 | 
374.552001953125 | 159.65139770507812 | 451.9756774902344 | 171.1231231689453 | Sam McCandlish∗
 | 4 | 
397.468994140625 | 177.61549377441406 | 429.5585021972656 | 187.57809448242188 | OpenAI
 | 5 | 
376.9100036621094 | 194.2275390625 | 450.1352844238281 | 204.1901397705078 | sam@openai.com
 | 6 | 
121.95401000976562 | 223.70355224609375 | 184.97738647460938 | 233.66615295410156 | Tom Henighan
 | 7 | 
137.42100524902344 | 240.1585235595703 | 169.51052856445312 | 250.12112426757812 | OpenAI
 | 8 | 
103.78900146484375 | 256.76953125 | 203.16587829589844 | 266.73211669921875 | henighan@openai.com
 | 9 | 
228.69900512695312 | 223.70355224609375 | 290.32763671875 | 233.66615295410156 | Tom B. Brown
 | 10 | 
243.46800231933594 | 240.1585235595703 | 275.5574951171875 | 250.12112426757812 | OpenAI
 | 11 | 
222.90899658203125 | 256.76953125 | 296.1341857910156 | 266.73211669921875 | tom@openai.com
 | 12 | 
325.875 | 223.70355224609375 | 394.7863464355469 | 233.66615295410156 | Benjamin Chess
 | 13 | 
344.2860107421875 | 240.1585235595703 | 376.3755187988281 | 250.12112426757812 | OpenAI
 | 14 | 
315.88299560546875 | 256.76953125 | 404.7994079589844 | 266.73211669921875 | bchess@openai.com
 | 15 | 
438.61199951171875 | 223.70355224609375 | 494.1435241699219 | 233.66615295410156 | Rewon Child
 | 16 | 
450.3330078125 | 240.1585235595703 | 482.4225158691406 | 250.12112426757812 | OpenAI
 | 17 | 
424.5450134277344 | 256.76953125 | 508.2310485839844 | 266.73211669921875 | rewon@openai.com
 | 18 | 
122.51901245117188 | 286.2465515136719 | 168.7255401611328 | 296.2091369628906 | Scott Gray
 | 19 | 
129.57701110839844 | 302.7015075683594 | 161.66653442382812 | 312.66412353515625 | OpenAI
 | 20 | 
103.78900909423828 | 319.31256103515625 | 187.4748077392578 | 329.275146484375 | scott@openai.com
 | 21 | 
217.927001953125 | 286.24658203125 | 274.95294189453125 | 296.20916748046875 | Alec Radford
 | 22 | 
230.39500427246094 | 302.7015380859375 | 262.4845275878906 | 312.6641540527344 | OpenAI
 | 23 | 
207.2220001220703 | 319.3125915527344 | 285.6775207519531 | 329.2751770019531 | alec@openai.com
 | 24 | 
326.35601806640625 | 286.2466125488281 | 373.3894348144531 | 296.2091979980469 | Jeffrey Wu
 | 25 | 
333.8280029296875 | 302.7015686035156 | 365.9175109863281 | 312.6641845703125 | OpenAI
 | 26 | 
305.42498779296875 | 319.3126220703125 | 394.3414001464844 | 329.27520751953125 | jeffwu@openai.com
 | 27 | 
431.1269836425781 | 286.24664306640625 | 491.17156982421875 | 296.209228515625 | Dario Amodei
 | 28 | 
445.1039733886719 | 302.70159912109375 | 477.1934814453125 | 312.6642150878906 | OpenAI
 | 29 | 
414.08697509765625 | 319.3126525878906 | 508.2337646484375 | 329.2752380371094 | damodei@openai.com
 | 30 | 
283.75799560546875 | 360.2633056640625 | 328.2432861328125 | 372.218505859375 | Abstract
 | 31 | 
125.864990234375 | 385.4945983886719 | 486.1325378417969 | 482.7301940917969 | We study empirical scaling laws for language model performance on the cross-entropy loss.
The loss scales as a power-law with model size, dataset size, and the amount of compute
used for training, with some trends spanning more than seven orders of magnitude. Other
architectural details such as network width or depth have minimal effects within a wide
range. Simple equations govern the dependence of overﬁtting on model/dataset size and the
dependence of training speed on model size. These relationships allow us to determine the
optimal allocation of a ﬁxed compute budget. Larger models are signiﬁcantly more sample-
efﬁcient, such that optimally compute-efﬁcient training involves training very large models
on a relatively modest amount of data and stopping signiﬁcantly before convergence.
 | 32 | 
101.8219985961914 | 641.703857421875 | 175.19821166992188 | 652.4284057617188 | ∗Equal contribution.
 | 33 | 
90.0 | 663.386962890625 | 522.0010986328125 | 702.2413940429688 | Contributions:
Jared Kaplan and Sam McCandlish led the research.
Tom Henighan contributed the LSTM ex-
periments.
Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer
implementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided
guidance throughout the project.
 | 34 | 
10.940000534057617 | 215.5999755859375 | 37.619998931884766 | 560.0 | arXiv:2001.08361v1  [cs.LG]  23 Jan 2020
 | 35 | 
Contents
1
Introduction
2
2
Background and Methods
6
3
Empirical Results and Basic Power Laws
7
4
Charting the Inﬁnite Data Limit and Overﬁtting
10
5
Scaling Laws with Model Size and Training Time
12
6
Optimal Allocation of the Compute Budget
14
7
Related Work
18
8
Discussion
18
Appendices
20
A Summary of Power Laws
20
B
Empirical Model of Compute-Efﬁcient Frontier
20
C Caveats
22
D Supplemental Figures
23
1
Introduction
Language provides a natural domain for the study of artiﬁcial intelligence, as the vast majority of reason-
ing tasks can be efﬁciently expressed and evaluated in language, and the world’s text provides a wealth of
data for unsupervised learning via generative modeling. Deep learning has recently seen rapid progress in lan-
guage modeling, with state of the art models [RNSS18, DCLT18, YDY+19, LOG+19, RSR+19] approaching
human-level performance on many speciﬁc tasks [WPN+19], including the composition of coherent multi-
paragraph prompted text samples [RWC+19].
One might expect language modeling performance to depend on model architecture, the size of neural models,
the computing power used to train them, and the data available for this training process. In this work we will
empirically investigate the dependence of language modeling loss on all of these factors, focusing on the
Transformer architecture [VSP+17, LSP+18]. The high ceiling and low ﬂoor for performance on language
tasks allows us to study trends over more than seven orders of magnitude in scale.
Throughout we will observe precise power-law scalings for performance as a function of training time, con-
text length, dataset size, model size, and compute budget.
1.1
Summary
Our key ﬁndings for Transformer language models are are as follows:
2Here we display predicted compute when using a sufﬁciently small batch size. See Figure 13 for comparison to the
purely empirical data.
2

90.0 | 72.78716278076172 | 135.82427978515625 | 84.74236297607422 | Contents
 |  | 
90.0 | 101.87651824951172 | 522.0003662109375 | 111.83911895751953 | 1
Introduction
2
 | 1 | 
90.00003051757812 | 129.4365234375 | 521.998291015625 | 139.3991241455078 | 2
Background and Methods
6
 | 2 | 
90.00003051757812 | 156.99749755859375 | 521.998291015625 | 166.96009826660156 | 3
Empirical Results and Basic Power Laws
7
 | 3 | 
90.00003051757812 | 184.5574951171875 | 521.998291015625 | 194.5200958251953 | 4
Charting the Inﬁnite Data Limit and Overﬁtting
10
 | 4 | 
90.00003051757812 | 212.11749267578125 | 521.998291015625 | 222.08009338378906 | 5
Scaling Laws with Model Size and Training Time
12
 | 5 | 
90.00003051757812 | 239.677490234375 | 521.998291015625 | 249.6400909423828 | 6
Optimal Allocation of the Compute Budget
14
 | 6 | 
90.00003051757812 | 267.23846435546875 | 521.9996337890625 | 277.2010498046875 | 7
Related Work
18
 | 7 | 
90.0 | 294.7984619140625 | 521.9996337890625 | 304.76104736328125 | 8
Discussion
18
 | 8 | 
90.0 | 322.35845947265625 | 521.9996337890625 | 332.321044921875 | Appendices
20
 | 9 | 
90.0 | 349.9194641113281 | 521.998291015625 | 359.8820495605469 | A Summary of Power Laws
20
 | 10 | 
90.0 | 377.4794616699219 | 521.9983520507812 | 387.4420471191406 | B
Empirical Model of Compute-Efﬁcient Frontier
20
 | 11 | 
90.0 | 405.0394592285156 | 521.9996337890625 | 415.0020446777344 | C Caveats
22
 | 12 | 
90.0 | 432.5994567871094 | 521.998291015625 | 442.5620422363281 | D Supplemental Figures
23
 | 13 | 
90.0 | 462.1841125488281 | 172.8136749267578 | 474.1393127441406 | 1
Introduction
 | 14 | 
90.0 | 488.5504150390625 | 522.001708984375 | 553.0579833984375 | Language provides a natural domain for the study of artiﬁcial intelligence, as the vast majority of reason-
ing tasks can be efﬁciently expressed and evaluated in language, and the world’s text provides a wealth of
data for unsupervised learning via generative modeling. Deep learning has recently seen rapid progress in lan-
guage modeling, with state of the art models [RNSS18, DCLT18, YDY+19, LOG+19, RSR+19] approaching
human-level performance on many speciﬁc tasks [WPN+19], including the composition of coherent multi-
paragraph prompted text samples [RWC+19].
 | 15 | 
90.00004577636719 | 559.484375 | 521.9982299804688 | 613.0829467773438 | One might expect language modeling performance to depend on model architecture, the size of neural models,
the computing power used to train them, and the data available for this training process. In this work we will
empirically investigate the dependence of language modeling loss on all of these factors, focusing on the
Transformer architecture [VSP+17, LSP+18]. The high ceiling and low ﬂoor for performance on language
tasks allows us to study trends over more than seven orders of magnitude in scale.
 | 16 | 
90.00007629394531 | 619.5093994140625 | 521.9981689453125 | 640.3809814453125 | Throughout we will observe precise power-law scalings for performance as a function of training time, con-
text length, dataset size, model size, and compute budget.
 | 17 | 
90.00007629394531 | 657.5064697265625 | 154.47801208496094 | 667.4690551757812 | 1.1
Summary
 | 18 | 
90.00007629394531 | 678.660400390625 | 367.4884033203125 | 688.6229858398438 | Our key ﬁndings for Transformer language models are are as follows:
 | 19 | 
90.0 | 701.6947021484375 | 522.0004272460938 | 722.1663818359375 | 2Here we display predicted compute when using a sufﬁciently small batch size. See Figure 13 for comparison to the
purely empirical data.
 | 20 | 
303.5090026855469 | 742.3324584960938 | 308.49029541015625 | 752.2950439453125 | 2
 | 21 | 
Dataset Size 
tokens
Parameters 
non-embedding
Compute 
PF-days, non-embedding
Test Loss
Figure 1
Language modeling performance improves smoothly as we increase the model size, datasetset
size, and amount of compute2 used for training. For optimal performance all three factors must be scaled
up in tandem. Empirical performance has a power-law relationship with each individual factor when not
bottlenecked by the other two.
Performance depends strongly on scale, weakly on model shape:
Model performance depends most
strongly on scale, which consists of three factors: the number of model parameters N (excluding embed-
dings), the size of the dataset D, and the amount of compute C used for training. Within reasonable limits,
performance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section
3)
Smooth power laws:
Performance has a power-law relationship with each of the three scale factors
N, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude
(see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance
must ﬂatten out eventually before reaching zero loss. (Section 3)
Universality of overﬁtting:
Performance improves predictably as long as we scale up N and D in tandem,
but enters a regime of diminishing returns if either N or D is held ﬁxed while the other increases. The
performance penalty depends predictably on the ratio N 0.74/D, meaning that every time we increase the
model size 8x, we only need to increase the data by roughly 5x to avoid a penalty. (Section 4)
Universality of training:
Training curves follow predictable power-laws whose parameters are roughly
independent of the model size. By extrapolating the early part of a training curve, we can roughly predict the
loss that would be achieved if we trained for much longer. (Section 5)
Transfer improves with test performance:
When we evaluate models on text with a different distribution
than they were trained on, the results are strongly correlated to those on the training validation set with
a roughly constant offset in the loss – in other words, transfer to a different distribution incurs a constant
penalty but otherwise improves roughly in line with performance on the training set. (Section 3.2.2)
Sample efﬁciency:
Large models are more sample-efﬁcient than small models, reaching the same level of
performance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4).
Convergence is inefﬁcient:
When working within a ﬁxed compute budget C but without any other restric-
tions on the model size N or available data D, we attain optimal performance by training very large models
and stopping signiﬁcantly short of convergence (see Figure 3). Maximally compute-efﬁcient training would
therefore be far more sample efﬁcient than one might expect based on training small models to convergence,
with data requirements growing very slowly as D ∼C0.27 with training compute. (Section 6)
Optimal batch size:
The ideal batch size for training these models is roughly a power of the loss only,
and continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million
tokens at convergence for the largest models we can train. (Section 5.1)
Taken together, these results show that language modeling performance improves smoothly and predictably
as we appropriately scale up model size, data, and compute. We expect that larger language models will
perform better and be more sample efﬁcient than current models.
3

292.803955078125 | 180.36312866210938 | 351.09588623046875 | 191.79893493652344 | Dataset Size 
 |  | 
308.63372802734375 | 191.9049530029297 | 332.1927490234375 | 201.52752685546875 | tokens
 | 1 | 
134.0265655517578 | 180.36312866210938 | 492.7867126464844 | 201.52752685546875 | Parameters 
non-embedding
Compute 
PF-days, non-embedding
 | 2 | 
91.0703353881836 | 101.59132385253906 | 101.36256408691406 | 139.34963989257812 | Test Loss
 | 3 | 
90.0 | 210.1915283203125 | 522.0015258789062 | 252.97207641601562 | Figure 1
Language modeling performance improves smoothly as we increase the model size, datasetset
size, and amount of compute2 used for training. For optimal performance all three factors must be scaled
up in tandem. Empirical performance has a power-law relationship with each individual factor when not
bottlenecked by the other two.
 | 4 | 
90.0 | 274.63250732421875 | 522.0048217773438 | 328.3220520019531 | Performance depends strongly on scale, weakly on model shape:
Model performance depends most
strongly on scale, which consists of three factors: the number of model parameters N (excluding embed-
dings), the size of the dataset D, and the amount of compute C used for training. Within reasonable limits,
performance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section
3)
 | 5 | 
89.99999237060547 | 341.0564880371094 | 522.0015869140625 | 383.8370361328125 | Smooth power laws:
Performance has a power-law relationship with each of the three scale factors
N, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude
(see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance
must ﬂatten out eventually before reaching zero loss. (Section 3)
 | 6 | 
89.99990844726562 | 396.431884765625 | 522.0033569335938 | 439.3520202636719 | Universality of overﬁtting:
Performance improves predictably as long as we scale up N and D in tandem,
but enters a regime of diminishing returns if either N or D is held ﬁxed while the other increases. The
performance penalty depends predictably on the ratio N 0.74/D, meaning that every time we increase the
model size 8x, we only need to increase the data by roughly 5x to avoid a penalty. (Section 4)
 | 7 | 
89.99990844726562 | 452.0864562988281 | 522.0015869140625 | 483.9580078125 | Universality of training:
Training curves follow predictable power-laws whose parameters are roughly
independent of the model size. By extrapolating the early part of a training curve, we can roughly predict the
loss that would be achieved if we trained for much longer. (Section 5)
 | 8 | 
89.99990844726562 | 496.69244384765625 | 521.998779296875 | 539.4729614257812 | Transfer improves with test performance:
When we evaluate models on text with a different distribution
than they were trained on, the results are strongly correlated to those on the training validation set with
a roughly constant offset in the loss – in other words, transfer to a different distribution incurs a constant
penalty but otherwise improves roughly in line with performance on the training set. (Section 3.2.2)
 | 9 | 
89.99990844726562 | 552.2073974609375 | 522.0020751953125 | 573.1699829101562 | Sample efﬁciency:
Large models are more sample-efﬁcient than small models, reaching the same level of
performance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4).
 | 10 | 
89.9998779296875 | 585.7648315429688 | 522.0 | 639.5939331054688 | Convergence is inefﬁcient:
When working within a ﬁxed compute budget C but without any other restric-
tions on the model size N or available data D, we attain optimal performance by training very large models
and stopping signiﬁcantly short of convergence (see Figure 3). Maximally compute-efﬁcient training would
therefore be far more sample efﬁcient than one might expect based on training small models to convergence,
with data requirements growing very slowly as D ∼C0.27 with training compute. (Section 6)
 | 11 | 
89.99986267089844 | 652.328369140625 | 522.0028076171875 | 684.1998901367188 | Optimal batch size:
The ideal batch size for training these models is roughly a power of the loss only,
and continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million
tokens at convergence for the largest models we can train. (Section 5.1)
 | 12 | 
89.99986267089844 | 690.6263427734375 | 521.9981079101562 | 722.4069213867188 | Taken together, these results show that language modeling performance improves smoothly and predictably
as we appropriately scale up model size, data, and compute. We expect that larger language models will
perform better and be more sample efﬁcient than current models.
 | 13 | 
303.50885009765625 | 742.3323364257812 | 308.4901428222656 | 752.294921875 | 3
 | 14 | 
Larger models require fewer samples 
to reach the same performance
10
8
6
4
The optimal model size grows smoothly 
with the loss target and compute budget
Line color indicates 
number of parameters
107
109
1011
Tokens Processed
Compute (PF-days)
10-9
10-6
10-3
100
Test Loss
Compute-eﬃcient 
training stops far 
short of convergence
103
109
106
103 Params
109 Params
10
8
6
4
Figure 2
We show a series of language model training runs, with models ranging in size from 103 to 109
parameters (excluding embeddings).
100x Batch Size
<10x Serial Steps
>1,000,000x Model Size
Data requirements 
grow relatively slowly
Optimal model size 
increases very quickly
Minimum serial steps 
increases negligibly
Figure 3
As more compute becomes available, we can choose how much to allocate towards training larger
models, using larger batches, and training for more steps. We illustrate this for a billion-fold increase in
compute. For optimally compute-efﬁcient training, most of the increase should go towards increased model
size. A relatively small increase in data is needed to avoid reuse. Of the increase in data, most can be used to
increase parallelism through larger batch sizes, with only a very small increase in serial training time required.
1.2
Summary of Scaling Laws
The test loss of a Transformer trained to autoregressively model language can be predicted using a power-law
when performance is limited by only either the number of non-embedding parameters N, the dataset size D,
or the optimally allocated compute budget Cmin (see Figure 1):
1. For models with a limited number of parameters, trained to convergence on sufﬁciently large
datasets:
L(N) = (Nc/N)αN ; αN ∼0.076,
Nc ∼8.8 × 1013 (non-embedding parameters)
(1.1)
2. For large models trained with a limited dataset with early stopping:
L(D) = (Dc/D)αD ; αD ∼0.095,
Dc ∼5.4 × 1013 (tokens)
(1.2)
3. When training with a limited amount of compute, a sufﬁciently large dataset, an optimally-sized
model, and a sufﬁciently small batch size (making optimal3 use of compute):
L(Cmin) =
 Cmin
c
/Cmin
αmin
C
; αmin
C
∼0.050,
Cmin
c
∼3.1 × 108 (PF-days)
(1.3)
3We also observe an empirical power-law trend with the training compute C (Figure 1) while training at ﬁxed batch
size, but it is the trend with Cmin that should be used to make predictions. They are related by equation (5.5).
4

138.29376220703125 | 69.64261627197266 | 283.9486999511719 | 93.51673889160156 | Larger models require fewer samples 
to reach the same performance
 |  | 
127.58155822753906 | 112.19542694091797 | 133.8510284423828 | 118.76370239257812 | 10
 | 1 | 
130.96435546875 | 139.25784301757812 | 134.09909057617188 | 145.82611083984375 | 8
 | 2 | 
130.96435546875 | 165.75643920898438 | 134.09909057617188 | 172.32470703125 | 6
 | 3 | 
130.96435546875 | 192.25503540039062 | 134.09909057617188 | 198.82330322265625 | 4
 | 4 | 
301.7957458496094 | 69.693359375 | 452.727783203125 | 93.51673889160156 | The optimal model size grows smoothly 
with the loss target and compute budget
 | 5 | 
457.404541015625 | 104.32928466796875 | 518.982177734375 | 118.31997680664062 | Line color indicates 
number of parameters
 | 6 | 
168.73895263671875 | 224.95542907714844 | 271.0874328613281 | 231.52369689941406 | 107
109
1011
 | 7 | 
137.16615295410156 | 233.31094360351562 | 361.199951171875 | 241.37554931640625 | Tokens Processed
Compute (PF-days)
 | 8 | 
322.6563415527344 | 224.95542907714844 | 433.0634460449219 | 231.52369689941406 | 10-9
10-6
10-3
100
 | 9 | 
94.88115692138672 | 110.96634674072266 | 124.46712493896484 | 119.03093719482422 | Test Loss
 | 10 | 
455.14935302734375 | 169.19334411621094 | 519.111328125 | 192.86167907714844 | Compute-eﬃcient 
training stops far 
short of convergence
 | 11 | 
454.5855407714844 | 129.10943603515625 | 515.3782348632812 | 135.67770385742188 | 103
109
106
 | 12 | 
238.08636474609375 | 146.07754516601562 | 272.4285583496094 | 153.95947265625 | 103 Params
 | 13 | 
156.33535766601562 | 174.8313446044922 | 190.6775360107422 | 182.71327209472656 | 109 Params
 | 14 | 
292.77496337890625 | 112.19542694091797 | 299.04443359375 | 118.76370239257812 | 10
 | 15 | 
296.1577453613281 | 138.69403076171875 | 299.29248046875 | 145.26229858398438 | 8
 | 16 | 
296.1577453613281 | 165.75643920898438 | 299.29248046875 | 172.32470703125 | 6
 | 17 | 
296.1577453613281 | 191.6912384033203 | 299.29248046875 | 198.25950622558594 | 4
 | 18 | 
90.0 | 247.2714385986328 | 521.5060424804688 | 271.7590637207031 | Figure 2
We show a series of language model training runs, with models ranging in size from 103 to 109
parameters (excluding embeddings).
 | 19 | 
284.6874084472656 | 313.4559020996094 | 350.56842041015625 | 351.6099548339844 | 100x Batch Size
 | 20 | 
278.0782470703125 | 289.9236755371094 | 347.1978759765625 | 335.60498046875 | <10x Serial Steps
 | 21 | 
259.85150146484375 | 347.1405029296875 | 358.1284484863281 | 388.29339599609375 | >1,000,000x Model Size
 | 22 | 
381.5390930175781 | 291.99102783203125 | 455.2760314941406 | 310.16546630859375 | Data requirements 
grow relatively slowly
 | 23 | 
381.5390930175781 | 346.55322265625 | 458.0509033203125 | 364.7276611328125 | Optimal model size 
increases very quickly
 | 24 | 
199.6650848388672 | 291.99102783203125 | 275.7637634277344 | 310.16546630859375 | Minimum serial steps 
increases negligibly
 | 25 | 
90.0 | 428.9925231933594 | 522.0010986328125 | 482.68206787109375 | Figure 3
As more compute becomes available, we can choose how much to allocate towards training larger
models, using larger batches, and training for more steps. We illustrate this for a billion-fold increase in
compute. For optimally compute-efﬁcient training, most of the increase should go towards increased model
size. A relatively small increase in data is needed to avoid reuse. Of the increase in data, most can be used to
increase parallelism through larger batch sizes, with only a very small increase in serial training time required.
 | 26 | 
90.0 | 515.1104736328125 | 223.94711303710938 | 525.0730590820312 | 1.2
Summary of Scaling Laws
 | 27 | 
90.0 | 535.1764526367188 | 521.9996948242188 | 568.4520263671875 | The test loss of a Transformer trained to autoregressively model language can be predicted using a power-law
when performance is limited by only either the number of non-embedding parameters N, the dataset size D,
or the optimally allocated compute budget Cmin (see Figure 1):
 | 28 | 
113.41204833984375 | 576.574462890625 | 521.998046875 | 597.446044921875 | 1. For models with a limited number of parameters, trained to convergence on sufﬁciently large
datasets:
 | 29 | 
142.31204223632812 | 599.1439208984375 | 522.0025024414062 | 614.4783935546875 | L(N) = (Nc/N)αN ; αN ∼0.076,
Nc ∼8.8 × 1013 (non-embedding parameters)
(1.1)
 | 30 | 
113.41201782226562 | 621.0734252929688 | 393.4008483886719 | 631.0360107421875 | 2. For large models trained with a limited dataset with early stopping:
 | 31 | 
193.57601928710938 | 632.73291015625 | 521.99560546875 | 648.0684204101562 | L(D) = (Dc/D)αD ; αD ∼0.095,
Dc ∼5.4 × 1013 (tokens)
(1.2)
 | 32 | 
113.4119873046875 | 654.6624145507812 | 521.9981079101562 | 675.5350341796875 | 3. When training with a limited amount of compute, a sufﬁciently large dataset, an optimally-sized
model, and a sufﬁciently small batch size (making optimal3 use of compute):
 | 33 | 
154.40798950195312 | 680.7619018554688 | 522.0006713867188 | 697.989501953125 | L(Cmin) =
 
Cmin
c
/Cmin
αmin
C
; αmin
C
∼0.050,
Cmin
c
∼3.1 × 108 (PF-days)
(1.3)
 | 34 | 
90.0 | 701.6947021484375 | 521.999755859375 | 723.162353515625 | 3We also observe an empirical power-law trend with the training compute C (Figure 1) while training at ﬁxed batch
size, but it is the trend with Cmin that should be used to make predictions. They are related by equation (5.5).
 | 35 | 
303.5090026855469 | 742.3324584960938 | 308.49029541015625 | 752.2950439453125 | 4
 | 36 | 
107
108
109
1010
Tokens in Dataset
2.5
3.0
3.5
4.0
4.5
Loss
Loss vs Model and Dataset Size
Params
708M
302M
85M
3M
25M
393.2K
104
105
Estimated Smin
2.4
2.8
3.2
3.6
4.0
4.4
Loss
Loss vs Model Size and Training Steps
106
107
108
Parameters (non-embed)
Figure 4
Left: The early-stopped test loss L(N, D) varies predictably with the dataset size D and model
size N according to Equation (1.5). Right: After an initial transient period, learning curves for all model
sizes N can be ﬁt with Equation (1.6), which is parameterized in terms of Smin, the number of steps when
training at large batch size (details in Section 5.1).
These relations hold across eight orders of magnitude in Cmin, six orders of magnitude in N, and over two
orders of magnitude in D. They depend very weakly on model shape and other Transformer hyperparameters
(depth, width, number of self-attention heads), with speciﬁc numerical values associated with the Webtext2
training set [RWC+19]. The power laws αN, αD, αmin
C
specify the degree of performance improvement
expected as we scale up N, D, or Cmin; for example, doubling the number of parameters yields a loss that
is smaller by a factor 2−αN = 0.95. The precise numerical values of Nc, Cmin
c
, and Dc depend on the
vocabulary size and tokenization and hence do not have a fundamental meaning.
The critical batch size, which determines the speed/efﬁciency tradeoff for data parallelism ([MKAT18]), also
roughly obeys a power law in L:
Bcrit (L) =
B∗
L1/αB ,
B∗∼2 · 108 tokens, αB ∼0.21
(1.4)
Equation (1.1) and (1.2) together suggest that as we increase the model size, we should increase the dataset
size sublinearly according to D ∝N
αN
αD ∼N 0.74. In fact, we ﬁnd that there is a single equation combining
(1.1) and (1.2) that governs the simultaneous dependence on N and D and governs the degree of overﬁtting:
L(N, D) =
"Nc
N
 αN
αD + Dc
D
#αD
(1.5)
with ﬁts pictured on the left in ﬁgure 4. We conjecture that this functional form may also parameterize the
trained log-likelihood for other generative modeling tasks.
When training a given model for a ﬁnite number of parameter update steps S in the inﬁnite data limit, after
an initial transient period, the learning curves can be accurately ﬁt by (see the right of ﬁgure 4)
L(N, S) =
Nc
N
αN
+

Sc
Smin(S)
αS
(1.6)
where Sc ≈2.1 × 103 and αS ≈0.76, and Smin(S) is the minimum possible number of optimization steps
(parameter updates) estimated using Equation (5.4).
When training within a ﬁxed compute budget C, but with no other constraints, Equation (1.6) leads to the
prediction that the optimal model size N, optimal batch size B, optimal number of steps S, and dataset size
D should grow as
N ∝Cαmin
C
/αN ,
B ∝Cαmin
C
/αB,
S ∝Cαmin
C
/αS,
D = B · S
(1.7)
with
αmin
C
= 1/ (1/αS + 1/αB + 1/αN)
(1.8)
which closely matches the empirically optimal results N ∝C0.73
min , B ∝C0.24
min , and S ∝C0.03
min . As the
computational budget C increases, it should be spent primarily on larger models, without dramatic increases
in training time or dataset size (see Figure 3). This also implies that as models grow larger, they become
increasingly sample efﬁcient. In practice, researchers typically train smaller models for longer than would
5

122.4058837890625 | 167.73049926757812 | 251.08119201660156 | 176.6460723876953 | 107
108
109
1010
 |  | 
168.35366821289062 | 175.7054901123047 | 222.72317504882812 | 184.3848419189453 | Tokens in Dataset
 | 1 | 
109.17179107666016 | 155.37158203125 | 118.64342498779297 | 164.05093383789062 | 2.5
 | 2 | 
109.17179107666016 | 134.7308349609375 | 118.64342498779297 | 143.41018676757812 | 3.0
 | 3 | 
109.17179107666016 | 117.27934265136719 | 118.64342498779297 | 125.95868682861328 | 3.5
 | 4 | 
109.17179107666016 | 102.16217041015625 | 118.64342498779297 | 110.84151458740234 | 4.0
 | 5 | 
109.17179107666016 | 88.82786560058594 | 118.64342498779297 | 97.50721740722656 | 4.5
 | 6 | 
99.62091064453125 | 118.53364562988281 | 108.30026245117188 | 132.1870880126953 | Loss
 | 7 | 
138.362548828125 | 73.07122802734375 | 253.0586395263672 | 83.48644256591797 | Loss vs Model and Dataset Size
 | 8 | 
276.2612609863281 | 101.66038513183594 | 298.9216613769531 | 110.33973693847656 | Params
 | 9 | 
285.87158203125 | 110.14485931396484 | 301.2278747558594 | 147.55880737304688 | 708M
302M
85M
3M
25M
393.2K
 | 10 | 
376.0389099121094 | 167.25070190429688 | 458.4567565917969 | 176.30038452148438 | 104
105
 | 11 | 
381.8867492675781 | 175.3456573486328 | 426.7275390625 | 185.0091094970703 | Estimated Smin
 | 12 | 
319.1791687011719 | 158.5919189453125 | 328.79327392578125 | 167.40182495117188 | 2.4
 | 13 | 
319.1791687011719 | 139.70201110839844 | 328.79327392578125 | 148.5119171142578 | 2.8
 | 14 | 
319.1791687011719 | 123.33882904052734 | 328.79327392578125 | 132.1487274169922 | 3.2
 | 15 | 
319.1791687011719 | 108.90547180175781 | 328.79327392578125 | 117.71536254882812 | 3.6
 | 16 | 
319.1791687011719 | 95.99440002441406 | 328.79327392578125 | 104.80429077148438 | 4.0
 | 17 | 
319.1791687011719 | 84.31491088867188 | 328.79327392578125 | 93.12480163574219 | 4.4
 | 18 | 
309.484619140625 | 118.04810333251953 | 318.2945251464844 | 131.90692138671875 | Loss
 | 19 | 
333.6691589355469 | 72.63612365722656 | 475.8924865722656 | 83.20800018310547 | Loss vs Model Size and Training Steps
 | 20 | 
494.3666687011719 | 149.8870391845703 | 504.8075866699219 | 158.88003540039062 | 106
 | 21 | 
494.3666687011719 | 124.783447265625 | 504.8075866699219 | 133.7764434814453 | 107
 | 22 | 
494.3666687011719 | 99.79322052001953 | 504.8075866699219 | 108.78621673583984 | 108
 | 23 | 
504.7757263183594 | 86.86798858642578 | 513.5856323242188 | 163.09144592285156 | Parameters (non-embed)
 | 24 | 
89.99993896484375 | 193.4419403076172 | 522.0027465820312 | 236.36306762695312 | Figure 4
Left: The early-stopped test loss L(N, D) varies predictably with the dataset size D and model
size N according to Equation (1.5). Right: After an initial transient period, learning curves for all model
sizes N can be ﬁt with Equation (1.6), which is parameterized in terms of Smin, the number of steps when
training at large batch size (details in Section 5.1).
 | 25 | 
89.9998779296875 | 258.73394775390625 | 522.0020141601562 | 334.3811340332031 | These relations hold across eight orders of magnitude in Cmin, six orders of magnitude in N, and over two
orders of magnitude in D. They depend very weakly on model shape and other Transformer hyperparameters
(depth, width, number of self-attention heads), with speciﬁc numerical values associated with the Webtext2
training set [RWC+19]. The power laws αN, αD, αmin
C
specify the degree of performance improvement
expected as we scale up N, D, or Cmin; for example, doubling the number of parameters yields a loss that
is smaller by a factor 2−αN = 0.95. The precise numerical values of Nc, Cmin
c
, and Dc depend on the
vocabulary size and tokenization and hence do not have a fundamental meaning.
 | 26 | 
89.9998779296875 | 340.8075256347656 | 521.9981079101562 | 361.67913818359375 | The critical batch size, which determines the speed/efﬁciency tradeoff for data parallelism ([MKAT18]), also
roughly obeys a power law in L:
 | 27 | 
187.6148681640625 | 367.67596435546875 | 522.0003051757812 | 391.3905029296875 | Bcrit (L) =
B∗
L1/αB ,
B∗∼2 · 108 tokens, αB ∼0.21
(1.4)
 | 28 | 
89.99993896484375 | 401.11749267578125 | 521.9981689453125 | 425.4781188964844 | Equation (1.1) and (1.2) together suggest that as we increase the model size, we should increase the dataset
size sublinearly according to D ∝N
 | 29 | 
89.99998474121094 | 410.0780029296875 | 522.0029296875 | 436.3870544433594 | αN
αD ∼N 0.74. In fact, we ﬁnd that there is a single equation combining
(1.1) and (1.2) that governs the simultaneous dependence on N and D and governs the degree of overﬁtting:
 | 30 | 
234.68101501464844 | 454.18292236328125 | 281.22198486328125 | 464.1455078125 | L(N, D) =
 | 31 | 
283.9840087890625 | 444.3103332519531 | 309.8926086425781 | 458.24755859375 | "Nc
 | 32 | 
299.8110046386719 | 461.0169372558594 | 307.8209228515625 | 470.9795227050781 | N
 | 33 | 
311.5830078125 | 442.1539611816406 | 330.02191162109375 | 457.2619323730469 |  αN
 | 34 | 
320.2130126953125 | 446.2889099121094 | 357.9396057128906 | 464.1455078125 | αD + Dc
 | 35 | 
348.01800537109375 | 461.0169372558594 | 356.26702880859375 | 470.9795227050781 | D
 | 36 | 
359.6300048828125 | 440.9147644042969 | 522.0003662109375 | 464.3760986328125 | #αD
(1.5)
 | 37 | 
90.0 | 480.84649658203125 | 521.9981689453125 | 501.7181091308594 | with ﬁts pictured on the left in ﬁgure 4. We conjecture that this functional form may also parameterize the
trained log-likelihood for other generative modeling tasks.
 | 38 | 
90.0 | 507.91400146484375 | 522.0021362304688 | 529.0160522460938 | When training a given model for a ﬁnite number of parameter update steps S in the inﬁnite data limit, after
an initial transient period, the learning curves can be accurately ﬁt by (see the right of ﬁgure 4)
 | 39 | 
224.58599853515625 | 536.306396484375 | 292.14459228515625 | 553.152587890625 | L(N, S) =
Nc
 | 40 | 
282.06201171875 | 550.02392578125 | 290.0719299316406 | 559.9865112304688 | N
 | 41 | 
293.83502197265625 | 533.542724609375 | 368.19244384765625 | 560.8275756835938 | αN
+

Sc
Smin(S)
 | 42 | 
369.3869934082031 | 533.542724609375 | 522.0003662109375 | 553.383056640625 | αS
(1.6)
 | 43 | 
90.0 | 564.6044311523438 | 522.0049438476562 | 589.0910034179688 | where Sc ≈2.1 × 103 and αS ≈0.76, and Smin(S) is the minimum possible number of optimization steps
(parameter updates) estimated using Equation (5.4).
 | 44 | 
90.00003051757812 | 595.2869262695312 | 522.0008544921875 | 627.2980346679688 | When training within a ﬁxed compute budget C, but with no other constraints, Equation (1.6) leads to the
prediction that the optimal model size N, optimal batch size B, optimal number of steps S, and dataset size
D should grow as
 | 45 | 
165.34805297851562 | 633.7659301757812 | 522.0003662109375 | 647.18603515625 | N ∝Cαmin
C
/αN ,
B ∝Cαmin
C
/αB,
S ∝Cαmin
C
/αS,
D = B · S
(1.7)
 | 46 | 
90.0 | 654.3614501953125 | 522.003173828125 | 722.4070434570312 | with
αmin
C
= 1/ (1/αS + 1/αB + 1/αN)
(1.8)
which closely matches the empirically optimal results N ∝C0.73
min , B ∝C0.24
min , and S ∝C0.03
min . As the
computational budget C increases, it should be spent primarily on larger models, without dramatic increases
in training time or dataset size (see Figure 3). This also implies that as models grow larger, they become
increasingly sample efﬁcient. In practice, researchers typically train smaller models for longer than would
 | 47 | 
303.5101318359375 | 742.3324584960938 | 308.4914245605469 | 752.2950439453125 | 5
 | 48 | 
be maximally compute-efﬁcient because of hardware constraints. Optimal performance depends on total
compute as a power law (see Equation (1.3)).
We provide some basic theoretical motivation for Equation (1.5), an analysis of learning curve ﬁts and their
implications for training time, and a breakdown of our results per token. We also make some brief compar-
isons to LSTMs and recurrent Transformers [DGV+18].
1.3
Notation
We use the following notation:
• L – the cross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in
some cases we report the loss for speciﬁc tokens within the context.
• N – the number of model parameters, excluding all vocabulary and positional embeddings
• C ≈6NBS – an estimate of the total non-embedding training compute, where B is the batch size,
and S is the number of training steps (ie parameter updates). We quote numerical values in PF-days,
where one PF-day = 1015 × 24 × 3600 = 8.64 × 1019 ﬂoating point operations.
• D – the dataset size in tokens
• Bcrit – the critical batch size [MKAT18], deﬁned and discussed in Section 5.1. Training at the
critical batch size provides a roughly optimal compromise between time and compute efﬁciency.
• Cmin – an estimate of the minimum amount of non-embedding compute to reach a given value of
the loss. This is the training compute that would be used if the model were trained at a batch size
much less than the critical batch size.
• Smin – an estimate of the minimal number of training steps needed to reach a given value of the loss.
This is also the number of training steps that would be used if the model were trained at a batch size
much greater than the critical batch size.
• αX – power-law exponents for the scaling of the loss as L(X) ∝1/XαX where X can be any of
N, D, C, S, B, Cmin.
2
Background and Methods
We train language models on WebText2, an extended version of the WebText [RWC+19] dataset, tokenized
using byte-pair encoding [SHB15] with a vocabulary size nvocab = 50257. We optimize the autoregres-
sive log-likelihood (i.e. cross-entropy loss) averaged over a 1024-token context, which is also our principal
performance metric. We record the loss on the WebText2 test distribution and on a selection of other text
distributions. We primarily train decoder-only [LSP+18, RNSS18] Transformer [VSP+17] models, though
we also train LSTM models and Universal Transformers [DGV+18] for comparison.
2.1
Parameter and Compute Scaling of Transformers
We parameterize the Transformer architecture using hyperparameters nlayer (number of layers), dmodel (di-
mension of the residual stream), dﬀ(dimension of the intermediate feed-forward layer), dattn (dimension of
the attention output), and nheads (number of attention heads per layer). We include nctx tokens in the input
context, with nctx = 1024 except where otherwise noted.
We use N to denote the model size, which we deﬁne as the number of non-embedding parameters
N ≈2dmodelnlayer (2dattn + dﬀ)
= 12nlayerd2
model
with the standard
dattn = dﬀ/4 = dmodel
(2.1)
where we have excluded biases and other sub-leading terms. Our models also have nvocabdmodel parameters
in an embedding matrix, and use nctxdmodel parameters for positional embeddings, but we do not include
these when discussing the ‘model size’ N; we will see that this produces signiﬁcantly cleaner scaling laws.
Evaluating a forward pass of the Transformer involves roughly
Cforward ≈2N + 2nlayernctxdmodel
(2.2)
add-multiply operations, where the factor of two comes from the multiply-accumulate operation used in
matrix multiplication. A more detailed per-operation parameter and compute count is included in Table 1.
6

90.0 | 74.40748596191406 | 521.998046875 | 95.27908325195312 | be maximally compute-efﬁcient because of hardware constraints. Optimal performance depends on total
compute as a power law (see Equation (1.3)).
 |  | 
90.0 | 101.70448303222656 | 521.9982299804688 | 133.48507690429688 | We provide some basic theoretical motivation for Equation (1.5), an analysis of learning curve ﬁts and their
implications for training time, and a breakdown of our results per token. We also make some brief compar-
isons to LSTMs and recurrent Transformers [DGV+18].
 | 1 | 
90.0 | 147.49151611328125 | 149.49664306640625 | 157.45411682128906 | 1.3
Notation
 | 2 | 
90.0 | 167.5574493408203 | 212.37057495117188 | 177.52005004882812 | We use the following notation:
 | 3 | 
115.90299987792969 | 186.6857452392578 | 522.0042724609375 | 396.3479919433594 | • L – the cross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in
some cases we report the loss for speciﬁc tokens within the context.
• N – the number of model parameters, excluding all vocabulary and positional embeddings
• C ≈6NBS – an estimate of the total non-embedding training compute, where B is the batch size,
and S is the number of training steps (ie parameter updates). We quote numerical values in PF-days,
where one PF-day = 1015 × 24 × 3600 = 8.64 × 1019 ﬂoating point operations.
• D – the dataset size in tokens
• Bcrit – the critical batch size [MKAT18], deﬁned and discussed in Section 5.1. Training at the
critical batch size provides a roughly optimal compromise between time and compute efﬁciency.
• Cmin – an estimate of the minimum amount of non-embedding compute to reach a given value of
the loss. This is the training compute that would be used if the model were trained at a batch size
much less than the critical batch size.
• Smin – an estimate of the minimal number of training steps needed to reach a given value of the loss.
This is also the number of training steps that would be used if the model were trained at a batch size
much greater than the critical batch size.
• αX – power-law exponents for the scaling of the loss as L(X) ∝1/XαX where X can be any of
N, D, C, S, B, Cmin.
 | 4 | 
90.0 | 412.76007080078125 | 240.5757598876953 | 424.71527099609375 | 2
Background and Methods
 | 5 | 
90.0 | 435.58465576171875 | 522.001953125 | 501.60296630859375 | We train language models on WebText2, an extended version of the WebText [RWC+19] dataset, tokenized
using byte-pair encoding [SHB15] with a vocabulary size nvocab = 50257. We optimize the autoregres-
sive log-likelihood (i.e. cross-entropy loss) averaged over a 1024-token context, which is also our principal
performance metric. We record the loss on the WebText2 test distribution and on a selection of other text
distributions. We primarily train decoder-only [LSP+18, RNSS18] Transformer [VSP+17] models, though
we also train LSTM models and Universal Transformers [DGV+18] for comparison.
 | 6 | 
90.00009155273438 | 515.6083984375 | 323.00537109375 | 525.5709838867188 | 2.1
Parameter and Compute Scaling of Transformers
 | 7 | 
90.00009155273438 | 535.4437866210938 | 522.0001220703125 | 579.62841796875 | We parameterize the Transformer architecture using hyperparameters nlayer (number of layers), dmodel (di-
mension of the residual stream), dﬀ(dimension of the intermediate feed-forward layer), dattn (dimension of
the attention output), and nheads (number of attention heads per layer). We include nctx tokens in the input
context, with nctx = 1024 except where otherwise noted.
 | 8 | 
90.0000991821289 | 584.559814453125 | 479.8300476074219 | 594.7529296875 | We use N to denote the model size, which we deﬁne as the number of non-embedding parameters
 | 9 | 
144.006103515625 | 600.0986938476562 | 278.9065856933594 | 611.6854248046875 | N ≈2dmodelnlayer (2dattn + dﬀ)
 | 10 | 
155.8641357421875 | 614.3156127929688 | 522.00048828125 | 627.866455078125 | = 12nlayerd2
model
with the standard
dattn = dﬀ/4 = dmodel
(2.1)
 | 11 | 
90.00015258789062 | 631.7617797851562 | 522.0045776367188 | 663.7728881835938 | where we have excluded biases and other sub-leading terms. Our models also have nvocabdmodel parameters
in an embedding matrix, and use nctxdmodel parameters for positional embeddings, but we do not include
these when discussing the ‘model size’ N; we will see that this produces signiﬁcantly cleaner scaling laws.
 | 12 | 
90.00016784667969 | 670.1993408203125 | 339.78240966796875 | 680.1619262695312 | Evaluating a forward pass of the Transformer involves roughly
 | 13 | 
233.1541748046875 | 685.5076293945312 | 522.0005493164062 | 696.9642944335938 | Cforward ≈2N + 2nlayernctxdmodel
(2.2)
 | 14 | 
90.00018310546875 | 701.5353393554688 | 521.998291015625 | 722.4069213867188 | add-multiply operations, where the factor of two comes from the multiply-accumulate operation used in
matrix multiplication. A more detailed per-operation parameter and compute count is included in Table 1.
 | 15 | 
303.51019287109375 | 742.3323364257812 | 308.4914855957031 | 752.294921875 | 6
 | 16 | 
Operation
Parameters
FLOPs per Token
Embed
(nvocab + nctx) dmodel
4dmodel
Attention: QKV
nlayerdmodel3dattn
2nlayerdmodel3dattn
Attention: Mask
—
2nlayernctxdattn
Attention: Project
nlayerdattndmodel
2nlayerdattndembd
Feedforward
nlayer2dmodeldﬀ
2nlayer2dmodeldﬀ
De-embed
—
2dmodelnvocab
Total (Non-Embedding)
N = 2dmodelnlayer (2dattn + dﬀ)
Cforward = 2N + 2nlayernctxdattn
Table 1
Parameter counts and compute (forward pass) estimates for a Transformer model. Sub-leading
terms such as nonlinearities, biases, and layer normalization are omitted.
For contexts and models with dmodel > nctx/12, the context-dependent computational cost per token is a
relatively small fraction of the total compute. Since we primarily study models where dmodel ≫nctx/12,
we do not include context-dependent terms in our training compute estimate. Accounting for the backwards
pass (approximately twice the compute as the forwards pass), we then deﬁne the estimated non-embedding
compute as C ≈6N ﬂoating point operators per training token.
2.2
Training Procedures
Unless otherwise noted, we train models with the Adam optimizer [KB14] for a ﬁxed 2.5 × 105 steps with
a batch size of 512 sequences of 1024 tokens. Due to memory constraints, our largest models (more than
1B parameters) were trained with Adafactor [SS18]. We experimented with a variety of learning rates and
schedules, as discussed in Appendix D.6. We found that results at convergence were largely independent of
learning rate schedule. Unless otherwise noted, all training runs included in our data used a learning rate
schedule with a 3000 step linear warmup followed by a cosine decay to zero.
2.3
Datasets
We train our models on an extended version of the WebText dataset described in [RWC+19]. The original
WebText dataset was a web scrape of outbound links from Reddit through December 2017 which received at
least 3 karma. In the second version, WebText2, we added outbound Reddit links from the period of January
to October 2018, also with a minimum of 3 karma. The karma threshold served as a heuristic for whether
people found the link interesting or useful. The text of the new links was extracted with the Newspaper3k
python library. In total, the dataset consists of 20.3M documents containing 96 GB of text and 1.62 × 1010
words (as deﬁned by wc). We then apply the reversible tokenizer described in [RWC+19], which yields
2.29 × 1010 tokens. We reserve 6.6 × 108 of these tokens for use as a test set, and we also test on similarly-
prepared samples of Books Corpus [ZKZ+15], Common Crawl [Fou], English Wikipedia, and a collection
of publicly-available Internet Books.
3
Empirical Results and Basic Power Laws
To characterize language model scaling we train a wide variety of models, varying a number of factors
including:
• Model size (ranging in size from 768 to 1.5 billion non-embedding parameters)
• Dataset size (ranging from 22 million to 23 billion tokens)
• Shape (including depth, width, attention heads, and feed-forward dimension)
• Context length (1024 for most runs, though we also experiment with shorter contexts)
• Batch size (219 for most runs, but we also vary it to measure the critical batch size)
7

104.66600036621094 | 76.20653533935547 | 441.40704345703125 | 86.16913604736328 | Operation
Parameters
FLOPs per Token
 |  | 
104.66600036621094 | 95.21989440917969 | 396.5833435058594 | 106.676513671875 | Embed
(nvocab + nctx) dmodel
4dmodel
 | 1 | 
104.66600036621094 | 111.98191833496094 | 444.9551696777344 | 122.78655242919922 | Attention: QKV
nlayerdmodel3dattn
2nlayerdmodel3dattn
 | 2 | 
104.66600036621094 | 128.7439422607422 | 430.6811828613281 | 139.548583984375 | Attention: Mask
—
2nlayernctxdattn
 | 3 | 
104.66600036621094 | 145.5059051513672 | 437.9390563964844 | 156.310546875 | Attention: Project
nlayerdattndmodel
2nlayerdattndembd
 | 4 | 
104.66600036621094 | 162.26792907714844 | 435.08721923828125 | 173.07257080078125 | Feedforward
nlayer2dmodeldﬀ
2nlayer2dmodeldﬀ
 | 5 | 
104.66600036621094 | 179.0309295654297 | 423.132568359375 | 189.8345947265625 | De-embed
—
2dmodelnvocab
 | 6 | 
104.66600036621094 | 198.1838836669922 | 504.34417724609375 | 209.6405029296875 | Total (Non-Embedding)
N = 2dmodelnlayer (2dattn + dﬀ)
Cforward = 2N + 2nlayernctxdattn
 | 7 | 
90.0 | 221.2294921875 | 522.0026245117188 | 242.19204711914062 | Table 1
Parameter counts and compute (forward pass) estimates for a Transformer model. Sub-leading
terms such as nonlinearities, biases, and layer normalization are omitted.
 | 8 | 
89.99996948242188 | 273.3279113769531 | 521.9996337890625 | 327.1580505371094 | For contexts and models with dmodel > nctx/12, the context-dependent computational cost per token is a
relatively small fraction of the total compute. Since we primarily study models where dmodel ≫nctx/12,
we do not include context-dependent terms in our training compute estimate. Accounting for the backwards
pass (approximately twice the compute as the forwards pass), we then deﬁne the estimated non-embedding
compute as C ≈6N ﬂoating point operators per training token.
 | 9 | 
89.9999771118164 | 342.906494140625 | 199.59854125976562 | 352.86907958984375 | 2.2
Training Procedures
 | 10 | 
89.99993896484375 | 359.8944396972656 | 522.0037231445312 | 428.0180358886719 | Unless otherwise noted, we train models with the Adam optimizer [KB14] for a ﬁxed 2.5 × 105 steps with
a batch size of 512 sequences of 1024 tokens. Due to memory constraints, our largest models (more than
1B parameters) were trained with Adafactor [SS18]. We experimented with a variety of learning rates and
schedules, as discussed in Appendix D.6. We found that results at convergence were largely independent of
learning rate schedule. Unless otherwise noted, all training runs included in our data used a learning rate
schedule with a 3000 step linear warmup followed by a cosine decay to zero.
 | 11 | 
89.99993896484375 | 443.7664794921875 | 148.38079833984375 | 453.72906494140625 | 2.3
Datasets
 | 12 | 
89.99990844726562 | 462.8607177734375 | 521.9981689453125 | 572.5150146484375 | We train our models on an extended version of the WebText dataset described in [RWC+19]. The original
WebText dataset was a web scrape of outbound links from Reddit through December 2017 which received at
least 3 karma. In the second version, WebText2, we added outbound Reddit links from the period of January
to October 2018, also with a minimum of 3 karma. The karma threshold served as a heuristic for whether
people found the link interesting or useful. The text of the new links was extracted with the Newspaper3k
python library. In total, the dataset consists of 20.3M documents containing 96 GB of text and 1.62 × 1010
words (as deﬁned by wc). We then apply the reversible tokenizer described in [RWC+19], which yields
2.29 × 1010 tokens. We reserve 6.6 × 108 of these tokens for use as a test set, and we also test on similarly-
prepared samples of Books Corpus [ZKZ+15], Common Crawl [Fou], English Wikipedia, and a collection
of publicly-available Internet Books.
 | 13 | 
89.99990844726562 | 590.6691284179688 | 317.12481689453125 | 602.6243286132812 | 3
Empirical Results and Basic Power Laws
 | 14 | 
89.99990844726562 | 616.2094116210938 | 521.9981079101562 | 637.0809936523438 | To characterize language model scaling we train a wide variety of models, varying a number of factors
including:
 | 15 | 
115.90290832519531 | 647.729736328125 | 441.8595886230469 | 658.052001953125 | • Model size (ranging in size from 768 to 1.5 billion non-embedding parameters)
 | 16 | 
115.90290832519531 | 663.8187255859375 | 358.49249267578125 | 674.1409912109375 | • Dataset size (ranging from 22 million to 23 billion tokens)
 | 17 | 
115.90290832519531 | 679.90673828125 | 432.0662536621094 | 690.22900390625 | • Shape (including depth, width, attention heads, and feed-forward dimension)
 | 18 | 
115.90290832519531 | 695.9957275390625 | 467.71246337890625 | 706.3179931640625 | • Context length (1024 for most runs, though we also experiment with shorter contexts)
 | 19 | 
115.90290832519531 | 708.8294067382812 | 456.1971435546875 | 722.4070434570312 | • Batch size (219 for most runs, but we also vary it to measure the critical batch size)
 | 20 | 
303.5099182128906 | 742.3323974609375 | 308.4912109375 | 752.2949829101562 | 7
 | 21 | 
Feed-Forward Ratio (dff / dmodel) 
50M Parameters
Aspect Ratio (dmodel / nlayer)
Attention Head Dimension (dmodel / nhead) 
25M Parameters
10%
8%
6%
4%
2%
0%
Loss Increase
A wide range of architectures 
achieve similar performance
22% additional compute 
compensates for 1% loss increase
Figure 5
Performance depends very mildly on model shape when the total number of non-embedding
parameters N is held ﬁxed. The loss varies only a few percent over a wide range of shapes. Small differences
in parameter counts are compensated for by using the ﬁt to L(N) as a baseline. Aspect ratio in particular can
vary by a factor of 40 while only slightly impacting performance; an (nlayer, dmodel) = (6, 4288) reaches a
loss within 3% of the (48, 1600) model used in [RWC+19].
106
107
108
109
Parameters (with embedding)
2
3
4
5
6
7
Test Loss
0 Layer
1 Layer
2 Layers
3 Layers
6 Layers
> 6 Layers
103
104
105
106
107
108
109
Parameters (non-embedding)
2
3
4
5
6
7
Test Loss
1 Layer
2 Layers
3 Layers
6 Layers
> 6 Layers
Figure 6
Left: When we include embedding parameters, performance appears to depend strongly on the
number of layers in addition to the number of parameters. Right: When we exclude embedding parameters,
the performance of models with different depths converge to a single trend. Only models with fewer than 2
layers or with extreme depth-to-width ratios deviate signiﬁcantly from the trend.
In this section we will display data along with empirically-motivated ﬁts, deferring theoretical analysis to
later sections.
3.1
Approximate Transformer Shape and Hyperparameter Independence
Transformer performance depends very weakly on the shape parameters nlayer, nheads, and dﬀwhen we hold
the total non-embedding parameter count N ﬁxed. To establish these results we trained models with ﬁxed
size while varying a single hyperparameter. This was simplest for the case of nheads. When varying nlayer,
we simultaneously varied dmodel while keeping N ≈12nlayerd2
model ﬁxed. Similarly, to vary dﬀat ﬁxed
model size we also simultaneously varied the dmodel parameter, as required by the parameter counts in Table
1. Independence of nlayers would follow if deeper Transformers effectively behave as ensembles of shallower
models, as has been suggested for ResNets [VWB16]. The results are shown in Figure 5.
3.2
Performance with Non-Embedding Parameter Count N
In Figure 6 we display the performance of a wide variety of models, ranging from small models with shape
(nlayer, dmodel) = (2, 128) through billion-parameter models, ranging in shape from (6, 4288) through
(207, 768). Here we have trained to near convergence on the full WebText2 dataset and observe no over-
ﬁtting (except possibly for the very largest models).
As shown in Figure 1, we ﬁnd a steady trend with non-embedding parameter count N, which can be ﬁt to the
ﬁrst term of Equation (1.5), so that
L(N) ≈
Nc
N
αN
(3.1)
8

131.9876708984375 | 161.04222106933594 | 239.79576110839844 | 169.7822265625 | Feed-Forward Ratio (dff / dmodel) 
 |  | 
158.69790649414062 | 161.04222106933594 | 524.491455078125 | 178.65631103515625 | 50M Parameters
Aspect Ratio (dmodel / nlayer)
Attention Head Dimension (dmodel / nhead) 
 | 1 | 
428.4188232421875 | 170.00428771972656 | 480.8955078125 | 178.65631103515625 | 25M Parameters
 | 2 | 
100.04014587402344 | 72.07621002197266 | 111.101318359375 | 78.17766571044922 | 10%
 | 3 | 
102.65879821777344 | 86.21691131591797 | 110.80803680419922 | 92.31836700439453 | 8%
 | 4 | 
102.65879821777344 | 100.35762786865234 | 110.80803680419922 | 106.4590835571289 | 6%
 | 5 | 
102.65879821777344 | 114.49834442138672 | 110.80803680419922 | 120.59980010986328 | 4%
 | 6 | 
102.65879821777344 | 128.1153106689453 | 110.80803680419922 | 134.21676635742188 | 2%
 | 7 | 
102.65879821777344 | 142.25601196289062 | 110.80803680419922 | 148.3574676513672 | 0%
 | 8 | 
91.32003784179688 | 91.32433319091797 | 100.06004333496094 | 138.33853149414062 | Loss Increase
 | 9 | 
270.776123046875 | 100.26335144042969 | 357.53094482421875 | 107.7547836303711 | A wide range of architectures 
 | 10 | 
272.3472900390625 | 108.11930847167969 | 354.2062683105469 | 115.6107406616211 | achieve similar performance
 | 11 | 
404.8509826660156 | 113.50115966796875 | 501.9316711425781 | 128.15512084960938 | 22% additional compute 
compensates for 1% loss increase
 | 12 | 
90.0 | 187.25750732421875 | 522.0037231445312 | 240.94705200195312 | Figure 5
Performance depends very mildly on model shape when the total number of non-embedding
parameters N is held ﬁxed. The loss varies only a few percent over a wide range of shapes. Small differences
in parameter counts are compensated for by using the ﬁt to L(N) as a baseline. Aspect ratio in particular can
vary by a factor of 40 while only slightly impacting performance; an (nlayer, dmodel) = (6, 4288) reaches a
loss within 3% of the (48, 1600) model used in [RWC+19].
 | 13 | 
159.70535278320312 | 360.030517578125 | 292.4484558105469 | 369.8311462402344 | 106
107
108
109
 | 14 | 
166.59640502929688 | 368.7972106933594 | 265.8825988769531 | 378.3381652832031 | Parameters (with embedding)
 | 15 | 
123.17610168457031 | 354.6152038574219 | 127.34085845947266 | 364.1561584472656 | 2
 | 16 | 
123.17610168457031 | 322.8787841796875 | 127.34085845947266 | 332.41973876953125 | 3
 | 17 | 
123.17610168457031 | 300.3614501953125 | 127.34085845947266 | 309.90240478515625 | 4
 | 18 | 
123.17610168457031 | 282.8956604003906 | 127.34085845947266 | 292.4366149902344 | 5
 | 19 | 
123.17610168457031 | 268.62506103515625 | 127.34085845947266 | 278.166015625 | 6
 | 20 | 
123.17610168457031 | 256.5594482421875 | 127.34085845947266 | 266.10040283203125 | 7
 | 21 | 
112.67709350585938 | 293.6032409667969 | 122.21804809570312 | 324.93060302734375 | Test Loss
 | 22 | 
154.0966339111328 | 295.66607666015625 | 182.66058349609375 | 344.0294494628906 | 0 Layer
1 Layer
2 Layers
3 Layers
6 Layers
 | 23 | 
155.37240600585938 | 343.2716979980469 | 190.69973754882812 | 354.3777160644531 | > 6 Layers
 | 24 | 
331.6294250488281 | 360.030517578125 | 492.9139099121094 | 369.8311462402344 | 103
104
105
106
107
108
109
 | 25 | 
364.50592041015625 | 368.7972106933594 | 461.7490539550781 | 378.3381652832031 | Parameters (non-embedding)
 | 26 | 
320.06610107421875 | 354.6152038574219 | 324.2308654785156 | 364.1561584472656 | 2
 | 27 | 
320.06610107421875 | 322.8787841796875 | 324.2308654785156 | 332.41973876953125 | 3
 | 28 | 
320.06610107421875 | 300.3614501953125 | 324.2308654785156 | 309.90240478515625 | 4
 | 29 | 
320.06610107421875 | 282.8956604003906 | 324.2308654785156 | 292.4366149902344 | 5
 | 30 | 
320.06610107421875 | 268.62506103515625 | 324.2308654785156 | 278.166015625 | 6
 | 31 | 
320.06610107421875 | 256.5594482421875 | 324.2308654785156 | 266.10040283203125 | 7
 | 32 | 
309.5671081542969 | 293.6032409667969 | 319.1080627441406 | 324.93060302734375 | Test Loss
 | 33 | 
350.98663330078125 | 305.3716735839844 | 379.55059814453125 | 344.0294494628906 | 1 Layer
2 Layers
3 Layers
6 Layers
 | 34 | 
352.2624206542969 | 343.2716979980469 | 387.5897521972656 | 354.3777160644531 | > 6 Layers
 | 35 | 
89.99996948242188 | 387.85150146484375 | 522.002685546875 | 430.633056640625 | Figure 6
Left: When we include embedding parameters, performance appears to depend strongly on the
number of layers in addition to the number of parameters. Right: When we exclude embedding parameters,
the performance of models with different depths converge to a single trend. Only models with fewer than 2
layers or with extreme depth-to-width ratios deviate signiﬁcantly from the trend.
 | 36 | 
89.99996948242188 | 455.5454406738281 | 521.9981689453125 | 476.41705322265625 | In this section we will display data along with empirically-motivated ﬁts, deferring theoretical analysis to
later sections.
 | 37 | 
89.99996948242188 | 492.7514953613281 | 408.37493896484375 | 502.7140808105469 | 3.1
Approximate Transformer Shape and Hyperparameter Independence
 | 38 | 
89.99992370605469 | 513.3589477539062 | 522.0012817382812 | 590.052978515625 | Transformer performance depends very weakly on the shape parameters nlayer, nheads, and dﬀwhen we hold
the total non-embedding parameter count N ﬁxed. To establish these results we trained models with ﬁxed
size while varying a single hyperparameter. This was simplest for the case of nheads. When varying nlayer,
we simultaneously varied dmodel while keeping N ≈12nlayerd2
model ﬁxed. Similarly, to vary dﬀat ﬁxed
model size we also simultaneously varied the dmodel parameter, as required by the parameter counts in Table
1. Independence of nlayers would follow if deeper Transformers effectively behave as ensembles of shallower
models, as has been suggested for ResNets [VWB16]. The results are shown in Figure 5.
 | 39 | 
89.9999771118164 | 606.2478637695312 | 348.7078857421875 | 616.3500366210938 | 3.2
Performance with Non-Embedding Parameter Count N
 | 40 | 
89.99993896484375 | 627.2254028320312 | 522.0009155273438 | 669.9149780273438 | In Figure 6 we display the performance of a wide variety of models, ranging from small models with shape
(nlayer, dmodel) = (2, 128) through billion-parameter models, ranging in shape from (6, 4288) through
(207, 768). Here we have trained to near convergence on the full WebText2 dataset and observe no over-
ﬁtting (except possibly for the very largest models).
 | 41 | 
89.99993896484375 | 676.10986328125 | 522.0001831054688 | 697.2129516601562 | As shown in Figure 1, we ﬁnd a steady trend with non-embedding parameter count N, which can be ﬁt to the
ﬁrst term of Equation (1.5), so that
 | 42 | 
266.9489440917969 | 700.8462524414062 | 323.94952392578125 | 717.6934814453125 | L(N) ≈
Nc
 | 43 | 
313.8680114746094 | 714.56494140625 | 321.8779296875 | 724.5275268554688 | N
 | 44 | 
325.6400146484375 | 698.083740234375 | 522.0003662109375 | 717.9240112304688 | αN
(3.1)
 | 45 | 
303.509033203125 | 742.3324584960938 | 308.4903259277344 | 752.2950439453125 | 8
 | 46 | 
LSTM plateaus after <100 tokens 
Transformer improves through the whole context
2M
200M
3M
300M
5
4
3
2
6
Token Index in Context
103
102
101
Transformers asymptotically outperform LSTMs 
due to improved use of long contexts
3.6
4.2
3.0
2.4
4.8
5.4
105
108
106
107
109
Parameters (non-embedding)
Transformers
LSTMs
1 Layer
2 Layers
4 Layers
Test Loss
Per-token 
Test Loss
Parameters:
400K
400K
Figure 7
To observe these trends it is crucial to study performance as a function of N; if we instead use the total
parameter count (including the embedding parameters) the trend is somewhat obscured (see Figure 6). This
suggests that the embedding matrix can be made smaller without impacting performance, as has been seen in
recent work [LCG+19].
Although these models have been trained on the WebText2 dataset, their test loss on a variety of other datasets
is also a power-law in N with nearly identical power, as shown in Figure 8.
3.2.1
Comparing to LSTMs and Universal Transformers
In Figure 7 we compare LSTM and Transformer performance as a function of non-embedding parameter
count N. The LSTMs were trained with the same dataset and context length. We see from these ﬁgures
that the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match
the Transformer performance for later tokens. We present power-law relationships between performance and
context position Appendix D.5, where increasingly large powers for larger models suggest improved ability
to quickly recognize patterns.
We also compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure
17 in the appendix. These models re-use parameters, and so perform slightly better as a function of N, at the
cost of additional compute per-parameter.
3.2.2
Generalization Among Data Distributions
We have also tested our models on a set of additional text data distributions. The test loss on these datasets
as a function of model size is shown in Figure 8; in all cases the models were trained only on the WebText2
dataset. We see that the loss on these other data distributions improves smoothly with model size, in direct
parallel with the improvement on WebText2. We ﬁnd that generalization depends almost exclusively on the
in-distribution validation loss, and does not depend on the duration of training or proximity to convergence.
We also observe no dependence on model depth (see Appendix D.8).
3.3
Performance with Dataset Size and Compute
We display empirical trends for the test loss as a function of dataset size D (in tokens) and training compute
C in Figure 1.
For the trend with D we trained a model with (nlayer, nembd) = (36, 1280) on ﬁxed subsets of the WebText2
dataset. We stopped training once the test loss ceased to decrease. We see that the resulting test losses can be
ﬁt with simple power-law
L(D) ≈
Dc
D
αD
(3.2)
in the dataset size. The data and ﬁt appear in Figure 1.
The total amount of non-embedding compute used during training can be estimated as C = 6NBS, where
B is the batch size, S is the number of parameter updates, and the factor of 6 accounts for the forward and
backward passes. Thus for a given value of C we can scan over all models with various N to ﬁnd the model
9

339.2064208984375 | 72.30683898925781 | 486.4994812011719 | 88.21244049072266 | LSTM plateaus after <100 tokens 
Transformer improves through the whole context
 |  | 
502.3408203125 | 147.27108764648438 | 508.6667175292969 | 152.43553161621094 | 2M
 | 1 | 
502.3408203125 | 164.55978393554688 | 513.59619140625 | 169.72422790527344 | 200M
 | 2 | 
502.3408203125 | 154.3638916015625 | 508.6667175292969 | 159.52833557128906 | 3M
 | 3 | 
502.3408203125 | 178.74539184570312 | 513.59619140625 | 183.9098358154297 | 300M
 | 4 | 
331.6703186035156 | 143.76724243164062 | 334.6280212402344 | 149.96456909179688 | 5
 | 5 | 
331.6703186035156 | 121.60224151611328 | 334.6280212402344 | 127.79957580566406 | 4
 | 6 | 
331.6703186035156 | 165.48895263671875 | 334.6280212402344 | 171.686279296875 | 3
 | 7 | 
331.6703186035156 | 187.2106475830078 | 334.6280212402344 | 193.40797424316406 | 2
 | 8 | 
331.6703186035156 | 99.4372329711914 | 334.6280212402344 | 105.63456726074219 | 6
 | 9 | 
336.98992919921875 | 202.62596130371094 | 402.7445983886719 | 210.02374267578125 | Token Index in Context
 | 10 | 
379.54669189453125 | 193.92398071289062 | 502.00390625 | 201.67063903808594 | 103
102
101
 | 11 | 
129.96881103515625 | 72.30683898925781 | 283.1201171875 | 88.21244049072266 | Transformers asymptotically outperform LSTMs 
due to improved use of long contexts
 | 12 | 
120.21621704101562 | 135.34454345703125 | 127.61045837402344 | 141.5418701171875 | 3.6
 | 13 | 
120.21621704101562 | 118.49913787841797 | 127.61045837402344 | 124.69647216796875 | 4.2
 | 14 | 
120.21621704101562 | 155.2930450439453 | 127.61045837402344 | 161.49037170410156 | 3.0
 | 15 | 
120.21621704101562 | 179.2312469482422 | 127.61045837402344 | 185.42857360839844 | 2.4
 | 16 | 
120.21621704101562 | 104.31354522705078 | 127.61045837402344 | 110.51087951660156 | 4.8
 | 17 | 
120.21621704101562 | 91.45784759521484 | 127.61045837402344 | 97.65518188476562 | 5.4
 | 18 | 
141.9379119873047 | 193.92398071289062 | 284.3436279296875 | 201.67063903808594 | 105
108
106
107
109
 | 19 | 
129.5255126953125 | 202.62596130371094 | 211.7328338623047 | 210.02374267578125 | Parameters (non-embedding)
 | 20 | 
157.4534149169922 | 155.62506103515625 | 192.37083435058594 | 162.49444580078125 | Transformers
 | 21 | 
233.70101928710938 | 118.38787078857422 | 252.2748565673828 | 125.25724792480469 | LSTMs
 | 22 | 
246.11341857910156 | 141.91424560546875 | 263.06695556640625 | 148.19137573242188 | 1 Layer
 | 23 | 
248.77320861816406 | 149.00704956054688 | 280.6978759765625 | 161.0470733642578 | 2 Layers
4 Layers
 | 24 | 
91.84500885009766 | 91.3465805053711 | 329.515869140625 | 105.30874633789062 | Test Loss
Per-token 
Test Loss
 | 25 | 
496.57794189453125 | 125.00413513183594 | 521.3805541992188 | 130.28826904296875 | Parameters:
 | 26 | 
502.3408203125 | 133.9720916748047 | 512.69189453125 | 139.13653564453125 | 400K
 | 27 | 
502.3408203125 | 142.39479064941406 | 512.69189453125 | 147.55923461914062 | 400K
 | 28 | 
288.24200439453125 | 216.93853759765625 | 323.7586975097656 | 226.90113830566406 | Figure 7
 | 29 | 
90.0 | 249.9999237060547 | 521.9982299804688 | 292.9200744628906 | To observe these trends it is crucial to study performance as a function of N; if we instead use the total
parameter count (including the embedding parameters) the trend is somewhat obscured (see Figure 6). This
suggests that the embedding matrix can be made smaller without impacting performance, as has been seen in
recent work [LCG+19].
 | 30 | 
90.0 | 299.3464660644531 | 521.9979858398438 | 320.21807861328125 | Although these models have been trained on the WebText2 dataset, their test loss on a variety of other datasets
is also a power-law in N with nearly identical power, as shown in Figure 8.
 | 31 | 
90.00000762939453 | 333.322509765625 | 336.49462890625 | 343.28509521484375 | 3.2.1
Comparing to LSTMs and Universal Transformers
 | 32 | 
90.0 | 352.06146240234375 | 522.0033569335938 | 416.570068359375 | In Figure 7 we compare LSTM and Transformer performance as a function of non-embedding parameter
count N. The LSTMs were trained with the same dataset and context length. We see from these ﬁgures
that the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match
the Transformer performance for later tokens. We present power-law relationships between performance and
context position Appendix D.5, where increasingly large powers for larger models suggest improved ability
to quickly recognize patterns.
 | 33 | 
90.0 | 421.4857482910156 | 522.0026245117188 | 454.77606201171875 | We also compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure
17 in the appendix. These models re-use parameters, and so perform slightly better as a function of N, at the
cost of additional compute per-parameter.
 | 34 | 
90.0 | 467.8815002441406 | 297.6304931640625 | 477.8440856933594 | 3.2.2
Generalization Among Data Distributions
 | 35 | 
90.0 | 486.6204528808594 | 521.9981689453125 | 551.1279907226562 | We have also tested our models on a set of additional text data distributions. The test loss on these datasets
as a function of model size is shown in Figure 8; in all cases the models were trained only on the WebText2
dataset. We see that the loss on these other data distributions improves smoothly with model size, in direct
parallel with the improvement on WebText2. We ﬁnd that generalization depends almost exclusively on the
in-distribution validation loss, and does not depend on the duration of training or proximity to convergence.
We also observe no dependence on model depth (see Appendix D.8).
 | 36 | 
90.0 | 565.5784912109375 | 303.1597900390625 | 575.5410766601562 | 3.3
Performance with Dataset Size and Compute
 | 37 | 
90.0 | 585.431884765625 | 521.99755859375 | 606.5339965820312 | We display empirical trends for the test loss as a function of dataset size D (in tokens) and training compute
C in Figure 1.
 | 38 | 
89.99996948242188 | 612.7288818359375 | 521.998291015625 | 644.7410278320312 | For the trend with D we trained a model with (nlayer, nembd) = (36, 1280) on ﬁxed subsets of the WebText2
dataset. We stopped training once the test loss ceased to decrease. We see that the resulting test losses can be
ﬁt with simple power-law
 | 39 | 
267.2099609375 | 645.8623046875 | 323.8885192871094 | 662.70947265625 | L(D) ≈
Dc
 | 40 | 
313.9679870605469 | 659.5798950195312 | 322.2170104980469 | 669.54248046875 | D
 | 41 | 
325.5789794921875 | 643.0997314453125 | 522.0003662109375 | 662.9400634765625 | αD
(3.2)
 | 42 | 
90.0 | 674.2374877929688 | 306.13861083984375 | 684.2000732421875 | in the dataset size. The data and ﬁt appear in Figure 1.
 | 43 | 
89.99993896484375 | 690.3959350585938 | 522.0021362304688 | 722.4070434570312 | The total amount of non-embedding compute used during training can be estimated as C = 6NBS, where
B is the batch size, S is the number of parameter updates, and the factor of 6 accounts for the forward and
backward passes. Thus for a given value of C we can scan over all models with various N to ﬁnd the model
 | 44 | 
303.5089111328125 | 742.3324584960938 | 308.4902038574219 | 752.2950439453125 | 9
 | 45 | 
104
105
106
107
108
109
Parameters (non-embedding)
3
4
5
6
7
Test Loss
WebText2 (Test)
Internet Books
Books
Wikipedia
Common Crawl
2.5
3.0
3.5
4.0
4.5
5.0
Test Loss on Training Distribution
2.5
3.0
3.5
4.0
4.5
5.0
Loss on Other Distribution
Books during training
Wikipedia during training
Books at convergence
Wikipedia at convergence
Figure 8
Left: Generalization performance to other data distributions improves smoothly with model size,
with only a small and very slowly growing offset from the WebText2 training distribution. Right: Gener-
alization performance depends only on training distribution performance, and not on the phase of training.
We compare generalization of converged models (points) to that of a single large model (dashed curves) as it
trains.
with the best performance on step S =
C
6BS . Note that in these results the batch size B remains ﬁxed for
all models, which means that these empirical results are not truly optimal. We will account for this in later
sections using an adjusted Cmin to produce cleaner trends.
The result appears as the heavy black line on the left-hand plot in Figure 1. It can be ﬁt with
L(C) ≈
Cc
C
αC
(3.3)
The ﬁgure also includes images of individual learning curves to clarify when individual models are optimal.
We will study the optimal allocation of compute more closely later on. The data strongly suggests that sample
efﬁciency improves with model size, and we also illustrate this directly in Figure 19 in the appendix.
4
Charting the Inﬁnite Data Limit and Overﬁtting
In Section 3 we found a number of basic scaling laws for language modeling performance. Here we will
study the performance of a model of size N trained on a dataset with D tokens while varying N and D
simultaneously. We will empirically demonstrate that the optimally trained test loss accords with the scaling
law of Equation (1.5). This provides guidance on how much data we would need to train models of increasing
size while keeping overﬁtting under control.
4.1
Proposed L(N, D) Equation
We have chosen the parameterization (1.5) (repeated here for convenience):
L(N, D) =
"Nc
N
 αN
αD + Dc
D
#αD
(4.1)
using three principles:
1. Changes in vocabulary size or tokenization are expected to rescale the loss by an overall factor. The
parameterization of L(N, D) (and all models of the loss) must naturally allow for such a rescaling.
2. Fixing D and sending N →∞, the overall loss should approach L(D). Conversely, ﬁxing N and
sending D →∞the loss must approach L(N).
3. L(N, D) should be analytic at D = ∞, so that it has a series expansion in 1/D with integer powers.
Theoretical support for this principle is signiﬁcantly weaker than for the ﬁrst two.
Our choice of L(N, D) satisﬁes the ﬁrst requirement because we can rescale Nc, Dc with changes in the
vocabulary. This also implies that the values of Nc, Dc have no fundamental meaning.
10

132.5243377685547 | 185.18667602539062 | 288.96295166015625 | 195.64085388183594 | 104
105
106
107
108
109
 |  | 
152.2411346435547 | 194.5379638671875 | 255.96865844726562 | 204.7151336669922 | Parameters (non-embedding)
 | 1 | 
104.83795928955078 | 151.6612091064453 | 109.28043365478516 | 161.83837890625 | 3
 | 2 | 
104.83795928955078 | 125.57516479492188 | 109.28043365478516 | 135.75233459472656 | 4
 | 3 | 
104.83795928955078 | 105.34125518798828 | 109.28043365478516 | 115.5184326171875 | 5
 | 4 | 
104.83795928955078 | 88.8089599609375 | 109.28043365478516 | 98.98613739013672 | 6
 | 5 | 
104.83795928955078 | 74.83110046386719 | 109.28043365478516 | 85.0082778930664 | 7
 | 6 | 
93.63884735107422 | 115.21663665771484 | 103.81602478027344 | 148.63296508789062 | Test Loss
 | 7 | 
249.8756561279297 | 82.0772705078125 | 291.7508850097656 | 118.62763214111328 | WebText2 (Test)
Internet Books
Books
Wikipedia
Common Crawl
 | 8 | 
335.2508850097656 | 185.76942443847656 | 496.31097412109375 | 195.953125 | 2.5
3.0
3.5
4.0
4.5
5.0
 | 9 | 
368.8568115234375 | 194.52969360351562 | 490.3343200683594 | 204.71339416503906 | Test Loss on Training Distribution
 | 10 | 
326.99688720703125 | 163.71466064453125 | 338.11016845703125 | 173.8983612060547 | 2.5
 | 11 | 
326.99688720703125 | 139.89263916015625 | 338.11016845703125 | 150.0763397216797 | 3.0
 | 12 | 
326.99688720703125 | 119.75139617919922 | 338.11016845703125 | 129.9351043701172 | 3.5
 | 13 | 
326.99688720703125 | 102.30426025390625 | 338.11016845703125 | 112.48796081542969 | 4.0
 | 14 | 
326.99688720703125 | 86.914794921875 | 338.11016845703125 | 97.09849548339844 | 4.5
 | 15 | 
326.99688720703125 | 73.14844512939453 | 338.11016845703125 | 83.33214569091797 | 5.0
 | 16 | 
315.79058837890625 | 84.66349029541016 | 325.9742736816406 | 179.1265869140625 | Loss on Other Distribution
 | 17 | 
448.14801025390625 | 80.9933853149414 | 513.874267578125 | 110.45287322998047 | Books during training
Wikipedia during training
Books at convergence
Wikipedia at convergence
 | 18 | 
90.0 | 214.3995361328125 | 521.9999389648438 | 268.0890808105469 | Figure 8
Left: Generalization performance to other data distributions improves smoothly with model size,
with only a small and very slowly growing offset from the WebText2 training distribution. Right: Gener-
alization performance depends only on training distribution performance, and not on the phase of training.
We compare generalization of converged models (points) to that of a single large model (dashed curves) as it
trains.
 | 19 | 
90.0 | 289.6607666015625 | 521.9989013671875 | 324.7530517578125 | with the best performance on step S =
C
6BS . Note that in these results the batch size B remains ﬁxed for
all models, which means that these empirical results are not truly optimal. We will account for this in later
sections using an adjusted Cmin to produce cleaner trends.
 | 20 | 
90.0 | 329.6844482421875 | 456.7431640625 | 339.6470642089844 | The result appears as the heavy black line on the left-hand plot in Figure 1. It can be ﬁt with
 | 21 | 
268.32098388671875 | 347.87335205078125 | 323.1795654296875 | 364.7195129394531 | L(C) ≈
Cc
 | 22 | 
314.1679992675781 | 361.5909423828125 | 321.291259765625 | 371.55352783203125 | C
 | 23 | 
324.8699951171875 | 345.1097412109375 | 522.0003662109375 | 364.9500732421875 | αC
(3.3)
 | 24 | 
90.0 | 379.36846923828125 | 521.9981689453125 | 411.1490783691406 | The ﬁgure also includes images of individual learning curves to clarify when individual models are optimal.
We will study the optimal allocation of compute more closely later on. The data strongly suggests that sample
efﬁciency improves with model size, and we also illustrate this directly in Figure 19 in the appendix.
 | 25 | 
90.0 | 428.1741638183594 | 354.2338562011719 | 440.1293640136719 | 4
Charting the Inﬁnite Data Limit and Overﬁtting
 | 26 | 
89.99996948242188 | 453.0364685058594 | 521.9981079101562 | 506.63507080078125 | In Section 3 we found a number of basic scaling laws for language modeling performance. Here we will
study the performance of a model of size N trained on a dataset with D tokens while varying N and D
simultaneously. We will empirically demonstrate that the optimally trained test loss accords with the scaling
law of Equation (1.5). This provides guidance on how much data we would need to train models of increasing
size while keeping overﬁtting under control.
 | 27 | 
89.99996948242188 | 521.1149291992188 | 232.3993682861328 | 531.2171020507812 | 4.1
Proposed L(N, D) Equation
 | 28 | 
89.9999771118164 | 541.406494140625 | 390.9402160644531 | 551.3690795898438 | We have chosen the parameterization (1.5) (repeated here for convenience):
 | 29 | 
234.68197631835938 | 570.1009521484375 | 281.221923828125 | 580.0635375976562 | L(N, D) =
 | 30 | 
283.98394775390625 | 560.2283325195312 | 309.8925476074219 | 574.16455078125 | "Nc
 | 31 | 
299.8110046386719 | 576.9349365234375 | 307.8209228515625 | 586.8975219726562 | N
 | 32 | 
311.5830078125 | 558.0709228515625 | 330.02191162109375 | 573.178955078125 |  αN
 | 33 | 
320.2130126953125 | 562.2059326171875 | 357.9396057128906 | 580.0635375976562 | αD + Dc
 | 34 | 
348.01800537109375 | 576.9349365234375 | 356.26702880859375 | 586.8975219726562 | D
 | 35 | 
359.6300048828125 | 556.8317260742188 | 522.0003662109375 | 580.2940063476562 | #αD
(4.1)
 | 36 | 
90.0 | 597.700439453125 | 178.547607421875 | 607.6630249023438 | using three principles:
 | 37 | 
113.41200256347656 | 618.16943359375 | 521.9979858398438 | 639.041015625 | 1. Changes in vocabulary size or tokenization are expected to rescale the loss by an overall factor. The
parameterization of L(N, D) (and all models of the loss) must naturally allow for such a rescaling.
 | 38 | 
113.41200256347656 | 643.8037719726562 | 522.0 | 665.0350341796875 | 2. Fixing D and sending N →∞, the overall loss should approach L(D). Conversely, ﬁxing N and
sending D →∞the loss must approach L(N).
 | 39 | 
113.41195678710938 | 669.7977905273438 | 521.99658203125 | 691.029052734375 | 3. L(N, D) should be analytic at D = ∞, so that it has a series expansion in 1/D with integer powers.
Theoretical support for this principle is signiﬁcantly weaker than for the ﬁrst two.
 | 40 | 
89.99993896484375 | 701.304931640625 | 522.0039672851562 | 723.9010620117188 | Our choice of L(N, D) satisﬁes the ﬁrst requirement because we can rescale Nc, Dc with changes in the
vocabulary. This also implies that the values of Nc, Dc have no fundamental meaning.
 | 41 | 
301.0189208984375 | 742.3324584960938 | 310.98150634765625 | 752.2950439453125 | 10
 | 42 | 
106
107
108
109
Params (non-embed)
2.5
3.0
3.5
4.0
4.5
Test Loss
Data Size Bottleneck
Data Size
21M
43M
86M
172M
344M
688M
1.4B
22.0B
10
4
10
3
10
2
10
1
N
N/
D/D
0.0
0.1
0.2
0.3
0.4
0.5
L/L(D =
)
1
Overfitting
Data Size
21M
43M
86M
172M
344M
688M
1.4B
22.0B
Figure 9
The early-stopped test loss L(N, D) depends predictably on the dataset size D and model size N
according to Equation (1.5). Left: For large D, performance is a straight power law in N. For a smaller ﬁxed
D, performance stops improving as N increases and the model begins to overﬁt. (The reverse is also true,
see Figure 4.) Right: The extent of overﬁtting depends predominantly on the ratio N
αN
αD /D, as predicted in
equation (4.3). The line is our ﬁt to that equation.
Since we stop training early when the test loss ceases to improve and optimize all models in the same way, we
expect that larger models should always perform better than smaller models. But with ﬁxed ﬁnite D, we also
do not expect any model to be capable of approaching the best possible loss (ie the entropy of text). Similarly,
a model with ﬁxed size will be capacity-limited. These considerations motivate our second principle. Note
that knowledge of L(N) at inﬁnite D and L(D) at inﬁnite N fully determines all the parameters in L(N, D).
The third principle is more speculative. There is a simple and general reason one might expect overﬁtting
to scale ∝1/D at very large D. Overﬁtting should be related to the variance or the signal-to-noise ratio
of the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function,
since we expect to be able to expand the loss about the D →∞limit. However, this argument assumes that
1/D corrections dominate over other sources of variance, such as the ﬁnite batch size and other limits on the
efﬁcacy of optimization. Without empirical conﬁrmation, we would not be very conﬁdent of its applicability.
Our third principle explains the asymmetry between the roles of N and D in Equation (1.5). Very similar
symmetric expressions4 are possible, but they would not have a 1/D expansion with integer powers, and
would require the introduction of an additional parameter.
In any case, we will see that our equation for L(N, D) ﬁts the data well, which is the most important justiﬁ-
cation for our L(N, D) ansatz.
4.2
Results
We regularize all our models with 10% dropout, and by tracking test loss and stopping once it is no longer
decreasing. The results are displayed in Figure 9, including a ﬁt to the four parameters αN, αD, Nc, Dc in
Equation (1.5):
Parameter
αN
αD
Nc
Dc
Value
0.076
0.103
6.4 × 1013
1.8 × 1013
Table 2
Fits to L(N, D)
We obtain an excellent ﬁt, with the exception of the runs where the dataset has been reduced by a factor of
1024, to about 2 × 107 tokens. With such a small dataset, an epoch consists of only 40 parameter updates.
Perhaps such a tiny dataset represents a different regime for language modeling, as overﬁtting happens very
early in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in
Section 3, as here we are ﬁtting the full L(N, D) rather than just L(N, ∞) or L(∞, D).
To chart the borderlands of the inﬁnite data limit, we can directly study the extent of overﬁtting. For all but
the largest models, we see no sign of overﬁtting when training with the full 22B token WebText2 dataset,
so we can take it as representative of D = ∞. Thus we can compare ﬁnite D to the inﬁnite data limit by
4For example, one might have used L(N, D) =
  Nc
N
αN +
  Dc
D
αDβ, but this does not have a 1/D expansion.
11

136.10789489746094 | 167.3115997314453 | 260.68377685546875 | 176.2294921875 | 106
107
108
109
 |  | 
157.1931610107422 | 175.28868103027344 | 220.00807189941406 | 183.97027587890625 | Params (non-embed)
 | 1 | 
103.02351379394531 | 153.02777099609375 | 112.49759674072266 | 161.70936584472656 | 2.5
 | 2 | 
103.02351379394531 | 132.81033325195312 | 112.49759674072266 | 141.49192810058594 | 3.0
 | 3 | 
103.02351379394531 | 115.71673583984375 | 112.49759674072266 | 124.39833068847656 | 3.5
 | 4 | 
103.02351379394531 | 100.90959167480469 | 112.49759674072266 | 109.5911865234375 | 4.0
 | 5 | 
103.02351379394531 | 87.84877014160156 | 112.49759674072266 | 96.53035736083984 | 4.5
 | 6 | 
93.47016906738281 | 110.6749267578125 | 102.1517562866211 | 139.1805877685547 | Test Loss
 | 7 | 
150.79571533203125 | 72.62787628173828 | 226.66700744628906 | 83.04578399658203 | Data Size Bottleneck
 | 8 | 
266.255859375 | 94.97993469238281 | 295.083251953125 | 103.6615219116211 | Data Size
 | 9 | 
280.32427978515625 | 103.46659088134766 | 292.92657470703125 | 153.37921142578125 | 21M
43M
86M
172M
344M
688M
1.4B
22.0B
 | 10 | 
361.2155456542969 | 166.67356872558594 | 479.0862731933594 | 175.5699462890625 | 10
4
10
3
10
2
10
1
 | 11 | 
399.32708740234375 | 174.22042846679688 | 422.61517333984375 | 184.44252014160156 | N
N/
D/D
 | 12 | 
325.2153625488281 | 161.67083740234375 | 334.7261657714844 | 170.38607788085938 | 0.0
 | 13 | 
325.2153625488281 | 145.36148071289062 | 334.7261657714844 | 154.07672119140625 | 0.1
 | 14 | 
325.2153625488281 | 129.05210876464844 | 334.7261657714844 | 137.76734924316406 | 0.2
 | 15 | 
325.2153625488281 | 112.74273681640625 | 334.7261657714844 | 121.4579849243164 | 0.3
 | 16 | 
325.2153625488281 | 96.4333724975586 | 334.7261657714844 | 105.14862060546875 | 0.4
 | 17 | 
325.2153625488281 | 80.1240005493164 | 334.7261657714844 | 88.83924865722656 | 0.5
 | 18 | 
314.72900390625 | 104.55984497070312 | 324.8738708496094 | 145.98599243164062 | L/L(D =
)
1
 | 19 | 
391.2057800292969 | 73.95935821533203 | 431.0650634765625 | 84.41765594482422 | Overfitting
 | 20 | 
488.2793884277344 | 95.20173645019531 | 517.2185668945312 | 103.91698455810547 | Data Size
 | 21 | 
502.40234375 | 103.72129821777344 | 515.0535278320312 | 153.82740783691406 | 21M
43M
86M
172M
344M
688M
1.4B
22.0B
 | 22 | 
89.99996948242188 | 193.02793884277344 | 522.000244140625 | 239.43807983398438 | Figure 9
The early-stopped test loss L(N, D) depends predictably on the dataset size D and model size N
according to Equation (1.5). Left: For large D, performance is a straight power law in N. For a smaller ﬁxed
D, performance stops improving as N increases and the model begins to overﬁt. (The reverse is also true,
see Figure 4.) Right: The extent of overﬁtting depends predominantly on the ratio N
 | 23 | 
90.00003051757812 | 224.03695678710938 | 522.004638671875 | 250.34707641601562 | αN
αD /D, as predicted in
equation (4.3). The line is our ﬁt to that equation.
 | 24 | 
90.00003051757812 | 274.8564453125 | 522.0036010742188 | 328.4550476074219 | Since we stop training early when the test loss ceases to improve and optimize all models in the same way, we
expect that larger models should always perform better than smaller models. But with ﬁxed ﬁnite D, we also
do not expect any model to be capable of approaching the best possible loss (ie the entropy of text). Similarly,
a model with ﬁxed size will be capacity-limited. These considerations motivate our second principle. Note
that knowledge of L(N) at inﬁnite D and L(D) at inﬁnite N fully determines all the parameters in L(N, D).
 | 25 | 
90.00007629394531 | 334.8814392089844 | 522.0042114257812 | 399.3890380859375 | The third principle is more speculative. There is a simple and general reason one might expect overﬁtting
to scale ∝1/D at very large D. Overﬁtting should be related to the variance or the signal-to-noise ratio
of the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function,
since we expect to be able to expand the loss about the D →∞limit. However, this argument assumes that
1/D corrections dominate over other sources of variance, such as the ﬁnite batch size and other limits on the
efﬁcacy of optimization. Without empirical conﬁrmation, we would not be very conﬁdent of its applicability.
 | 26 | 
90.00006103515625 | 405.58489990234375 | 522.0022583007812 | 437.5960388183594 | Our third principle explains the asymmetry between the roles of N and D in Equation (1.5). Very similar
symmetric expressions4 are possible, but they would not have a 1/D expansion with integer powers, and
would require the introduction of an additional parameter.
 | 27 | 
90.00004577636719 | 443.7919006347656 | 522.0038452148438 | 464.89404296875 | In any case, we will see that our equation for L(N, D) ﬁts the data well, which is the most important justiﬁ-
cation for our L(N, D) ansatz.
 | 28 | 
90.00003814697266 | 480.77349853515625 | 143.4095458984375 | 490.736083984375 | 4.2
Results
 | 29 | 
89.99996948242188 | 501.429443359375 | 521.9998779296875 | 533.2109985351562 | We regularize all our models with 10% dropout, and by tracking test loss and stopping once it is no longer
decreasing. The results are displayed in Figure 9, including a ﬁt to the four parameters αN, αD, Nc, Dc in
Equation (1.5):
 | 30 | 
193.9250030517578 | 545.3129272460938 | 399.58758544921875 | 556.1165771484375 | Parameter
αN
αD
Nc
Dc
 | 31 | 
202.70199584960938 | 565.427734375 | 415.0931396484375 | 576.9010620117188 | Value
0.076
0.103
6.4 × 1013
1.8 × 1013
 | 32 | 
253.70899963378906 | 584.6329345703125 | 358.29144287109375 | 594.8260498046875 | Table 2
Fits to L(N, D)
 | 33 | 
90.0 | 607.1664428710938 | 521.9982299804688 | 660.7650146484375 | We obtain an excellent ﬁt, with the exception of the runs where the dataset has been reduced by a factor of
1024, to about 2 × 107 tokens. With such a small dataset, an epoch consists of only 40 parameter updates.
Perhaps such a tiny dataset represents a different regime for language modeling, as overﬁtting happens very
early in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in
Section 3, as here we are ﬁtting the full L(N, D) rather than just L(N, ∞) or L(∞, D).
 | 34 | 
89.99996948242188 | 667.1914672851562 | 522.0028076171875 | 698.9720458984375 | To chart the borderlands of the inﬁnite data limit, we can directly study the extent of overﬁtting. For all but
the largest models, we see no sign of overﬁtting when training with the full 22B token WebText2 dataset,
so we can take it as representative of D = ∞. Thus we can compare ﬁnite D to the inﬁnite data limit by
 | 35 | 
102.65299987792969 | 708.0853271484375 | 295.0908508300781 | 722.1663818359375 | 4For example, one might have used L(N, D) =
  Nc
 | 36 | 
287.6210021972656 | 708.0853271484375 | 338.599853515625 | 724.348876953125 | N
αN +
  Dc
 | 37 | 
331.16900634765625 | 709.790283203125 | 513.2798461914062 | 724.348876953125 | D
αDβ, but this does not have a 1/D expansion.
 | 38 | 
301.01898193359375 | 742.3324584960938 | 310.9815673828125 | 752.2950439453125 | 11
 | 39 | 
101
3 × 100
4 × 100
6 × 100
WebText2 Train Loss
103
104
105
106
Critical Batch Size (Tokens)
Critical Batch Size vs. Performance
Empirical Bcrit, N = 3M
Empirical Bcrit, N = 85M
Bcrit = 2.1 × 108 tokens L
4.8
Noise Scale Measurement
Figure 10
The critical batch size Bcrit follows a power law in the loss as performance increase, and does
not depend directly on the model size. We ﬁnd that the critical batch size approximately doubles for every
13% decrease in loss. Bcrit is measured empirically from the data shown in Figure 18, but it is also roughly
predicted by the gradient noise scale, as in [MKAT18].
deﬁning
δL(N, D) ≡L(N, D)
L(N, ∞) −1
(4.2)
and studying it as a function of N, D. In fact, we see empirically that δL depends only a speciﬁc combination
of N and D, as shown in Figure 16. This follows from the scaling law of Equation (1.5), which implies
δL ≈
 
1 +
 N
Nc
 αN
αD Dc
D
!αD
−1
(4.3)
Note that at large D this formula also has a series expansion in powers of 1/D.
We estimate that the variation in the loss with different random seeds is roughly 0.02, which means that to
avoid overﬁtting when training to within that threshold of convergence we require
D ≳(5 × 103) N 0.74
(4.4)
With this relation, models smaller than 109 parameters can be trained with minimal overﬁtting on the 22B
token WebText2 dataset, but our largest models will encounter some mild overﬁtting. More generally, this
relation shows that dataset size may grow sub-linearly in model size while avoiding overﬁtting. Note however
that this does not typically represent maximally compute-efﬁcient training. We should also emphasize that
we have not optimized regularization (eg the dropout probability) while varying dataset and model size.
5
Scaling Laws with Model Size and Training Time
In this section we will demonstrate that a simple scaling law provides a good description for the loss as a
function of model size N and training time. First we will explain how to use the results of [MKAT18] to
deﬁne a universal training step Smin, which accounts for the fact that most of our models have not been
trained at an optimal batch size. Then we will demonstrate that we can ﬁt the model size and training time
dependence of the loss using Equation (1.6). Later we will use these results to predict the optimal allocation
of training compute between model size and training time, and then conﬁrm that prediction.
5.1
Adjustment for Training at Bcrit(L)
A simple empirical theory for the batch size dependence of training was developed in [MKAT18] (see also
[SLA+18, ZLN+19]). It was argued that there is a critical batch size Bcrit for training; for B up to Bcrit
the batch size can be increased with very minimal degradation in compute-efﬁciency, whereas for B > Bcrit
increases in B result in diminishing returns. It was also argued that the gradient noise scale provides a simple
12

228.57716369628906 | 210.9593963623047 | 413.11871337890625 | 224.964599609375 | 101
3 × 100
4 × 100
6 × 100
 |  | 
275.7100830078125 | 223.58387756347656 | 369.7503967285156 | 236.3248748779297 | WebText2 Train Loss
 | 1 | 
193.1013946533203 | 204.12176513671875 | 208.20120239257812 | 217.1275634765625 | 103
 | 2 | 
193.1013946533203 | 169.65077209472656 | 208.20120239257812 | 182.6565704345703 | 104
 | 3 | 
193.1013946533203 | 135.09779357910156 | 208.20120239257812 | 148.1035919189453 | 105
 | 4 | 
193.1013946533203 | 100.70877838134766 | 208.20120239257812 | 113.7145767211914 | 106
 | 5 | 
179.08102416992188 | 87.68567657470703 | 191.822021484375 | 211.68524169921875 | Critical Batch Size (Tokens)
 | 6 | 
228.4488067626953 | 72.92012786865234 | 417.57525634765625 | 88.20932006835938 | Critical Batch Size vs. Performance
 | 7 | 
336.7987060546875 | 165.80105590820312 | 407.73382568359375 | 176.55178833007812 | Empirical Bcrit, N = 3M
 | 8 | 
336.7987060546875 | 175.7949676513672 | 411.70782470703125 | 186.5457000732422 | Empirical Bcrit, N = 85M
 | 9 | 
336.7987060546875 | 184.24197387695312 | 426.54010009765625 | 198.2514190673828 | Bcrit = 2.1 × 108 tokens L
4.8
 | 10 | 
336.7987060546875 | 197.1757049560547 | 419.97314453125 | 206.27642822265625 | Noise Scale Measurement
 | 11 | 
90.0 | 246.4269256591797 | 522.0044555664062 | 289.3470764160156 | Figure 10
The critical batch size Bcrit follows a power law in the loss as performance increase, and does
not depend directly on the model size. We ﬁnd that the critical batch size approximately doubles for every
13% decrease in loss. Bcrit is measured empirically from the data shown in Figure 18, but it is also roughly
predicted by the gradient noise scale, as in [MKAT18].
 | 12 | 
90.00001525878906 | 312.88446044921875 | 122.65741729736328 | 322.8470764160156 | deﬁning
 | 13 | 
250.4440155029297 | 323.7879333496094 | 342.4854431152344 | 340.4905090332031 | δL(N, D) ≡L(N, D)
 | 14 | 
305.7460021972656 | 330.3988037109375 | 522.0003662109375 | 347.32452392578125 | L(N, ∞) −1
(4.2)
 | 15 | 
90.00003051757812 | 352.26092529296875 | 521.9990844726562 | 373.3630676269531 | and studying it as a function of N, D. In fact, we see empirically that δL depends only a speciﬁc combination
of N and D, as shown in Figure 16. This follows from the scaling law of Equation (1.5), which implies
 | 16 | 
232.92303466796875 | 392.1138000488281 | 255.0269317626953 | 402.20550537109375 | δL ≈
 | 17 | 
257.7920227050781 | 382.3703308105469 | 265.6824035644531 | 392.3329162597656 |  
 | 18 | 
265.67901611328125 | 378.1959228515625 | 300.8619384765625 | 402.20550537109375 | 1 +
 N
 | 19 | 
291.3659973144531 | 399.0769348144531 | 302.93359375 | 409.88055419921875 | Nc
 | 20 | 
304.6239929199219 | 380.2139587402344 | 323.0628967285156 | 395.3219299316406 |  αN
 | 21 | 
313.2539978027344 | 384.3489074707031 | 340.464599609375 | 396.3075256347656 | αD Dc
 | 22 | 
330.5429992675781 | 399.0769348144531 | 338.7920227050781 | 409.0395202636719 | D
 | 23 | 
342.1549987792969 | 378.9747619628906 | 522.0003662109375 | 402.43609619140625 | !αD
−1
(4.3)
 | 24 | 
90.0 | 420.47796630859375 | 405.29864501953125 | 430.6711120605469 | Note that at large D this formula also has a series expansion in powers of 1/D.
 | 25 | 
89.99996948242188 | 436.8669738769531 | 522.0030517578125 | 457.9691162109375 | We estimate that the variation in the loss with different random seeds is roughly 0.02, which means that to
avoid overﬁtting when training to within that threshold of convergence we require
 | 26 | 
262.6619873046875 | 463.82147216796875 | 522.0003662109375 | 476.1911315917969 | D ≳(5 × 103) N 0.74
(4.4)
 | 27 | 
90.00003051757812 | 480.83551025390625 | 522.001220703125 | 538.049072265625 | With this relation, models smaller than 109 parameters can be trained with minimal overﬁtting on the 22B
token WebText2 dataset, but our largest models will encounter some mild overﬁtting. More generally, this
relation shows that dataset size may grow sub-linearly in model size while avoiding overﬁtting. Note however
that this does not typically represent maximally compute-efﬁcient training. We should also emphasize that
we have not optimized regularization (eg the dropout probability) while varying dataset and model size.
 | 28 | 
90.00004577636719 | 555.2411499023438 | 358.9203796386719 | 567.1963500976562 | 5
Scaling Laws with Model Size and Training Time
 | 29 | 
90.00004577636719 | 580.2044677734375 | 522.0035400390625 | 644.7120361328125 | In this section we will demonstrate that a simple scaling law provides a good description for the loss as a
function of model size N and training time. First we will explain how to use the results of [MKAT18] to
deﬁne a universal training step Smin, which accounts for the fact that most of our models have not been
trained at an optimal batch size. Then we will demonstrate that we can ﬁt the model size and training time
dependence of the loss using Equation (1.6). Later we will use these results to predict the optimal allocation
of training compute between model size and training time, and then conﬁrm that prediction.
 | 30 | 
90.00004577636719 | 659.3589477539062 | 265.031494140625 | 670.16259765625 | 5.1
Adjustment for Training at Bcrit(L)
 | 31 | 
90.00003051757812 | 679.7174682617188 | 521.9981689453125 | 722.4070434570312 | A simple empirical theory for the batch size dependence of training was developed in [MKAT18] (see also
[SLA+18, ZLN+19]). It was argued that there is a critical batch size Bcrit for training; for B up to Bcrit
the batch size can be increased with very minimal degradation in compute-efﬁciency, whereas for B > Bcrit
increases in B result in diminishing returns. It was also argued that the gradient noise scale provides a simple
 | 32 | 
301.01904296875 | 742.3324584960938 | 310.98162841796875 | 752.2950439453125 | 12
 | 33 | 
prediction for Bcrit, and that neither depends directly on model size except through the value of the loss that
has been attained. These results can be used to predict how training time and compute will vary with the
batch size. To utilize both training time and compute as effectively as possible, it is best to train with a batch
size B ≈Bcrit. Training at B ≫Bcrit minimizes the number of training steps, while B ≪Bcrit minimizes
the use of compute.
More speciﬁcally, it was demonstrated that for a wide variety of neural network tasks, the number of training
steps S and the number of data examples processed E = BS satisfy the simple relation
 S
Smin
−1
  E
Emin
−1

= 1
(5.1)
when training to any ﬁxed value of the loss L. Here Smin is the minimum number of steps necessary to reach
L, while Emin is the minimum number of data examples that must be processed.
We demonstrate the relation (5.1) for Transformers in Figure 18 in the appendix. This relation deﬁnes the
critical batch size
Bcrit(L) ≡Emin
Smin
(5.2)
which is a function of the target value of the loss. Training at the critical batch size makes a roughly optimal
time/compute tradeoff, requiring 2Smin training steps and processing E = 2Emin data examples.
In Figure 10 we have plotted the critical batch size and gradient noise scale5 as a function of training loss for
two different models. We see that Bcrit(L) is independent of model size, and only depends on the loss L. So
the predictions of [MKAT18] continue to hold for Transformer language models. The critical batch size can
be ﬁt with a power-law in the loss
Bcrit(L) ≈
B∗
L1/αB
(5.3)
where B∗≈2 × 108 and αB ≈0.21.
We have chosen this parameterization for Bcrit(L) because as the loss approaches its minimum value Lmin,
the gradient noise scale is expected to diverge, and we expect Bcrit to track this noise scale. We do not
know Lmin, as we see no sign that our models are approaching it, but Lmin > 0 since the entropy of natural
language is non-zero. Since apparently Lmin is much smaller than the values of L we have achieved, we used
a parameterization where Bcrit diverges as L →0.
We will use Bcrit(L) to estimate the relation between the number of training steps S while training at batch
size B = 219 tokens and the number of training steps while training at B ≫Bcrit. This is simply
Smin(S) ≡
S
1 + Bcrit(L)/B
(minimum steps, at B ≫Bcrit)
(5.4)
for any given target value L for the loss. This also deﬁnes a critical value of the compute needed to train to L
with a model of size N if we were to train at B ≪Bcrit(L). This is
Cmin(C) ≡
C
1 + B/Bcrit(L)
(minimum compute, at B ≪Bcrit)
(5.5)
where C = 6NBS estimates the (non-embedding) compute used at batch size B.
5.2
Results for L(N, Smin) and Performance with Model Size and Compute
Now we will use Smin deﬁned in Equation (5.4) to obtain a simple and universal ﬁt for the dependence of the
loss on model size and training time in the inﬁnite data limit. We will ﬁt the stable, Adam-optimized training
runs using Equation (1.6), repeated here for convenience:
L(N, Smin) =
Nc
N
αN
+
 Sc
Smin
αS
(5.6)
for the loss. We include all training steps after the warmup period of the learning rate schedule, and ﬁnd a ﬁt
to the data with the parameters:
5Although the critical batch size roughly matches the gradient noise scale, we are using a direct measurements of
Bcrit from Figures 18 and 10 for all our later analyses.
13

89.99999237060547 | 74.17692565917969 | 522.0001831054688 | 128.00607299804688 | prediction for Bcrit, and that neither depends directly on model size except through the value of the loss that
has been attained. These results can be used to predict how training time and compute will vary with the
batch size. To utilize both training time and compute as effectively as possible, it is best to train with a batch
size B ≈Bcrit. Training at B ≫Bcrit minimizes the number of training steps, while B ≪Bcrit minimizes
the use of compute.
 |  | 
90.00006103515625 | 134.4324493408203 | 521.9982299804688 | 172.80548095703125 | More speciﬁcally, it was demonstrated that for a wide variety of neural network tasks, the number of training
steps S and the number of data examples processed E = BS satisfy the simple relation
 S
 | 1 | 
250.75 | 155.5349578857422 | 320.3493957519531 | 187.2205810546875 | Smin
−1
  E
 | 2 | 
306.43499755859375 | 162.69837951660156 | 522.0003662109375 | 187.2205810546875 | Emin
−1

= 1
(5.1)
 | 3 | 
90.00001525878906 | 193.72496032714844 | 521.9996948242188 | 216.32113647460938 | when training to any ﬁxed value of the loss L. Here Smin is the minimum number of steps necessary to reach
L, while Emin is the minimum number of data examples that must be processed.
 | 4 | 
90.00001525878906 | 221.2525177001953 | 521.9981689453125 | 257.37860107421875 | We demonstrate the relation (5.1) for Transformers in Figure 18 in the appendix. This relation deﬁnes the
critical batch size
Bcrit(L) ≡Emin
 | 5 | 
320.0419921875 | 246.8054962158203 | 522.0003662109375 | 264.21258544921875 | Smin
(5.2)
 | 6 | 
90.0 | 266.80950927734375 | 521.9979858398438 | 289.17510986328125 | which is a function of the target value of the loss. Training at the critical batch size makes a roughly optimal
time/compute tradeoff, requiring 2Smin training steps and processing E = 2Emin data examples.
 | 7 | 
90.00003051757812 | 290.49151611328125 | 522.003662109375 | 336.7961120605469 | In Figure 10 we have plotted the critical batch size and gradient noise scale5 as a function of training loss for
two different models. We see that Bcrit(L) is independent of model size, and only depends on the loss L. So
the predictions of [MKAT18] continue to hold for Transformer language models. The critical batch size can
be ﬁt with a power-law in the loss
 | 8 | 
267.655029296875 | 336.56396484375 | 522.0003662109375 | 360.2795104980469 | Bcrit(L) ≈
B∗
L1/αB
(5.3)
 | 9 | 
90.0 | 359.9624938964844 | 239.9226531982422 | 374.6744079589844 | where B∗≈2 × 108 and αB ≈0.21.
 | 10 | 
90.0 | 379.7359619140625 | 522.0020141601562 | 435.05908203125 | We have chosen this parameterization for Bcrit(L) because as the loss approaches its minimum value Lmin,
the gradient noise scale is expected to diverge, and we expect Bcrit to track this noise scale. We do not
know Lmin, as we see no sign that our models are approaching it, but Lmin > 0 since the entropy of natural
language is non-zero. Since apparently Lmin is much smaller than the values of L we have achieved, we used
a parameterization where Bcrit diverges as L →0.
 | 11 | 
90.00009155273438 | 439.7609558105469 | 522.0023193359375 | 461.47357177734375 | We will use Bcrit(L) to estimate the relation between the number of training steps S while training at batch
size B = 219 tokens and the number of training steps while training at B ≫Bcrit. This is simply
 | 12 | 
175.0701141357422 | 467.5039367675781 | 522.0004272460938 | 491.8815612792969 | Smin(S) ≡
S
1 + Bcrit(L)/B
(minimum steps, at B ≫Bcrit)
(5.4)
 | 13 | 
90.00006103515625 | 498.24493408203125 | 522.004638671875 | 519.9585571289062 | for any given target value L for the loss. This also deﬁnes a critical value of the compute needed to train to L
with a model of size N if we were to train at B ≪Bcrit(L). This is
 | 14 | 
166.79611206054688 | 526.3219604492188 | 522.0003662109375 | 550.6995849609375 | Cmin(C) ≡
C
1 + B/Bcrit(L)
(minimum compute, at B ≪Bcrit)
(5.5)
 | 15 | 
90.00003051757812 | 557.0639038085938 | 414.9806823730469 | 567.2570190429688 | where C = 6NBS estimates the (non-embedding) compute used at batch size B.
 | 16 | 
90.00003051757812 | 581.5039672851562 | 414.9790954589844 | 592.3075561523438 | 5.2
Results for L(N, Smin) and Performance with Model Size and Compute
 | 17 | 
90.0000228881836 | 601.4789428710938 | 521.9982299804688 | 633.4900512695312 | Now we will use Smin deﬁned in Equation (5.4) to obtain a simple and universal ﬁt for the dependence of the
loss on model size and training time in the inﬁnite data limit. We will ﬁt the stable, Adam-optimized training
runs using Equation (1.6), repeated here for convenience:
 | 18 | 
225.24002075195312 | 641.42529296875 | 305.9226379394531 | 659.112548828125 | L(N, Smin) =
Nc
 | 19 | 
295.84100341796875 | 655.1429443359375 | 303.8509216308594 | 665.1055297851562 | N
 | 20 | 
307.6130065917969 | 634.2619018554688 | 362.2226257324219 | 658.271484375 | αN
+
 Sc
 | 21 | 
347.7300109863281 | 655.1429443359375 | 367.0404357910156 | 665.946533203125 | Smin
 | 22 | 
368.7330322265625 | 638.6617431640625 | 522.0004272460938 | 658.5020141601562 | αS
(5.6)
 | 23 | 
90.00006103515625 | 672.6284790039062 | 521.9981689453125 | 693.5000610351562 | for the loss. We include all training steps after the warmup period of the learning rate schedule, and ﬁnd a ﬁt
to the data with the parameters:
 | 24 | 
90.0 | 701.6947021484375 | 522.00048828125 | 723.162353515625 | 5Although the critical batch size roughly matches the gradient noise scale, we are using a direct measurements of
Bcrit from Figures 18 and 10 for all our later analyses.
 | 25 | 
301.01898193359375 | 742.3324584960938 | 310.9815673828125 | 752.2950439453125 | 13
 | 26 | 
104
106
108
Parameters (non-embedding)
2
3
4
5
6
7
8
Test Loss
Performance vs Compute Budget
10
5
10
4
10
3
10
2
10
1
100
PF-dayss
106
107
108
109
Parameters (non-embedding)
2.4
3.0
3.6
4.2
4.8
5.4
Test Loss
Performance vs Steps
104
105
Steps
Figure 11
When we hold either total compute or number of training steps ﬁxed, performance follows
L(N, S) from Equation (5.6). Each value of compute budget has an associated optimal model size that
maximizes performance. Mediocre ﬁts at small S are unsurprising, as the power-law equation for the learning
curves breaks down very early in training.
Parameter
αN
αS
Nc
Sc
Value
0.077
0.76
6.5 × 1013
2.1 × 103
Table 3
Fits to L(N, S)
With these parameters, we obtain the learning curve ﬁts in Figure 4. Though the ﬁts are imperfect, we believe
they are quite compelling given the simplicity of Equation (5.6).
The data and ﬁts can be visualized in a different and more interesting way, as shown in Figure 11. There we
study the test loss as a function of model size while ﬁxing either the total non-embedding compute C used
in training, or the number of steps S. For the ﬁts we use Equation (5.5) and (5.4) along with the parameters
above and Equation (5.6).
The power-law dependence of the loss on Smin reﬂects the interplay of optimizer dynamics and the loss
landscape. Since the ﬁts are best late in training, when the loss may be approximately quadratic, the power-
law should provide information about the spectrum of the Hessian of the loss. Its universality suggests that
the Hessian eigenvalue density is roughly independent of model size.
5.3
Lower Bound on Early Stopping Step
The results for L(N, Smin) can be used to derive a lower-bound (and rough estimate) of the step at which
early stopping should occur when training is data limited. It is motivated by the idea that ﬁnite and inﬁnite D
learning curves for a given model will be very similar until we reach Smin ≈Sstop. Thus overﬁtting should
be proportional to the correction from simply ending training at Sstop. This will underestimate Sstop, because
in reality the test loss will decrease more slowly when we have a ﬁnite D, and therefore we will require more
training steps to reach the optimal test loss at ﬁnite D. This line of reasoning leads to the inequality
Sstop(N, D) ≳
Sc
[L(N, D) −L(N, ∞)]1/αS
(5.7)
where L(N, ∞) is the converged loss, evaluated with inﬁnite available data. This inequality and its com-
parison to the empirical data is displayed in Figure 16 in the appendix. In that ﬁgure, the values of Sstop
and L(N, D) are empirical (though Sstop is adjusted to mimic training at B ≫Bcrit), while L(N, ∞) is
computed from the ﬁt to L(N, D) evaluated at D = ∞.
6
Optimal Allocation of the Compute Budget
We displayed the empirical trend of performance as a function of the computation used during training in
the top-right of Figure 1. However, this result involved training at a ﬁxed batch size B, whereas we know
14

129.76760864257812 | 182.46713256835938 | 229.6159210205078 | 192.7504425048828 | 104
106
108
 |  | 
129.52801513671875 | 191.65875244140625 | 232.20359802246094 | 201.7327117919922 | Parameters (non-embedding)
 | 1 | 
104.71273803710938 | 176.68446350097656 | 109.11015319824219 | 186.7584228515625 | 2
 | 2 | 
104.71273803710938 | 148.36268615722656 | 109.11015319824219 | 158.4366455078125 | 3
 | 3 | 
104.71273803710938 | 128.26806640625 | 109.11015319824219 | 138.34202575683594 | 4
 | 4 | 
104.71273803710938 | 112.68147277832031 | 109.11015319824219 | 122.75543975830078 | 5
 | 5 | 
104.71273803710938 | 99.94630432128906 | 109.11015319824219 | 110.020263671875 | 6
 | 6 | 
104.71273803710938 | 79.8516845703125 | 109.11015319824219 | 99.2528305053711 | 7
8
 | 7 | 
93.62721252441406 | 116.77861785888672 | 103.701171875 | 149.85604858398438 | Test Loss
 | 8 | 
111.35128021240234 | 72.7259750366211 | 250.8324432373047 | 84.81472778320312 | Performance vs Compute Budget
 | 9 | 
266.916015625 | 164.58206176757812 | 282.91033935546875 | 174.86538696289062 | 10
5
 | 10 | 
266.916015625 | 149.9785614013672 | 282.91033935546875 | 160.26187133789062 | 10
4
 | 11 | 
266.916015625 | 135.31024169921875 | 282.91033935546875 | 145.5935516357422 | 10
3
 | 12 | 
266.916015625 | 120.64191436767578 | 282.91033935546875 | 130.92523193359375 | 10
2
 | 13 | 
266.916015625 | 105.97357940673828 | 282.91033935546875 | 116.25691223144531 | 10
1
 | 14 | 
266.916015625 | 91.30525207519531 | 278.8550109863281 | 101.58858489990234 | 100
 | 15 | 
282.76953125 | 117.6528549194336 | 292.843505859375 | 148.98793029785156 | PF-dayss
 | 16 | 
354.5531921386719 | 182.43798828125 | 486.4498291015625 | 192.76937866210938 | 106
107
108
109
 | 17 | 
361.48004150390625 | 191.679443359375 | 463.9892883300781 | 201.73709106445312 | Parameters (non-embedding)
 | 18 | 
331.1609191894531 | 159.30543518066406 | 342.13665771484375 | 169.3630828857422 | 2.4
 | 19 | 
331.1609191894531 | 137.98020935058594 | 342.13665771484375 | 148.03785705566406 | 3.0
 | 20 | 
331.1609191894531 | 120.55624389648438 | 342.13665771484375 | 130.6138916015625 | 3.6
 | 21 | 
331.1609191894531 | 105.82447814941406 | 342.13665771484375 | 115.88211822509766 | 4.2
 | 22 | 
331.1609191894531 | 93.06324768066406 | 342.13665771484375 | 103.12088775634766 | 4.8
 | 23 | 
331.1609191894531 | 81.80705261230469 | 342.13665771484375 | 91.86469268798828 | 5.4
 | 24 | 
320.0933837890625 | 116.92058563232422 | 330.1510009765625 | 149.9444580078125 | Test Loss
 | 25 | 
367.0001220703125 | 72.93930053710938 | 458.77386474609375 | 85.00846862792969 | Performance vs Steps
 | 26 | 
497.4720153808594 | 149.99942016601562 | 509.3916931152344 | 160.26609802246094 | 104
 | 27 | 
497.4720153808594 | 99.60411071777344 | 509.3916931152344 | 109.87078857421875 | 105
 | 28 | 
509.35528564453125 | 123.65786743164062 | 519.4129028320312 | 143.2071075439453 | Steps
 | 29 | 
89.99995422363281 | 211.3895263671875 | 522.003173828125 | 254.17007446289062 | Figure 11
When we hold either total compute or number of training steps ﬁxed, performance follows
L(N, S) from Equation (5.6). Each value of compute budget has an associated optimal model size that
maximizes performance. Mediocre ﬁts at small S are unsurprising, as the power-law equation for the learning
curves breaks down very early in training.
 | 30 | 
198.40196228027344 | 273.450927734375 | 396.027587890625 | 284.25555419921875 | Parameter
αN
αS
Nc
Sc
 | 31 | 
207.17897033691406 | 293.5667419433594 | 410.6130676269531 | 305.0390625 | Value
0.077
0.76
6.5 × 1013
2.1 × 103
 | 32 | 
254.62997436523438 | 312.77093505859375 | 357.3704528808594 | 322.9640808105469 | Table 3
Fits to L(N, S)
 | 33 | 
90.0 | 345.56646728515625 | 521.9982299804688 | 366.4380798339844 | With these parameters, we obtain the learning curve ﬁts in Figure 4. Though the ﬁts are imperfect, we believe
they are quite compelling given the simplicity of Equation (5.6).
 | 34 | 
90.0 | 372.8644714355469 | 522.00048828125 | 415.5540771484375 | The data and ﬁts can be visualized in a different and more interesting way, as shown in Figure 11. There we
study the test loss as a function of model size while ﬁxing either the total non-embedding compute C used
in training, or the number of steps S. For the ﬁts we use Equation (5.5) and (5.4) along with the parameters
above and Equation (5.6).
 | 35 | 
90.0 | 421.74993896484375 | 522.0003051757812 | 464.6700744628906 | The power-law dependence of the loss on Smin reﬂects the interplay of optimizer dynamics and the loss
landscape. Since the ﬁts are best late in training, when the loss may be approximately quadratic, the power-
law should provide information about the spectrum of the Hessian of the loss. Its universality suggests that
the Hessian eigenvalue density is roughly independent of model size.
 | 36 | 
90.00001525878906 | 481.76751708984375 | 272.026611328125 | 491.7301025390625 | 5.3
Lower Bound on Early Stopping Step
 | 37 | 
90.0 | 502.679931640625 | 522.0049438476562 | 567.4180297851562 | The results for L(N, Smin) can be used to derive a lower-bound (and rough estimate) of the step at which
early stopping should occur when training is data limited. It is motivated by the idea that ﬁnite and inﬁnite D
learning curves for a given model will be very similar until we reach Smin ≈Sstop. Thus overﬁtting should
be proportional to the correction from simply ending training at Sstop. This will underestimate Sstop, because
in reality the test loss will decrease more slowly when we have a ﬁnite D, and therefore we will require more
training steps to reach the optimal test loss at ﬁnite D. This line of reasoning leads to the inequality
 | 38 | 
217.9520263671875 | 576.5529174804688 | 522.0003051757812 | 602.3995361328125 | Sstop(N, D) ≳
Sc
[L(N, D) −L(N, ∞)]1/αS
(5.7)
 | 39 | 
90.0 | 612.6337890625 | 522.0051879882812 | 655.6830444335938 | where L(N, ∞) is the converged loss, evaluated with inﬁnite available data. This inequality and its com-
parison to the empirical data is displayed in Figure 16 in the appendix. In that ﬁgure, the values of Sstop
and L(N, D) are empirical (though Sstop is adjusted to mimic training at B ≫Bcrit), while L(N, ∞) is
computed from the ﬁt to L(N, D) evaluated at D = ∞.
 | 40 | 
90.00007629394531 | 675.1861572265625 | 327.44232177734375 | 687.141357421875 | 6
Optimal Allocation of the Compute Budget
 | 41 | 
90.00006866455078 | 701.3590087890625 | 522.003662109375 | 722.4070434570312 | We displayed the empirical trend of performance as a function of the computation used during training in
the top-right of Figure 1. However, this result involved training at a ﬁxed batch size B, whereas we know
 | 42 | 
301.0190734863281 | 742.3324584960938 | 310.9816589355469 | 752.2950439453125 | 14
 | 43 | 
Models between 0.6x and 2.2x the 
optimal size can be trained with a 
20% larger compute budget
Smaller models require 
more steps to train, while 
larger models require fewer
Our framework does not 
capture early training dynamics
Figure 12
Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger
or smaller models can be trained with minimal additional compute. Right: Models larger than the compute-
efﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-
lelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the
power-law region of the learning curve, after initial transient effects.
10
8
10
6
10
4
10
2
100
Compute (PF-days), non-embedding
2
3
4
5
6
7
Test Loss
L = (Cmin/2.3 108)
0.050
L = (C/2.0 107)
0.057
Figure 13
When adjusting performance to simulate training far below the critical batch size, we ﬁnd a
somewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous
lump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks
in the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger
compute.
that in fact we could train more efﬁciently6 by training at the batch size Bcrit discussed in Section 5.1.
Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,
and correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more
predictable trends.
In this section we will adjust for this oversight. More importantly, we will use the results of Section 5
to determine the optimal allocation of compute between model size N and the quantity of data processed
during training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by
using the equation for L(N, Smin), and we will demonstrate that these methods agree.
6.1
Optimal Performance and Allocations
Let us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is
plotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the
new ﬁt with Cmin is somewhat improved.
Given L(Cmin), it is natural to ask for the optimal model size N(Cmin) that provides the minimal loss with a
given quantity of training compute. The optimal model size is shown in Figure 14. We observe that N(Cmin)
6One might ask why we did not simply train at Bcrit in the ﬁrst place. The reason is that it depends not only on the
model but also on the target value of the loss we wish to achieve, and so is a moving target.
15

141.67323303222656 | 123.1880111694336 | 258.30145263671875 | 131.82337951660156 | Models between 0.6x and 2.2x the 
 |  | 
142.70816040039062 | 131.98483276367188 | 256.916748046875 | 140.6201934814453 | optimal size can be trained with a 
 | 1 | 
152.02243041992188 | 140.78164672851562 | 245.9611053466797 | 149.41700744628906 | 20% larger compute budget
 | 2 | 
365.2159423828125 | 82.82613372802734 | 456.7639465332031 | 109.05513763427734 | Smaller models require 
more steps to train, while 
larger models require fewer
 | 3 | 
374.5302429199219 | 163.6078338623047 | 471.14935302734375 | 180.95309448242188 | Our framework does not 
capture early training dynamics
 | 4 | 
90.0 | 213.7864990234375 | 521.9990234375 | 267.4770202636719 | Figure 12
Left: Given a ﬁxed compute budget, a particular model size is optimal, though somewhat larger
or smaller models can be trained with minimal additional compute. Right: Models larger than the compute-
efﬁcient size require fewer steps to train, allowing for potentially faster training if sufﬁcient additional paral-
lelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the
power-law region of the learning curve, after initial transient effects.
 | 5 | 
209.72154235839844 | 397.348388671875 | 395.00238037109375 | 408.170166015625 | 10
8
10
6
10
4
10
2
100
 | 6 | 
247.92996215820312 | 407.02130126953125 | 380.5754089355469 | 417.62274169921875 | Compute (PF-days), non-embedding
 | 7 | 
210.8616943359375 | 391.262939453125 | 215.48936462402344 | 401.8643798828125 | 2
 | 8 | 
210.8616943359375 | 356.9736328125 | 215.48936462402344 | 367.5750732421875 | 3
 | 9 | 
210.8616943359375 | 332.6449890136719 | 215.48936462402344 | 343.2464294433594 | 4
 | 10 | 
210.8616943359375 | 313.77423095703125 | 215.48936462402344 | 324.37567138671875 | 5
 | 11 | 
210.8616943359375 | 298.3556823730469 | 215.48936462402344 | 308.9571228027344 | 6
 | 12 | 
210.8616943359375 | 285.31951904296875 | 215.48936462402344 | 295.92095947265625 | 7
 | 13 | 
199.19570922851562 | 323.2828674316406 | 209.79714965820312 | 358.09228515625 | Test Loss
 | 14 | 
318.480224609375 | 288.0309143066406 | 403.1048889160156 | 301.462646484375 | L = (Cmin/2.3 108)
0.050
 | 15 | 
318.480224609375 | 300.32794189453125 | 393.07452392578125 | 312.7623596191406 | L = (C/2.0 107)
0.057
 | 16 | 
90.0 | 427.4195251464844 | 522.0015869140625 | 481.10906982421875 | Figure 13
When adjusting performance to simulate training far below the critical batch size, we ﬁnd a
somewhat altered power law for L(Cmin) when compared with the fully empirical results. The conspicuous
lump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks
in the power-law ﬁts. It is the L(Cmin) trend that we expect to provide a reliable extrapolation for larger
compute.
 | 17 | 
90.0 | 501.449462890625 | 521.998291015625 | 547.7540283203125 | that in fact we could train more efﬁciently6 by training at the batch size Bcrit discussed in Section 5.1.
Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,
and correcting for this inefﬁciency by standardizing to the critical batch size results in cleaner and more
predictable trends.
 | 18 | 
90.0 | 554.180419921875 | 522.0028076171875 | 597.4805297851562 | In this section we will adjust for this oversight. More importantly, we will use the results of Section 5
to determine the optimal allocation of compute between model size N and the quantity of data processed
during training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by
using the equation for L(N, Smin), and we will demonstrate that these methods agree.
 | 19 | 
90.00001525878906 | 612.12646484375 | 273.84979248046875 | 622.0890502929688 | 6.1
Optimal Performance and Allocations
 | 20 | 
90.00001525878906 | 632.533447265625 | 521.9982299804688 | 665.8080444335938 | Let us ﬁrst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is
plotted in Figure 13, along with a power-law ﬁt. We see that as compared to the compute plot of Figure 1, the
new ﬁt with Cmin is somewhat improved.
 | 21 | 
90.0 | 670.5089111328125 | 522.0014038085938 | 692.2225341796875 | Given L(Cmin), it is natural to ask for the optimal model size N(Cmin) that provides the minimal loss with a
given quantity of training compute. The optimal model size is shown in Figure 14. We observe that N(Cmin)
 | 22 | 
89.99996948242188 | 701.6947021484375 | 522.0018920898438 | 722.1663818359375 | 6One might ask why we did not simply train at Bcrit in the ﬁrst place. The reason is that it depends not only on the
model but also on the target value of the loss we wish to achieve, and so is a moving target.
 | 23 | 
301.01898193359375 | 742.3324584960938 | 310.9815673828125 | 752.2950439453125 | 15
 | 24 | 
10
7
10
5
10
3
10
1
Compute (PF-days), non-embedding
103
105
107
Parameters (non-embedding)
N = (1.3 109) C0.73
min
N = (1.6 109) C0.88
10
7
10
5
10
3
10
1
Compute (PF-days), excluding embeddings
0
5000
10000
15000
Steps
Smin (adjusted)
Smin = (5.4 103) C0.03
min
S (fixed-batch)
Figure 14
Left: Each value of the compute budget Cmin has an associated optimal model size N. Optimal
model size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number
of data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.
Right: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most
of the growth in data examples processed can be used for increased batch sizes.
can be ﬁt very well with a power-law
N(Cmin) ∝(Cmin)0.73.
(6.1)
In Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4).
By deﬁnition Cmin ≡6NBcritS, and so we can use N(Cmin) to extract further results. In particular, since
prior ﬁts show B ∝L−4.8 and L ∝C−0.05
min
, we can conclude that Bcrit ∝C0.24
min . This leads us to conclude
that the optimal number of steps will only grow very slowly with compute, as
Smin ∝(Cmin)0.03,
(6.2)
matching the empirical results in Figure 14. In fact the measured exponent is sufﬁciently small that our results
may even be consistent with an exponent of zero.
Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we
should predominantly increase the model size N, while simultaneously scaling up the batch size via B ∝
Bcrit with negligible increase in the number of serial steps. Since compute-efﬁcient training uses relatively
few optimization steps, additional work on speeding up early training dynamics may be warranted.
6.2
Predictions from L(N, Smin)
The results for L(Cmin) and the allocations can be predicted from the L(N, Smin) equation obtained in
Section 5. Given our equation for L(N, Smin), we can substitute Smin = Cmin
6NB and then ﬁnd the minimum
of the loss as a function of N, while ﬁxing the training compute. We carry out this procedure in detail in
Appendix B, where we also provide some additional predictions.
For the loss as a function of training compute, we predict that
L(Cmin) =
Cmin
c
Cmin
αmin
C
(6.3)
where
αmin
C
≡
1
1/αS + 1/αB + 1/αN
≈0.054
(6.4)
in excellent agreement with the exponent of Figure 13. We also predict that
N(Cmin) ∝(Cmin)αmin
C
/αN ≈(Cmin)0.71
(6.5)
which also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive
framework for the performance of language modeling.
16

142.45748901367188 | 183.7381134033203 | 289.3598327636719 | 194.20758056640625 | 10
7
10
5
10
3
10
1
 |  | 
144.81146240234375 | 193.10308837890625 | 272.3346862792969 | 203.29515075683594 | Compute (PF-days), non-embedding
 | 1 | 
104.95751190185547 | 160.43600463867188 | 117.03649139404297 | 170.8398895263672 | 103
 | 2 | 
104.95751190185547 | 129.89358520507812 | 117.03649139404297 | 140.29747009277344 | 105
 | 3 | 
104.95751190185547 | 99.41671752929688 | 117.03649139404297 | 109.82061004638672 | 107
 | 4 | 
93.6405258178711 | 77.40242004394531 | 103.83258819580078 | 181.28175354003906 | Parameters (non-embedding)
 | 5 | 
146.8290252685547 | 79.49041748046875 | 215.6947021484375 | 102.98368835449219 | N = (1.3 109) C0.73
min
N = (1.6 109) C0.88
 | 6 | 
368.8979187011719 | 183.77735900878906 | 512.7354125976562 | 194.2283935546875 | 10
7
10
5
10
3
10
1
 | 7 | 
359.5565185546875 | 193.12583923339844 | 511.4839172363281 | 203.29995727539062 | Compute (PF-days), excluding embeddings
 | 8 | 
344.7542724609375 | 178.00267028808594 | 349.1954040527344 | 188.17678833007812 | 0
 | 9 | 
331.427490234375 | 148.67868041992188 | 349.1920471191406 | 158.85279846191406 | 5000
 | 10 | 
326.9852600097656 | 119.35467529296875 | 349.19091796875 | 129.52879333496094 | 10000
 | 11 | 
326.9852600097656 | 90.03067016601562 | 349.19091796875 | 100.20478820800781 | 15000
 | 12 | 
315.7895202636719 | 119.57813262939453 | 325.963623046875 | 139.353759765625 | Steps
 | 13 | 
378.5248107910156 | 79.66865539550781 | 430.3208923339844 | 90.82849884033203 | Smin (adjusted)
 | 14 | 
378.5248107910156 | 90.68640899658203 | 455.8866882324219 | 112.8145751953125 | Smin = (5.4 103) C0.03
min
S (fixed-batch)
 | 15 | 
90.0 | 212.8429412841797 | 522.00244140625 | 266.6730651855469 | Figure 14
Left: Each value of the compute budget Cmin has an associated optimal model size N. Optimal
model size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number
of data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.
Right: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most
of the growth in data examples processed can be used for increased batch sizes.
 | 16 | 
90.00001525878906 | 291.2314453125 | 237.82505798339844 | 301.1940612792969 | can be ﬁt very well with a power-law
 | 17 | 
257.4720153808594 | 308.46673583984375 | 522.0003662109375 | 321.04754638671875 | N(Cmin) ∝(Cmin)0.73.
(6.1)
 | 18 | 
90.0 | 329.71746826171875 | 459.7220153808594 | 339.6800842285156 | In Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4).
 | 19 | 
90.0 | 345.746826171875 | 522.0028076171875 | 379.2210998535156 | By deﬁnition Cmin ≡6NBcritS, and so we can use N(Cmin) to extract further results. In particular, since
prior ﬁts show B ∝L−4.8 and L ∝C−0.05
min
, we can conclude that Bcrit ∝C0.24
min . This leads us to conclude
that the optimal number of steps will only grow very slowly with compute, as
 | 20 | 
266.39801025390625 | 386.4937744140625 | 522.0003662109375 | 399.5984191894531 | Smin ∝(Cmin)0.03,
(6.2)
 | 21 | 
90.00003051757812 | 407.7445068359375 | 521.9981689453125 | 428.6161193847656 | matching the empirical results in Figure 14. In fact the measured exponent is sufﬁciently small that our results
may even be consistent with an exponent of zero.
 | 22 | 
90.00003051757812 | 435.0425109863281 | 522.0023193359375 | 477.73211669921875 | Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we
should predominantly increase the model size N, while simultaneously scaling up the batch size via B ∝
Bcrit with negligible increase in the number of serial steps. Since compute-efﬁcient training uses relatively
few optimization steps, additional work on speeding up early training dynamics may be warranted.
 | 23 | 
90.00003051757812 | 493.5279846191406 | 233.51148986816406 | 504.33160400390625 | 6.2
Predictions from L(N, Smin)
 | 24 | 
90.00003051757812 | 514.114990234375 | 522.0001220703125 | 537.9085693359375 | The results for L(Cmin) and the allocations can be predicted from the L(N, Smin) equation obtained in
Section 5. Given our equation for L(N, Smin), we can substitute Smin = Cmin
 | 25 | 
89.99996185302734 | 526.6824951171875 | 522.004150390625 | 558.4630737304688 | 6NB and then ﬁnd the minimum
of the loss as a function of N, while ﬁxing the training compute. We carry out this procedure in detail in
Appendix B, where we also provide some additional predictions.
 | 26 | 
89.99996185302734 | 564.8894653320312 | 335.0101623535156 | 574.85205078125 | For the loss as a function of training compute, we predict that
 | 27 | 
253.69696044921875 | 587.1017456054688 | 331.8923645019531 | 612.759521484375 | L(Cmin) =
Cmin
c
Cmin
 | 28 | 
333.5849914550781 | 583.4249267578125 | 522.0003662109375 | 605.3140258789062 | αmin
C
(6.3)
 | 29 | 
90.0 | 620.9014282226562 | 114.33863067626953 | 630.864013671875 | where
 | 30 | 
223.34800720214844 | 630.2279052734375 | 522.0003662109375 | 655.258544921875 | αmin
C
≡
1
1/αS + 1/αB + 1/αN
≈0.054
(6.4)
 | 31 | 
90.0 | 660.2984619140625 | 390.26275634765625 | 670.2610473632812 | in excellent agreement with the exponent of Figure 13. We also predict that
 | 32 | 
221.4080047607422 | 678.82080078125 | 522.0003662109375 | 692.8656005859375 | N(Cmin) ∝(Cmin)αmin
C
/αN ≈(Cmin)0.71
(6.5)
 | 33 | 
90.0 | 701.5354614257812 | 521.9981689453125 | 722.4070434570312 | which also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive
framework for the performance of language modeling.
 | 34 | 
301.01898193359375 | 742.3324584960938 | 310.9815673828125 | 752.2950439453125 | 16
 | 35 | 
The intersection point is sensitive to 
the precise power-law parameters
Figure 15
Far beyond the model sizes we study empirically, we ﬁnd a contradiction between our equations
for L(Cmin) and L(D) due to the slow growth of data needed for compute-efﬁcient training. The intersection
marks the point before which we expect our predictions to break down. The location of this point is highly
sensitive to the precise exponents from our power-law ﬁts.
6.3
Contradictions and a Conjecture
We observe no signs of deviation from straight power-law trends at large values of compute, data, or model
size. Our trends must eventually level off, though, since natural language has non-zero entropy.
Indeed, the trends for compute-efﬁcient training described in this section already contain an apparent contra-
diction. At scales several orders of magnitude above those documented here, the performance predicted by
the L(Cmin) scaling law decreases below what should be possible given the slow growth in training data with
compute. This implies that our scaling laws must break down before this point, but we conjecture that the
intersection point has a deeper meaning: it provides an estimate of the point at which Transformer language
models reach maximal performance.
Since the amount of data used by compute-efﬁcient training grows slowly with the compute budget, the
performance predicted by L(Cmin) eventually hits a lower bound set by the L(D) power law (see Figure 15).
Let us work this out in more detail.
To keep overﬁtting under control, the results of Section 4 imply that we should scale the dataset size as
D ∝N 0.74 ∝C0.54
min
(6.6)
where we have used the compute-efﬁcient N(Cmin) from Figure 14.
Let us compare this to the data requirements of compute-efﬁcient training. If we train at the critical batch
size (i.e. C = 2Cmin) and never re-use data during training, we ﬁnd that data usage grows with compute as
D(Cmin) =
2Cmin
6N(Cmin) ≈
 4 × 1010 tokens

(Cmin/PF-Day)0.26
(6.7)
This is the maximum rate at which the dataset size can productively grow with compute, since it means that
we are only training for a single epoch. But it grows the dataset much more slowly than in Equation (6.6).
It appears to imply that compute-efﬁcient training will eventually run into a problem with overﬁtting, even if
the training process never re-uses any data!
According to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by overﬁtting), the
loss should scale as L(D) ∝D−0.095. This implies that the loss would scale with compute as L(D(Cmin)) ∝
C−0.03
min
once we are data-limited. Once again, we have a contradiction, as this will eventually intersect with
our prediction for L(Cmin) from Figure 13, where we found a scaling L(Cmin) ∝C−0.050
min
.
The intersection point of L(D(Cmin)) and L(Cmin) occurs at
C∗∼104 PF-Days
N ∗∼1012 parameters,
D∗∼1012 tokens,
L∗∼1.7 nats/token
(6.8)
though the numerical values are highly uncertain, varying by an order or magnitude in either direction de-
pending on the precise values of the exponents from the power-law ﬁts. The most obvious interpretation is
that our scaling laws break down at or before we reach this point, which is still many orders of magnitude
away in both compute and model size.
17

354.176025390625 | 164.52804565429688 | 481.7458801269531 | 173.6463165283203 | The intersection point is sensitive to 
 |  | 
357.3067626953125 | 173.3984832763672 | 476.3061828613281 | 182.51675415039062 | the precise power-law parameters
 | 1 | 
89.9999771118164 | 215.13250732421875 | 522.0025634765625 | 257.9130554199219 | Figure 15
Far beyond the model sizes we study empirically, we ﬁnd a contradiction between our equations
for L(Cmin) and L(D) due to the slow growth of data needed for compute-efﬁcient training. The intersection
marks the point before which we expect our predictions to break down. The location of this point is highly
sensitive to the precise exponents from our power-law ﬁts.
 | 2 | 
89.9999771118164 | 280.219482421875 | 251.9818878173828 | 290.18206787109375 | 6.3
Contradictions and a Conjecture
 | 3 | 
89.9999771118164 | 300.2854309082031 | 521.9981689453125 | 321.15704345703125 | We observe no signs of deviation from straight power-law trends at large values of compute, data, or model
size. Our trends must eventually level off, though, since natural language has non-zero entropy.
 | 4 | 
89.9999771118164 | 327.58343505859375 | 522.00439453125 | 392.0910339355469 | Indeed, the trends for compute-efﬁcient training described in this section already contain an apparent contra-
diction. At scales several orders of magnitude above those documented here, the performance predicted by
the L(Cmin) scaling law decreases below what should be possible given the slow growth in training data with
compute. This implies that our scaling laws must break down before this point, but we conjecture that the
intersection point has a deeper meaning: it provides an estimate of the point at which Transformer language
models reach maximal performance.
 | 5 | 
89.99996948242188 | 398.5174255371094 | 521.9984741210938 | 430.29803466796875 | Since the amount of data used by compute-efﬁcient training grows slowly with the compute budget, the
performance predicted by L(Cmin) eventually hits a lower bound set by the L(D) power law (see Figure 15).
Let us work this out in more detail.
 | 6 | 
89.99996948242188 | 436.7234191894531 | 499.7119140625 | 446.68603515625 | To keep overﬁtting under control, the results of Section 4 imply that we should scale the dataset size as
 | 7 | 
265.2139892578125 | 449.13275146484375 | 522.0003662109375 | 465.1474914550781 | D ∝N 0.74 ∝C0.54
min
(6.6)
 | 8 | 
90.0 | 470.2568359375 | 362.7851867675781 | 481.0604553222656 | where we have used the compute-efﬁcient N(Cmin) from Figure 14.
 | 9 | 
90.00001525878906 | 486.8753662109375 | 521.9981689453125 | 508.35845947265625 | Let us compare this to the data requirements of compute-efﬁcient training. If we train at the critical batch
size (i.e. C = 2Cmin) and never re-use data during training, we ﬁnd that data usage grows with compute as
 | 10 | 
172.04800415039062 | 513.4878540039062 | 522.0003662109375 | 537.8655395507812 | D(Cmin) =
2Cmin
6N(Cmin) ≈
 
4 × 1010 tokens

(Cmin/PF-Day)0.26
(6.7)
 | 11 | 
90.0 | 543.5064697265625 | 521.9981689453125 | 586.1959838867188 | This is the maximum rate at which the dataset size can productively grow with compute, since it means that
we are only training for a single epoch. But it grows the dataset much more slowly than in Equation (6.6).
It appears to imply that compute-efﬁcient training will eventually run into a problem with overﬁtting, even if
the training process never re-uses any data!
 | 12 | 
89.99996185302734 | 592.6224365234375 | 522.0018920898438 | 640.3115234375 | According to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by overﬁtting), the
loss should scale as L(D) ∝D−0.095. This implies that the loss would scale with compute as L(D(Cmin)) ∝
C−0.03
min
once we are data-limited. Once again, we have a contradiction, as this will eventually intersect with
our prediction for L(Cmin) from Figure 13, where we found a scaling L(Cmin) ∝C−0.050
min
.
 | 13 | 
89.99993896484375 | 644.5379028320312 | 335.2349548339844 | 655.341552734375 | The intersection point of L(D(Cmin)) and L(Cmin) occurs at
 | 14 | 
113.9879150390625 | 658.4909057617188 | 522.0028076171875 | 672.7980346679688 | C∗∼104 PF-Days
N ∗∼1012 parameters,
D∗∼1012 tokens,
L∗∼1.7 nats/token
(6.8)
 | 15 | 
89.99993896484375 | 679.7174682617188 | 521.998046875 | 722.4070434570312 | though the numerical values are highly uncertain, varying by an order or magnitude in either direction de-
pending on the precise values of the exponents from the power-law ﬁts. The most obvious interpretation is
that our scaling laws break down at or before we reach this point, which is still many orders of magnitude
away in both compute and model size.
 | 16 | 
301.0189208984375 | 742.3324584960938 | 310.98150634765625 | 752.2950439453125 | 17
 | 17 | 
One might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model
size beyond N ∗without qualitatively different data requirements, perhaps this means that once we reach
C∗
min and N ∗, we have extracted all of the reliable information available in natural language data. In this
interpretation, L∗would provide a rough estimate for the entropy-per-token7 of natural language. In this
scenario, we would expect the loss trend to level off at or before L∗.
We can guess at the functional form of L(Cmin) as it levels off by considering a version of our training
dataset with added noise. For example, we could append a random string of tokens to each context shown
to the model to artiﬁcially boost the loss by a constant additive factor. Then, the distance from the noise
ﬂoor L −Lnoise would be a more meaningful performance metric, with even a small decrease in this distance
potentially representing a signiﬁcant boost in qualitative performance. Since the artiﬁcial noise would affect
all of our trends equally, the critical point of 6.8 would not change (aside from the absolute value of L∗), and
may be meaningful even if it occurs after the leveling off.
7
Related Work
Power laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset
size in density estimation [Was06] and in random forest models [Bia12] may be connected with our results.
These models suggest that power-law exponents may have a very rough interpretation as the inverse of the
number of relevant features in the data.
Some early [BB01, Goo01] work found power-law scalings between performance and dataset size. More
recent work [HNA+17, HAD19] also investigated scaling between model size and data size; their work is
perhaps the closest to ours in the literature8. Note, however, that [HNA+17] found super-linear scaling of
dataset size with model size, whereas we ﬁnd a sub-linear scaling. There are some parallels between our
ﬁndings on optimal allocation of compute and [Kom19], including power-law learning curves. EfﬁcientNets
[TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent
work [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and ﬁts an
ansatz similar to ours.
EfﬁcientNet [TL19] advocates scaling depth and width exponentially (with different coefﬁcients) for optimal
performance of image models, resulting in a power-law scaling of width as a function of depth. We ﬁnd that
for language models this power should be roughly one when scaling up (as width/depth should remain ﬁxed).
But more importantly, we ﬁnd that the precise architectural hyperparameters are unimportant compared to the
overall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles
of shallower models, which could potentially explain this ﬁnding. Earlier work [ZK16] has compared width
and depth, and found that wide ResNets can outperform deep ResNets on image classiﬁcation. Some studies
ﬁx computation per data example, which tends to scale in proportion to the number of model parameters,
whereas we investigate scaling with both model size and the quantity of training computation.
Various works [AS17, BHMM18] have investigated generalization in highly overparameterized models, ﬁnd-
ing a “jamming transition” [GJS+19] when the model size reaches the dataset size (this may require training
many orders of magnitude beyond typical practice, and in particular does not use early stopping). We do
not observe such a transition, and ﬁnd that the necessary training data scales sublinearly in the model size.
Expansions in the model size, particularly at large width [JGH18, LXS+19], may provide a useful framework
for thinking about some of our scaling relations. Our results on optimization, such as the shape of learning
curves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions
[ZLN+19] in realistic settings. Making this connection quantitative will require a characterization of the
Hessian spectrum [Pap18, GKX19, GARD18].
8
Discussion
We have observed consistent scalings of language model log-likelihood loss with non-embedding parameter
count N, dataset size D, and optimized training computation Cmin, as encapsulated in Equations (1.5) and
(1.6). Conversely, we ﬁnd very weak dependence on many architectural and optimization hyperparameters.
Since scalings with N, D, Cmin are power-laws, there are diminishing returns with increasing scale.
7Deﬁning words using the wc utility, the WebText2 dataset has 1.4 tokens per word and 4.3 characters per token.
8After this work was completed, [RRBS19a] also appeared, which makes similar predictions for the dependence of
loss on both model and dataset size.
18

90.0 | 74.40748596191406 | 522.0042724609375 | 128.00601196289062 | One might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model
size beyond N ∗without qualitatively different data requirements, perhaps this means that once we reach
C∗
min and N ∗, we have extracted all of the reliable information available in natural language data. In this
interpretation, L∗would provide a rough estimate for the entropy-per-token7 of natural language. In this
scenario, we would expect the loss trend to level off at or before L∗.
 |  | 
89.99996948242188 | 134.2018280029297 | 522.0035400390625 | 209.84896850585938 | We can guess at the functional form of L(Cmin) as it levels off by considering a version of our training
dataset with added noise. For example, we could append a random string of tokens to each context shown
to the model to artiﬁcially boost the loss by a constant additive factor. Then, the distance from the noise
ﬂoor L −Lnoise would be a more meaningful performance metric, with even a small decrease in this distance
potentially representing a signiﬁcant boost in qualitative performance. Since the artiﬁcial noise would affect
all of our trends equally, the critical point of 6.8 would not change (aside from the absolute value of L∗), and
may be meaningful even if it occurs after the leveling off.
 | 1 | 
90.0 | 227.5390625 | 179.09014892578125 | 239.4942626953125 | 7
Related Work
 | 2 | 
90.0 | 252.8003692626953 | 521.9982299804688 | 295.4899597167969 | Power laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset
size in density estimation [Was06] and in random forest models [Bia12] may be connected with our results.
These models suggest that power-law exponents may have a very rough interpretation as the inverse of the
number of relevant features in the data.
 | 3 | 
90.0 | 301.9163513183594 | 522.0048217773438 | 388.2429504394531 | Some early [BB01, Goo01] work found power-law scalings between performance and dataset size. More
recent work [HNA+17, HAD19] also investigated scaling between model size and data size; their work is
perhaps the closest to ours in the literature8. Note, however, that [HNA+17] found super-linear scaling of
dataset size with model size, whereas we ﬁnd a sub-linear scaling. There are some parallels between our
ﬁndings on optimal allocation of compute and [Kom19], including power-law learning curves. EfﬁcientNets
[TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent
work [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and ﬁts an
ansatz similar to ours.
 | 4 | 
90.0 | 394.6683349609375 | 521.9982299804688 | 491.9039306640625 | EfﬁcientNet [TL19] advocates scaling depth and width exponentially (with different coefﬁcients) for optimal
performance of image models, resulting in a power-law scaling of width as a function of depth. We ﬁnd that
for language models this power should be roughly one when scaling up (as width/depth should remain ﬁxed).
But more importantly, we ﬁnd that the precise architectural hyperparameters are unimportant compared to the
overall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles
of shallower models, which could potentially explain this ﬁnding. Earlier work [ZK16] has compared width
and depth, and found that wide ResNets can outperform deep ResNets on image classiﬁcation. Some studies
ﬁx computation per data example, which tends to scale in proportion to the number of model parameters,
whereas we investigate scaling with both model size and the quantity of training computation.
 | 5 | 
90.0 | 498.3293151855469 | 522.0048828125 | 595.5648803710938 | Various works [AS17, BHMM18] have investigated generalization in highly overparameterized models, ﬁnd-
ing a “jamming transition” [GJS+19] when the model size reaches the dataset size (this may require training
many orders of magnitude beyond typical practice, and in particular does not use early stopping). We do
not observe such a transition, and ﬁnd that the necessary training data scales sublinearly in the model size.
Expansions in the model size, particularly at large width [JGH18, LXS+19], may provide a useful framework
for thinking about some of our scaling relations. Our results on optimization, such as the shape of learning
curves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions
[ZLN+19] in realistic settings. Making this connection quantitative will require a characterization of the
Hessian spectrum [Pap18, GKX19, GARD18].
 | 6 | 
90.0 | 613.2550048828125 | 161.7431640625 | 625.210205078125 | 8
Discussion
 | 7 | 
90.0 | 638.5162963867188 | 521.9981079101562 | 682.7008666992188 | We have observed consistent scalings of language model log-likelihood loss with non-embedding parameter
count N, dataset size D, and optimized training computation Cmin, as encapsulated in Equations (1.5) and
(1.6). Conversely, we ﬁnd very weak dependence on many architectural and optimization hyperparameters.
Since scalings with N, D, Cmin are power-laws, there are diminishing returns with increasing scale.
 | 8 | 
89.99998474121094 | 690.8446655273438 | 522.00146484375 | 722.1663818359375 | 7Deﬁning words using the wc utility, the WebText2 dataset has 1.4 tokens per word and 4.3 characters per token.
8After this work was completed, [RRBS19a] also appeared, which makes similar predictions for the dependence of
loss on both model and dataset size.
 | 9 | 
301.01898193359375 | 742.3324584960938 | 310.9815673828125 | 752.2950439453125 | 18
 | 10 | 
We were able to precisely model the dependence of the loss on N and D, and alternatively on N and S, when
these parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude
of overﬁtting, early stopping step, and data requirements when training large language models. So our scaling
relations go beyond mere observation to provide a predictive framework. One might interpret these relations
as analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way,
independent of most of the details of its microscopic consituents.
It is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a
maximum likelihood loss, and perhaps in other settings as well. To this purpose, it will be interesting to
test these relations on other domains, such as images, audio, and video models, and perhaps also for random
network distillation. At this point we do not know which of our results depend on the structure of natural
language data, and which are universal. It would also be exciting to ﬁnd a theoretical framework from
which the scaling relations can be derived: a ‘statistical mechanics’ underlying the ‘thermodynamics’ we
have observed. Such a theory might make it possible to derive other more precise predictions, and provide a
systematic understanding of the limitations of the scaling laws.
In the domain of natural language, it will be important to investigate whether continued improvement on the
loss translates into improvement on relevant language tasks. Smooth quantitative change can mask major
qualitative improvements: “more is different”. For example, the smooth aggregate growth of the economy
provides no indication of the speciﬁc technological developments that underwrite it. Similarly, the smooth
improvements in language model loss may hide seemingly qualitative changes in capability.
Our results strongly suggest that larger models will continue to perform better, and will also be much more
sample efﬁcient than has been previously appreciated. Big models may be more important than big data.
In this context, further investigation into model parallelism is warranted. Deep models can be trained using
pipelining [HCC+18], which splits parameters depth-wise between devices, but eventually requires increased
batch sizes as more devices are used. Wide networks on the other hand are more amenable to parallelization
[SCP+18], since large layers can be split between multiple workers with less serial dependency. Sparsity
[CGRS19, GRK17] or branching (e.g. [KSH12]) may allow for even faster training of large networks through
increased model parallelism. And using methods like [WRH17, WYL19], which grow networks as they train,
it might be possible to remain on the compute-efﬁcient frontier for an entire training run.
Acknowledgements
We would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner,
Danny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feed-
back on drafts of this work.
19

90.0 | 74.17692565917969 | 521.998291015625 | 138.91506958007812 | We were able to precisely model the dependence of the loss on N and D, and alternatively on N and S, when
these parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude
of overﬁtting, early stopping step, and data requirements when training large language models. So our scaling
relations go beyond mere observation to provide a predictive framework. One might interpret these relations
as analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way,
independent of most of the details of its microscopic consituents.
 |  | 
90.00003051757812 | 145.34144592285156 | 521.9982299804688 | 231.66702270507812 | It is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a
maximum likelihood loss, and perhaps in other settings as well. To this purpose, it will be interesting to
test these relations on other domains, such as images, audio, and video models, and perhaps also for random
network distillation. At this point we do not know which of our results depend on the structure of natural
language data, and which are universal. It would also be exciting to ﬁnd a theoretical framework from
which the scaling relations can be derived: a ‘statistical mechanics’ underlying the ‘thermodynamics’ we
have observed. Such a theory might make it possible to derive other more precise predictions, and provide a
systematic understanding of the limitations of the scaling laws.
 | 1 | 
90.00003051757812 | 238.09339904785156 | 521.998291015625 | 291.6919860839844 | In the domain of natural language, it will be important to investigate whether continued improvement on the
loss translates into improvement on relevant language tasks. Smooth quantitative change can mask major
qualitative improvements: “more is different”. For example, the smooth aggregate growth of the economy
provides no indication of the speciﬁc technological developments that underwrite it. Similarly, the smooth
improvements in language model loss may hide seemingly qualitative changes in capability.
 | 2 | 
90.00003051757812 | 298.1183776855469 | 521.9992065429688 | 395.35296630859375 | Our results strongly suggest that larger models will continue to perform better, and will also be much more
sample efﬁcient than has been previously appreciated. Big models may be more important than big data.
In this context, further investigation into model parallelism is warranted. Deep models can be trained using
pipelining [HCC+18], which splits parameters depth-wise between devices, but eventually requires increased
batch sizes as more devices are used. Wide networks on the other hand are more amenable to parallelization
[SCP+18], since large layers can be split between multiple workers with less serial dependency. Sparsity
[CGRS19, GRK17] or branching (e.g. [KSH12]) may allow for even faster training of large networks through
increased model parallelism. And using methods like [WRH17, WYL19], which grow networks as they train,
it might be possible to remain on the compute-efﬁcient frontier for an entire training run.
 | 3 | 
90.00003051757812 | 412.1640625 | 188.8336639404297 | 424.1192626953125 | Acknowledgements
 | 4 | 
90.00003051757812 | 436.8973693847656 | 521.998291015625 | 468.677978515625 | We would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner,
Danny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feed-
back on drafts of this work.
 | 5 | 
301.01904296875 | 742.3323974609375 | 310.98162841796875 | 752.2949829101562 | 19
 | 6 | 
Appendices
A
Summary of Power Laws
For easier reference, we provide a summary below of the key trends described throughout the paper.
Parameters
Data
Compute
Batch Size
Equation
N
∞
∞
Fixed
L (N) = (Nc/N)αN
∞
D
Early Stop
Fixed
L (D) = (Dc/D)αD
Optimal
∞
C
Fixed
L (C) = (Cc/C)αC (naive)
Nopt
Dopt
Cmin
B ≪Bcrit
L (Cmin) =
 Cmin
c
/Cmin
αmin
C
N
D
Early Stop
Fixed
L (N, D) =
  Nc
N
 αN
αD + Dc
D
αD
N
∞
S steps
B
L (N, S) =
  Nc
N
αN +

Sc
Smin(S,B)
αS
Table 4
The empirical ﬁtted values for these trends are:
Power Law
Scale (tokenization-dependent)
αN = 0.076
Nc = 8.8 × 1013 params (non-embed)
αD = 0.095
Dc = 5.4 × 1013 tokens
αC = 0.057
Cc = 1.6 × 107 PF-days
αmin
C
= 0.050
Cmin
c
= 3.1 × 108 PF-days
αB = 0.21
B∗= 2.1 × 108 tokens
αS = 0.76
Sc = 2.1 × 103 steps
Table 5
The optimal parameters for compute efﬁcient training are given by:
Compute-Efﬁcient Value
Power Law
Scale
Nopt = Ne · CpN
min
pN = 0.73
Ne = 1.3 · 109 params
B ≪Bcrit =
B∗
L1/αB = BeCpB
min
pB = 0.24
Be = 2.0 · 106 tokens
Smin = Se · CpS
min (lower bound)
pS = 0.03
Se = 5.4 · 103 steps
Dopt = De · CpD
min (1 epoch)
pD = 0.27
De = 2 · 1010 tokens
Table 6
B
Empirical Model of Compute-Efﬁcient Frontier
Throughout this appendix all values of C, S, and αC are adjusted for training at the critical batch size Bcrit.
We have left off the ‘adj’ label to avoid cluttering the notation.
B.1
Deﬁning Equations
The power-law ﬁt to the learning curves implies a simple prescription for compute-efﬁcient training. In this
appendix, we will derive the optimal performance, model size, and number of training steps as a function of
20

90.0 | 70.37715148925781 | 191.65951538085938 | 91.0396499633789 | Appendices
 |  | 
90.0 | 109.93718719482422 | 239.3921661376953 | 121.89238739013672 | A
Summary of Power Laws
 | 1 | 
90.0 | 135.35548400878906 | 488.43414306640625 | 145.31808471679688 | For easier reference, we provide a summary below of the key trends described throughout the paper.
 | 2 | 
123.73200225830078 | 157.26348876953125 | 368.9604187011719 | 167.22608947753906 | Parameters
Data
Compute
Batch Size
Equation
 | 3 | 
143.75399780273438 | 173.60377502441406 | 412.06292724609375 | 187.08056640625 | N
∞
∞
Fixed
L (N) = (Nc/N)αN
 | 4 | 
143.3179931640625 | 190.3657989501953 | 411.8982238769531 | 203.84259033203125 | ∞
D
Early Stop
Fixed
L (D) = (Dc/D)αD
 | 5 | 
131.9709930419922 | 206.0244903564453 | 440.4595642089844 | 220.60455322265625 | Optimal
∞
C
Fixed
L (C) = (Cc/C)αC (naive)
 | 6 | 
138.30499267578125 | 222.38796997070312 | 452.7919006347656 | 239.278564453125 | Nopt
Dopt
Cmin
B ≪Bcrit
L (Cmin) =
 
Cmin
c
/Cmin
αmin
C
 | 7 | 
143.75399780273438 | 240.9153594970703 | 400.98455810546875 | 257.9930725097656 | N
D
Early Stop
Fixed
L (N, D) =
  Nc
 | 8 | 
393.02801513671875 | 241.36593627929688 | 418.34991455078125 | 260.5445556640625 | N
 αN
 | 9 | 
408.5409851074219 | 245.5009002685547 | 444.20184326171875 | 257.76251220703125 | αD + Dc
 | 10 | 
436.2149963378906 | 253.5707550048828 | 442.728515625 | 260.5445556640625 | D
 | 11 | 
445.89801025390625 | 238.1527862548828 | 461.865234375 | 250.87796020507812 | αD
 | 12 | 
143.75399780273438 | 263.92572021484375 | 393.88458251953125 | 279.8530578613281 | N
∞
S steps
B
L (N, S) =
  Nc
 | 13 | 
385.9280090332031 | 263.00079345703125 | 484.5280456542969 | 282.96527099609375 | N
αN +

Sc
Smin(S,B)
αS
 | 14 | 
290.5429992675781 | 289.29052734375 | 321.4569396972656 | 299.25311279296875 | Table 4
 | 15 | 
90.0 | 311.3884582519531 | 277.61572265625 | 321.35107421875 | The empirical ﬁtted values for these trends are:
 | 16 | 
193.61099243164062 | 333.2955017089844 | 393.39691162109375 | 343.2580871582031 | Power Law
Scale (tokenization-dependent)
 | 17 | 
193.61099243164062 | 348.9244689941406 | 415.8984680175781 | 363.7655029296875 | αN = 0.076
Nc = 8.8 × 1013 params (non-embed)
 | 18 | 
193.61099243164062 | 365.68646240234375 | 359.334716796875 | 380.5274963378906 | αD = 0.095
Dc = 5.4 × 1013 tokens
 | 19 | 
193.61099243164062 | 382.4484558105469 | 360.98040771484375 | 397.2904968261719 | αC = 0.057
Cc = 1.6 × 107 PF-days
 | 20 | 
193.61099243164062 | 399.2104797363281 | 371.3514404296875 | 414.6485900878906 | αmin
C
= 0.050
Cmin
c
= 3.1 × 108 PF-days
 | 21 | 
193.61099243164062 | 415.97247314453125 | 355.2117004394531 | 430.81451416015625 | αB = 0.21
B∗= 2.1 × 108 tokens
 | 22 | 
193.61099243164062 | 432.7344665527344 | 347.23724365234375 | 447.5765075683594 | αS = 0.76
Sc = 2.1 × 103 steps
 | 23 | 
290.5429992675781 | 454.18450927734375 | 321.4569396972656 | 464.1470947265625 | Table 5
 | 24 | 
90.0 | 476.2814636230469 | 358.0138244628906 | 486.24407958984375 | The optimal parameters for compute efﬁcient training are given by:
 | 25 | 
157.5229949951172 | 498.1885070800781 | 382.9378967285156 | 508.1510925292969 | Compute-Efﬁcient Value
Power Law
Scale
 | 26 | 
157.5229949951172 | 513.8175048828125 | 451.98699951171875 | 529.3646240234375 | Nopt = Ne · CpN
min
pN = 0.73
Ne = 1.3 · 109 params
 | 27 | 
157.5229949951172 | 530.5794677734375 | 448.6796875 | 547.5555419921875 | B ≪Bcrit =
B∗
L1/αB = BeCpB
min
pB = 0.24
Be = 2.0 · 106 tokens
 | 28 | 
157.5229949951172 | 547.3414306640625 | 441.24420166015625 | 563.7730102539062 | Smin = Se · CpS
min (lower bound)
pS = 0.03
Se = 5.4 · 103 steps
 | 29 | 
157.5229949951172 | 564.1034545898438 | 445.59368896484375 | 580.5350341796875 | Dopt = De · CpD
min (1 epoch)
pD = 0.27
De = 2 · 1010 tokens
 | 30 | 
290.5429992675781 | 585.5535278320312 | 321.4569396972656 | 595.51611328125 | Table 6
 | 31 | 
90.0 | 619.1761474609375 | 352.7752990722656 | 631.13134765625 | B
Empirical Model of Compute-Efﬁcient Frontier
 | 32 | 
89.99996948242188 | 644.3639526367188 | 521.9996337890625 | 665.4660034179688 | Throughout this appendix all values of C, S, and αC are adjusted for training at the critical batch size Bcrit.
We have left off the ‘adj’ label to avoid cluttering the notation.
 | 33 | 
89.99996948242188 | 681.0125122070312 | 195.7429656982422 | 690.97509765625 | B.1
Deﬁning Equations
 | 34 | 
89.99996948242188 | 701.5354614257812 | 521.9982299804688 | 722.4070434570312 | The power-law ﬁt to the learning curves implies a simple prescription for compute-efﬁcient training. In this
appendix, we will derive the optimal performance, model size, and number of training steps as a function of
 | 35 | 
301.01898193359375 | 742.3324584960938 | 310.9815673828125 | 752.2950439453125 | 20
 | 36 | 
the compute budget. We start with the Equation (1.6), repeated here for convenience:
L (N, S) =
Nc
N
αN
+
Sc
S
αS
.
(B.1)
Here, S represents the number of parameter updates when training at the critical batch size [MKAT18],
which was deﬁned in Equation (5.2)9:
B (L) =
B∗
L1/αB .
(B.2)
We would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =
C/ (6NB (L)), where C is the number of FLOPs used in the training run:
L (N, C) =
Nc
N
αN
+

6B∗Sc
N
L1/αBC
αS
.
(B.3)
Now, we set ∂NL

C = 0 to ﬁnd the condition for optimality:
0 = ∂L
∂N

C
= −αN
N
Nc
N
αN
+ αS
N

6B∗Sc
N
L1/αBC
αS 
1 −5N
L
∂L
∂N

C

=⇒αN
αS
Nc
N
αN
=

6B∗Sc
N
L1/αBC
αS
(B.4)
Equation (B.3) and (B.4) together determine the compute-efﬁcient frontier.
B.2
Efﬁcient Training
Now we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields
L (Neﬀ(C) , C) =

1 + αN
αS

L (Neﬀ, ∞) ,
(B.5)
which implies that for compute-efﬁcient training, we should train to a ﬁxed percentage αN
αS ≈10% above
the converged loss. Next, let’s determine how the optimal loss depends on the compute budget. Eliminating
N yields a power-law dependence of performance on compute:
L (C) =
Cc
C
αC
(B.6)
where we deﬁned
αC = 1/ (1/αS + 1/αB + 1/αN) ≈0.052
(B.7)
Cc = 6NcB∗Sc

1 + αN
αS
1/αS+1/αN  αS
αN
1/αS
.
(B.8)
Similarly, we can eliminate L to ﬁnd N (C):
N (C)
Nc
=
 C
Cc
αC/αN 
1 + αN
αS
1/αN
(B.9)
and
S (C) =
Cc
6NcB∗

1 + αN
αS
−1/αN  C
Cc
αC/αS
(B.10)
9There is a slight ambiguity here: we can imagine training either at a constant batch size B (Ltarget), or we could
instead train at a variable batch size ˜B (L), where ˜B is the instantaneous critical batch size (as opposed to B, which is
the averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see
[MKAT18]).
21

90.0 | 74.40748596191406 | 428.5290832519531 | 84.37008666992188 | the compute budget. We start with the Equation (1.6), repeated here for convenience:
 |  | 
233.5780029296875 | 92.98237609863281 | 302.7966003417969 | 109.82855224609375 | L (N, S) =
Nc
 | 1 | 
292.7149963378906 | 106.69993591308594 | 300.72491455078125 | 116.66253662109375 | N
 | 2 | 
304.48699951171875 | 90.21882629394531 | 354.276611328125 | 109.82861328125 | αN
+
Sc
 | 3 | 
346.34600830078125 | 106.69993591308594 | 352.4530944824219 | 116.66253662109375 | S
 | 4 | 
355.9670104980469 | 90.21882629394531 | 522.0001220703125 | 110.05917358398438 | αS
.
(B.1)
 | 5 | 
90.00003051757812 | 124.74302673339844 | 522.0004272460938 | 145.84518432617188 | Here, S represents the number of parameter updates when training at the critical batch size [MKAT18],
which was deﬁned in Equation (5.2)9:
 | 6 | 
271.4530334472656 | 146.47801208496094 | 522.0001220703125 | 170.19354248046875 | B (L) =
B∗
L1/αB .
(B.2)
 | 7 | 
89.99996948242188 | 172.8859100341797 | 522.0018920898438 | 193.98806762695312 | We would like to determine optimal training parameters for a ﬁxed compute budget, so we replace S =
C/ (6NB (L)), where C is the number of FLOPs used in the training run:
 | 8 | 
207.32395935058594 | 202.93434143066406 | 277.6925354003906 | 219.780517578125 | L (N, C) =
Nc
 | 9 | 
267.6109924316406 | 216.6519012451172 | 275.62091064453125 | 226.614501953125 | N
 | 10 | 
279.38299560546875 | 200.17079162597656 | 380.31524658203125 | 226.79351806640625 | αN
+

6B∗Sc
N
L1/αBC
 | 11 | 
382.2209777832031 | 200.17079162597656 | 522.0001220703125 | 220.01113891601562 | αS
.
(B.3)
 | 12 | 
90.0 | 234.99342346191406 | 332.39202880859375 | 250.93405151367188 | Now, we set ∂NL

C = 0 to ﬁnd the condition for optimality:
 | 13 | 
205.20701599121094 | 255.7500457763672 | 238.45460510253906 | 272.45166015625 | 0 = ∂L
 | 14 | 
224.66700744628906 | 269.32293701171875 | 238.52499389648438 | 279.2855224609375 | ∂N
 | 15 | 
240.79600524902344 | 261.1843566894531 | 249.82855224609375 | 277.1249694824219 | 
C
 | 16 | 
212.9550018310547 | 281.553955078125 | 245.10031127929688 | 298.25653076171875 | = −αN
 | 17 | 
234.8350067138672 | 295.1279296875 | 242.84494018554688 | 305.09051513671875 | N
 | 18 | 
249.2010040283203 | 281.40936279296875 | 269.2986145019531 | 292.3575744628906 | Nc
 | 19 | 
259.21600341796875 | 295.1279296875 | 267.2259216308594 | 305.09051513671875 | N
 | 20 | 
270.989013671875 | 278.6467590332031 | 315.04058837890625 | 298.25653076171875 | αN
+ αS
 | 21 | 
305.2980041503906 | 295.1279296875 | 313.30792236328125 | 305.09051513671875 | N
 | 22 | 
318.7720031738281 | 281.40936279296875 | 388.1172790527344 | 305.2685546875 | 
6B∗Sc
N
L1/αBC
 | 23 | 
390.02301025390625 | 278.64678955078125 | 448.3879699707031 | 298.256591796875 | αS 
1 −5N
 | 24 | 
441.53399658203125 | 279.1784362792969 | 479.0075988769531 | 305.09051513671875 | L
∂L
∂N
 | 25 | 
467.9889831542969 | 286.9893493652344 | 477.02154541015625 | 302.9289245605469 | 
C
 | 26 | 
478.010986328125 | 281.40936279296875 | 485.3434753417969 | 291.3719482421875 | 
 | 27 | 
129.4229736328125 | 309.98895263671875 | 164.88827514648438 | 326.6915283203125 | =⇒αN
 | 28 | 
153.0959930419922 | 323.56292724609375 | 164.36460876464844 | 334.3665466308594 | αS
 | 29 | 
168.98899841308594 | 309.8453369140625 | 189.0866241455078 | 320.7935485839844 | Nc
 | 30 | 
179.00399780273438 | 323.56292724609375 | 187.01393127441406 | 333.5255126953125 | N
 | 31 | 
190.77699279785156 | 307.08172607421875 | 292.8162841796875 | 333.7045593261719 | αN
=

6B∗Sc
N
L1/αBC
 | 32 | 
294.7220153808594 | 307.081787109375 | 522.0001220703125 | 326.922119140625 | αS
(B.4)
 | 33 | 
90.00003051757812 | 341.72650146484375 | 388.3001403808594 | 351.6891174316406 | Equation (B.3) and (B.4) together determine the compute-efﬁcient frontier.
 | 34 | 
90.00003051757812 | 366.7425537109375 | 189.4566650390625 | 376.70513916015625 | B.2
Efﬁcient Training
 | 35 | 
90.00003051757812 | 387.0684814453125 | 494.3619689941406 | 397.0310974121094 | Now we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields
 | 36 | 
215.94802856445312 | 405.1043701171875 | 332.2823181152344 | 422.7925720214844 | L (Neﬀ(C) , C) =

1 + αN
 | 37 | 
320.4909973144531 | 418.8219299316406 | 331.75958251953125 | 429.62554931640625 | αS
 | 38 | 
334.7229919433594 | 405.1043395996094 | 522.0001220703125 | 422.79254150390625 | 
L (Neﬀ, ∞) ,
(B.5)
 | 39 | 
90.0 | 435.7177429199219 | 459.8858947753906 | 447.6650695800781 | which implies that for compute-efﬁcient training, we should train to a ﬁxed percentage αN
 | 40 | 
90.0 | 437.34283447265625 | 521.9998779296875 | 470.7620849609375 | αS ≈10% above
the converged loss. Next, let’s determine how the optimal loss depends on the compute budget. Eliminating
N yields a power-law dependence of performance on compute:
 | 41 | 
267.489990234375 | 479.3753662109375 | 324.00958251953125 | 496.2215270996094 | L (C) =
Cc
 | 42 | 
314.99798583984375 | 493.0929260253906 | 322.1212463378906 | 503.0555114746094 | C
 | 43 | 
325.6999816894531 | 476.6117248535156 | 522.0001220703125 | 496.4520568847656 | αC
(B.6)
 | 44 | 
90.0 | 511.2564697265625 | 160.26620483398438 | 521.2190551757812 | where we deﬁned
 | 45 | 
198.08499145507812 | 529.3567504882812 | 522.0001220703125 | 540.9425048828125 | αC = 1/ (1/αS + 1/αB + 1/αN) ≈0.052
(B.7)
 | 46 | 
199.97998046875 | 553.9569091796875 | 263.2966003417969 | 565.4135131835938 | Cc = 6NcB∗Sc
 | 47 | 
265.4519958496094 | 547.0733032226562 | 303.8232727050781 | 563.9194946289062 | 
1 + αN
 | 48 | 
292.031005859375 | 560.7909545898438 | 303.2995910644531 | 571.5945434570312 | αS
 | 49 | 
306.26397705078125 | 539.909912109375 | 380.4425048828125 | 560.1019287109375 | 1/αS+1/αN  αS
 | 50 | 
368.2820129394531 | 560.7909545898438 | 380.9662780761719 | 571.5945434570312 | αN
 | 51 | 
383.4070129394531 | 544.3097534179688 | 522.0001220703125 | 564.1500244140625 | 1/αS
.
(B.8)
 | 52 | 
90.0 | 579.44091796875 | 268.028564453125 | 589.634033203125 | Similarly, we can eliminate L to ﬁnd N (C):
 | 53 | 
223.53598022460938 | 600.9529418945312 | 249.87042236328125 | 610.91552734375 | N (C)
 | 54 | 
230.67100524902344 | 593.6449584960938 | 281.6742858886719 | 625.33056640625 | Nc
=
 C
 | 55 | 
272.87799072265625 | 614.5259399414062 | 283.56158447265625 | 625.33056640625 | Cc
 | 56 | 
285.2519836425781 | 598.0447387695312 | 359.7542724609375 | 617.654541015625 | αC/αN 
1 + αN
 | 57 | 
347.9630126953125 | 614.5259399414062 | 359.2315979003906 | 625.33056640625 | αS
 | 58 | 
362.19500732421875 | 598.0447387695312 | 522.0001220703125 | 617.8850708007812 | 1/αN
(B.9)
 | 59 | 
90.00003051757812 | 632.689453125 | 104.38602447509766 | 642.6520385742188 | and
 | 60 | 
205.7020263671875 | 651.5789184570312 | 272.7876892089844 | 675.95654296875 | S (C) =
Cc
6NcB∗
 | 61 | 
276.1440124511719 | 651.435302734375 | 314.5152893066406 | 668.281494140625 | 
1 + αN
 | 62 | 
302.7239990234375 | 665.1529541015625 | 313.9925842285156 | 675.95654296875 | αS
 | 63 | 
316.95599365234375 | 644.2719116210938 | 369.63226318359375 | 664.4639892578125 | −1/αN  C
 | 64 | 
360.83599853515625 | 665.1529541015625 | 371.5205993652344 | 675.95654296875 | Cc
 | 65 | 
373.21099853515625 | 648.6717529296875 | 522.0004272460938 | 668.5120239257812 | αC/αS
(B.10)
 | 66 | 
90.00003051757812 | 680.2286987304688 | 522.004638671875 | 722.1663818359375 | 9There is a slight ambiguity here: we can imagine training either at a constant batch size B (Ltarget), or we could
instead train at a variable batch size ˜B (L), where ˜B is the instantaneous critical batch size (as opposed to B, which is
the averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see
[MKAT18]).
 | 67 | 
301.01904296875 | 742.3324584960938 | 310.98162841796875 | 752.2950439453125 | 21
 | 68 | 
B.3
Comparison to Inefﬁcient
Typically, researchers train models until they appear to be close to convergence. In this section, we compare
the efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor
f as the percent deviation from the converged loss:
L (N, C) = (1 + f) L (N, ∞) .
(B.11)
For compute-efﬁcient training we have f = αN/αS ≈10% from the previous section, but researchers
typically use a much smaller value. Here, we choose f ′ = 2% as an estimate. For a ﬁxed value of the loss,
we predict:
Nf
Nf ′ =
 1 + f
1 + f ′
1/αN
≈2.7
(B.12)
Sf
Sf ′ =
 
1 + 1
f
1 + 1
f ′
!1/αS
≈0.13
(B.13)
Cf
Cf ′ = Nf
Nf ′
Sf
Sf ′ ≈0.35
(B.14)
So that compute-efﬁcient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less
compute to reach the same loss.
B.4
Suboptimal Model Sizes
We can solve A.1 to ﬁnd an expression for the amount of compute needed to reach a given value of the loss
L with a model of size N:
C (N, L) =

6B∗Sc
N
L1/αB
 
L −
Nc
N
αN −1/αS
.
(B.15)
Using A.6 and A.9, we can eliminate L in favor of Neﬀ(L), the model size which reaches L most efﬁciently.
From there, we ﬁnd an expression for the excess compute needed as a consequence of using a suboptimal
model size:
C (N, Neﬀ)
C (Neﬀ, Neﬀ) =
N
Neﬀ

1 + αS
αN

1 −
Neﬀ
N
αN −1/αS
.
(B.16)
The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a
20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A
larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism
and faster training if sufﬁcient harware is available (see Figure Y):
S (N, Neﬀ)
S (Neﬀ, Neﬀ) =

1 + αS
αN

1 −
Neﬀ
N
αN −1/αS
.
(B.17)
A 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation
should not be trusted for very large models, as it is only valid in the power-law region of the learning curve
after initial transient effects.
C
Caveats
In this section we list some potential caveats to our analysis.
• At present we do not have a solid theoretical understanding for any of our proposed scaling laws.
The scaling relations with model size and compute are especially mysterious. It may be possible to
understand scaling at very large D holding model size ﬁxed [AS17], and also the shape of learning
curves late in training, by modeling the loss with a noisy quadratic. But the scaling with D at very
large model size still remains mysterious. Without a theory or a systematic understanding of the
corrections to our scaling laws, it’s difﬁcult to determine in what circumstances they can be trusted.
22

90.0 | 74.31652069091797 | 223.1103057861328 | 84.27912139892578 | B.3
Comparison to Inefﬁcient
 |  | 
90.0 | 94.67646789550781 | 521.9982299804688 | 126.45706176757812 | Typically, researchers train models until they appear to be close to convergence. In this section, we compare
the efﬁcient training procedure described above to this more typical setup. We deﬁne a the convergence factor
f as the percent deviation from the converged loss:
 | 1 | 
242.83401489257812 | 134.6697540283203 | 522.00048828125 | 144.99203491210938 | L (N, C) = (1 + f) L (N, ∞) .
(B.11)
 | 2 | 
90.0 | 153.2057647705078 | 522.0029296875 | 185.34603881835938 | For compute-efﬁcient training we have f = αN/αS ≈10% from the previous section, but researchers
typically use a much smaller value. Here, we choose f ′ = 2% as an estimate. For a ﬁxed value of the loss,
we predict:
 | 3 | 
244.01800537109375 | 189.0989227294922 | 306.3266906738281 | 220.7845458984375 | Nf
Nf ′ =
 1 + f
 | 4 | 
282.8900146484375 | 209.34738159179688 | 308.2923889160156 | 219.9425048828125 | 1 + f ′
 | 5 | 
309.9880065917969 | 193.49977111816406 | 522.00048828125 | 213.34005737304688 | 1/αN
≈2.7
(B.12)
 | 6 | 
245.91299438476562 | 230.2778778076172 | 271.5959167480469 | 254.65557861328125 | Sf
Sf ′ =
 | 7 | 
274.3609924316406 | 226.75477600097656 | 307.467041015625 | 238.30352783203125 |  
1 + 1
 | 8 | 
283.4429931640625 | 234.1127471923828 | 307.4670715332031 | 254.301513671875 | f
1 + 1
 | 9 | 
301.7969970703125 | 249.6103973388672 | 308.65972900390625 | 257.08355712890625 | f ′
 | 10 | 
311.5489807128906 | 224.3817901611328 | 522.0003662109375 | 247.21109008789062 | !1/αS
≈0.13
(B.13)
 | 11 | 
244.90199279785156 | 260.263916015625 | 288.7923889160156 | 284.64154052734375 | Cf
Cf ′ = Nf
 | 12 | 
275.5559997558594 | 260.263916015625 | 522.0004272460938 | 284.64154052734375 | Nf ′
Sf
Sf ′ ≈0.35
(B.14)
 | 13 | 
90.0 | 292.29644775390625 | 521.9981689453125 | 313.1680603027344 | So that compute-efﬁcient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less
compute to reach the same loss.
 | 14 | 
90.0 | 328.3074951171875 | 216.48516845703125 | 338.27008056640625 | B.4
Suboptimal Model Sizes
 | 15 | 
90.0 | 348.6674499511719 | 521.998046875 | 369.5390625 | We can solve A.1 to ﬁnd an expression for the amount of compute needed to reach a given value of the loss
L with a model of size N:
 | 16 | 
192.5469970703125 | 378.9373474121094 | 303.81060791015625 | 402.7965087890625 | C (N, L) =

6B∗Sc
N
L1/αB
 | 17 | 
306.2349853515625 | 378.9373474121094 | 361.6175842285156 | 395.78350830078125 |  
L −
Nc
 | 18 | 
351.5350036621094 | 392.6549377441406 | 359.544921875 | 402.6175231933594 | N
 | 19 | 
363.3080139160156 | 375.54437255859375 | 522.0004272460938 | 396.0140686035156 | αN −1/αS
.
(B.15)
 | 20 | 
89.99996948242188 | 411.38092041015625 | 521.9981079101562 | 470.66455078125 | Using A.6 and A.9, we can eliminate L in favor of Neﬀ(L), the model size which reaches L most efﬁciently.
From there, we ﬁnd an expression for the excess compute needed as a consequence of using a suboptimal
model size:
C (N, Neﬀ)
C (Neﬀ, Neﬀ) =
N
Neﬀ
 | 21 | 
276.0169982910156 | 446.1433410644531 | 311.7895812988281 | 462.989501953125 | 
1 + αS
 | 22 | 
299.6289978027344 | 459.8609313964844 | 312.3132629394531 | 470.66455078125 | αN
 | 23 | 
316.4139709472656 | 446.1433410644531 | 365.7099304199219 | 462.989501953125 | 
1 −
Neﬀ
 | 24 | 
353.5530090332031 | 459.8609313964844 | 361.56292724609375 | 469.8235168457031 | N
 | 25 | 
367.9580078125 | 442.7493591308594 | 522.0004272460938 | 463.2200622558594 | αN −1/αS
.
(B.16)
 | 26 | 
90.0 | 475.43743896484375 | 521.9981079101562 | 518.1270141601562 | The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a
20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A
larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism
and faster training if sufﬁcient harware is available (see Figure Y):
 | 27 | 
195.0989990234375 | 529.5823364257812 | 300.5256042480469 | 554.1045532226562 | S (N, Neﬀ)
S (Neﬀ, Neﬀ) =

1 + αS
 | 28 | 
288.364013671875 | 543.3009643554688 | 301.04827880859375 | 554.1045532226562 | αN
 | 29 | 
305.1499938964844 | 529.5823364257812 | 354.4459533691406 | 546.4295654296875 | 
1 −
Neﬀ
 | 30 | 
342.2879943847656 | 543.3009643554688 | 350.29791259765625 | 553.2635498046875 | N
 | 31 | 
356.6929931640625 | 526.1893920898438 | 522.00048828125 | 546.6600952148438 | αN −1/αS
.
(B.17)
 | 32 | 
90.00003051757812 | 561.5394897460938 | 521.998291015625 | 593.320068359375 | A 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation
should not be trusted for very large models, as it is only valid in the power-law region of the learning curve
after initial transient effects.
 | 33 | 
90.00003051757812 | 610.8651733398438 | 150.6726531982422 | 622.8203735351562 | C
Caveats
 | 34 | 
90.00003051757812 | 636.0394897460938 | 330.1186218261719 | 646.0020751953125 | In this section we list some potential caveats to our analysis.
 | 35 | 
115.90303039550781 | 657.5398559570312 | 521.9996948242188 | 722.4070434570312 | • At present we do not have a solid theoretical understanding for any of our proposed scaling laws.
The scaling relations with model size and compute are especially mysterious. It may be possible to
understand scaling at very large D holding model size ﬁxed [AS17], and also the shape of learning
curves late in training, by modeling the loss with a noisy quadratic. But the scaling with D at very
large model size still remains mysterious. Without a theory or a systematic understanding of the
corrections to our scaling laws, it’s difﬁcult to determine in what circumstances they can be trusted.
 | 36 | 
301.01904296875 | 742.33251953125 | 310.98162841796875 | 752.2951049804688 | 22
 | 37 | 
103
104
105
Sc × [L(N, D)
L(N,
)]
1/
S
103
104
105
Sstop
Early Stopping Step
Data Size
21M
43M
86M
172M
344M
688M
1.4B
103
104
105
Step
2
3
4
5
6
Loss
Test Loss
Train Loss
108
109
1010
Dataset Size (Tokens)
Figure 16
Left: We characterize the step on which early stopping occurs, as a function of the extent of
overﬁtting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right:
We display train and test loss for a series of 300M parameter models trained on different sized dataset sub-
samples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the
degree of overﬁtting (as compared to the inﬁnite data limit) is signiﬁcantly overestimated by Ltest −Ltrain
(denoted by a black bar for each run).
• We are not especially conﬁdent in the prediction of Bcrit(L) for values of the loss far outside the
range we have explored. Changes in Bcrit could have a signiﬁcant impact on trade-offs between
data parallelism and the number of serial training steps required, which would have a major impact
on training time.
• We did not thoroughly investigate the small data regime, and our ﬁts for L(N, D) were poor for
the smallest values of D (where an epoch corresponded to only 40 steps). Furthermore, we did
not experiment with regularization and data augmentation. Improvements in these could alter our
results, quantitatively or qualitatively.
• We used the estimated training compute C ≈6NBS, which did not include contributions propor-
tional to nctx (see Section 2.1). So our scalings with compute may be confounded in practice in the
regime of very large nctx, speciﬁcally where nctx ≳12dmodel.
• We tuned learning rates, and we experimented with learning rate schedules. But we may have
neglected to tune some hyperparameter (e.g. intialization scale or momentum) that have an important
effect on scaling.
• The optimal choice of learning rate is sensitive to the target loss. When training close to convergence,
it may be necessary to use a smaller learning rate to avoid divergences. But when conducting a short
training run (eg due to compute limitations), it may be possible to use a larger learning rate. We did
not experiment with higher learning rates for training runs that did not proceed to convergence.
D
Supplemental Figures
D.1
Early Stopping and Test vs Train
In section 5.3 we described the result shown in Figure 16, which provides a prediction for a lower bound on
the early stopping step. We also show the train and test loss for a given model size when training on different
sized datasets.
D.2
Universal Transformers
We compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure 17.
These models re-use parameters, and so perform slightly better as a function of N, but slightly worse as a
function of compute C. We include several different different possibilities for parameter re-use.
D.3
Batch Size
We measure the critical batch size using the data displayed in ﬁgure 18. This made it possible to estimate
Bcrit(L) in ﬁgure 10.
23

122.31973266601562 | 187.0601043701172 | 259.99530029296875 | 196.01844787597656 | 103
104
105
 |  | 
150.48263549804688 | 195.05548095703125 | 229.08209228515625 | 206.26629638671875 | Sc × [L(N, D)
L(N,
)]
1/
S
 | 1 | 
104.12008666992188 | 177.55624389648438 | 114.45561218261719 | 186.45846557617188 | 103
 | 2 | 
104.12008666992188 | 150.4701690673828 | 114.45561218261719 | 159.3723907470703 | 104
 | 3 | 
104.12008666992188 | 123.32798767089844 | 114.45561218261719 | 132.23020935058594 | 105
 | 4 | 
93.6191177368164 | 139.62811279296875 | 102.9336166381836 | 152.477294921875 | Sstop
 | 5 | 
153.44869995117188 | 94.68329620361328 | 226.83407592773438 | 105.14844512939453 | Early Stopping Step
 | 6 | 
266.106689453125 | 118.90497589111328 | 295.0648193359375 | 127.62593841552734 | Data Size
 | 7 | 
280.3023376464844 | 127.43011474609375 | 292.8377990722656 | 171.2962646484375 | 21M
43M
86M
172M
344M
688M
1.4B
 | 8 | 
354.0931396484375 | 184.73138427734375 | 462.9344177246094 | 195.28404235839844 | 103
104
105
 | 9 | 
397.5262145996094 | 194.17074584960938 | 413.8770446777344 | 204.4438018798828 | Step
 | 10 | 
327.1052551269531 | 172.49710083007812 | 331.5895690917969 | 182.77015686035156 | 2
 | 11 | 
327.1052551269531 | 147.66241455078125 | 331.5895690917969 | 157.9354705810547 | 3
 | 12 | 
327.1052551269531 | 122.8277359008789 | 331.5895690917969 | 133.1007843017578 | 4
 | 13 | 
327.1052551269531 | 97.9930648803711 | 331.5895690917969 | 108.26610565185547 | 5
 | 14 | 
327.1052551269531 | 73.15838623046875 | 331.5895690917969 | 83.43142700195312 | 6
 | 15 | 
315.8006591796875 | 123.1015396118164 | 326.0736999511719 | 139.26202392578125 | Loss
 | 16 | 
432.453857421875 | 82.18359375 | 469.928955078125 | 102.80465698242188 | Test Loss
Train Loss
 | 17 | 
493.9243469238281 | 155.19284057617188 | 506.09930419921875 | 165.6793975830078 | 108
 | 18 | 
493.9243469238281 | 120.06604766845703 | 506.09930419921875 | 130.5526123046875 | 109
 | 19 | 
493.9243469238281 | 84.93926239013672 | 509.2394714355469 | 95.42581176757812 | 1010
 | 20 | 
509.08392333984375 | 92.16057586669922 | 519.3569946289062 | 170.19198608398438 | Dataset Size (Tokens)
 | 21 | 
89.99996948242188 | 214.051513671875 | 522.0031127929688 | 278.6500549316406 | Figure 16
Left: We characterize the step on which early stopping occurs, as a function of the extent of
overﬁtting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right:
We display train and test loss for a series of 300M parameter models trained on different sized dataset sub-
samples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the
degree of overﬁtting (as compared to the inﬁnite data limit) is signiﬁcantly overestimated by Ltest −Ltrain
(denoted by a black bar for each run).
 | 22 | 
115.90292358398438 | 299.5257873535156 | 522.0045166015625 | 509.280029296875 | • We are not especially conﬁdent in the prediction of Bcrit(L) for values of the loss far outside the
range we have explored. Changes in Bcrit could have a signiﬁcant impact on trade-offs between
data parallelism and the number of serial training steps required, which would have a major impact
on training time.
• We did not thoroughly investigate the small data regime, and our ﬁts for L(N, D) were poor for
the smallest values of D (where an epoch corresponded to only 40 steps). Furthermore, we did
not experiment with regularization and data augmentation. Improvements in these could alter our
results, quantitatively or qualitatively.
• We used the estimated training compute C ≈6NBS, which did not include contributions propor-
tional to nctx (see Section 2.1). So our scalings with compute may be confounded in practice in the
regime of very large nctx, speciﬁcally where nctx ≳12dmodel.
• We tuned learning rates, and we experimented with learning rate schedules. But we may have
neglected to tune some hyperparameter (e.g. intialization scale or momentum) that have an important
effect on scaling.
• The optimal choice of learning rate is sensitive to the target loss. When training close to convergence,
it may be necessary to use a smaller learning rate to avoid divergences. But when conducting a short
training run (eg due to compute limitations), it may be possible to use a larger learning rate. We did
not experiment with higher learning rates for training runs that did not proceed to convergence.
 | 23 | 
89.99992370605469 | 525.6481323242188 | 222.29617309570312 | 537.6033325195312 | D
Supplemental Figures
 | 24 | 
89.99992370605469 | 549.8494873046875 | 254.7713165283203 | 559.8120727539062 | D.1
Early Stopping and Test vs Train
 | 25 | 
89.99992370605469 | 569.9154052734375 | 521.998046875 | 601.6959838867188 | In section 5.3 we described the result shown in Figure 16, which provides a prediction for a lower bound on
the early stopping step. We also show the train and test loss for a given model size when training on different
sized datasets.
 | 26 | 
89.99992370605469 | 615.659423828125 | 215.8973388671875 | 625.6220092773438 | D.2
Universal Transformers
 | 27 | 
89.99990844726562 | 634.2156982421875 | 522.0039672851562 | 667.5059814453125 | We compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure 17.
These models re-use parameters, and so perform slightly better as a function of N, but slightly worse as a
function of compute C. We include several different different possibilities for parameter re-use.
 | 28 | 
89.9999008178711 | 681.469482421875 | 158.98095703125 | 691.4320678710938 | D.3
Batch Size
 | 29 | 
89.9999008178711 | 701.535400390625 | 521.9981079101562 | 723.0175170898438 | We measure the critical batch size using the data displayed in ﬁgure 18. This made it possible to estimate
Bcrit(L) in ﬁgure 10.
 | 30 | 
301.0188903808594 | 742.3323974609375 | 310.9814758300781 | 752.2949829101562 | 23
 | 31 | 
105
106
107
108
109
Parameters, including reuse (non-embedding)
2.5
3.0
3.5
4.0
4.5
Test Loss
2x Reuse
4x Reuse
8x Reuse
Non-recurrent Models
105
106
107
108
109
Parameters (non-embedding)
2.5
3.0
3.5
4.0
4.5
Test Loss
2x Reuse
4x Reuse
8x Reuse
Non-recurrent Models
Figure 17
We compare recurrent Transformers [DGV+18], which re-use parameters, to standard Trans-
formers. Recurrent Transformers perform slightly better when comparing models with equal parameter count,
but slightly worse when accounting for reuse and comparing per FLOP.
102
103
104
105
Step
106
107
108
109
1010
1011
Tokens Processed
Batch Size Scan - 3M Params
4
6
8
10
Test Loss
101
102
103
104
105
Step
106
108
1010
Tokens Processed
Batch Size Scan - 85M Params
4
6
8
10
Test Loss
Figure 18
These ﬁgures demonstrate ﬁts to Equation (5.1) for a large number of values of the loss L, and
for two different Transformer model sizes. These ﬁts were used to measure Bcrit(L) for Figure 10.
D.4
Sample Efﬁciency vs Model Size
It is easy to see from ﬁgure 2 that larger models train faster, and are therefore more sample efﬁcient. We
provide another way of looking at this phenomenon in ﬁgure 19, which shows when different models reach
various ﬁxed values of the loss.
106
107
108
Parameters (non-embedding)
103
104
105
Minimum Steps (Smin)
2.5
3.0
3.5
4.0
4.5
5.0
5.5
Loss
106
107
108
Parameters (non-embedding)
108
109
1010
1011
Minimum Examples (Emin)
2.5
3.0
3.5
4.0
4.5
5.0
5.5
Loss
Figure 19
The number of minimum serial steps needed to reach any ﬁxed value of the test loss decreases
precipitously with model size. Sample efﬁciency (show here for training far below the critical batch size)
improves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model
to a very large one.
24

120.49006652832031 | 183.6464080810547 | 294.0143127441406 | 194.10728454589844 | 105
106
107
108
109
 |  | 
125.8192138671875 | 193.00369262695312 | 289.0589599609375 | 203.18739318847656 | Parameters, including reuse (non-embedding)
 | 1 | 
104.84587097167969 | 165.51138305664062 | 115.95917510986328 | 175.69508361816406 | 2.5
 | 2 | 
104.84587097167969 | 140.01541137695312 | 115.95917510986328 | 150.19911193847656 | 3.0
 | 3 | 
104.84587097167969 | 118.45887756347656 | 115.95917510986328 | 128.642578125 | 3.5
 | 4 | 
104.84587097167969 | 99.78575897216797 | 115.95917510986328 | 109.9694595336914 | 4.0
 | 5 | 
104.84587097167969 | 83.31489562988281 | 115.95917510986328 | 93.49859619140625 | 4.5
 | 6 | 
93.63957977294922 | 112.5649642944336 | 103.82328033447266 | 146.00274658203125 | Test Loss
 | 7 | 
144.5190887451172 | 136.6945343017578 | 223.75205993652344 | 177.65231323242188 | 2x Reuse
4x Reuse
8x Reuse
Non-recurrent Models
 | 8 | 
342.6410827636719 | 183.6464080810547 | 516.1653442382812 | 194.10728454589844 | 105
106
107
108
109
 | 9 | 
377.698974609375 | 193.00369262695312 | 481.4930419921875 | 203.18739318847656 | Parameters (non-embedding)
 | 10 | 
326.99688720703125 | 165.51138305664062 | 338.11016845703125 | 175.69508361816406 | 2.5
 | 11 | 
326.99688720703125 | 140.01541137695312 | 338.11016845703125 | 150.19911193847656 | 3.0
 | 12 | 
326.99688720703125 | 118.45887756347656 | 338.11016845703125 | 128.642578125 | 3.5
 | 13 | 
326.99688720703125 | 99.78575897216797 | 338.11016845703125 | 109.9694595336914 | 4.0
 | 14 | 
326.99688720703125 | 83.31489562988281 | 338.11016845703125 | 93.49859619140625 | 4.5
 | 15 | 
315.79058837890625 | 112.5649642944336 | 325.9742736816406 | 146.00274658203125 | Test Loss
 | 16 | 
432.8787536621094 | 79.43961334228516 | 512.1116943359375 | 120.39739227294922 | 2x Reuse
4x Reuse
8x Reuse
Non-recurrent Models
 | 17 | 
90.0 | 211.74977111816406 | 521.9982299804688 | 245.04006958007812 | Figure 17
We compare recurrent Transformers [DGV+18], which re-use parameters, to standard Trans-
formers. Recurrent Transformers perform slightly better when comparing models with equal parameter count,
but slightly worse when accounting for reuse and comparing per FLOP.
 | 18 | 
141.2659149169922 | 381.2104797363281 | 246.99990844726562 | 391.7427978515625 | 102
103
104
105
 | 19 | 
190.71292114257812 | 390.63165283203125 | 207.0322723388672 | 400.8849182128906 | Step
 | 20 | 
114.09722137451172 | 360.2099914550781 | 126.24872589111328 | 370.67633056640625 | 106
 | 21 | 
114.09722137451172 | 344.2586364746094 | 126.24872589111328 | 354.7249755859375 | 107
 | 22 | 
114.09722137451172 | 328.439208984375 | 126.24872589111328 | 338.9055480957031 | 108
 | 23 | 
114.09722137451172 | 312.5538330078125 | 126.24872589111328 | 323.0201721191406 | 109
 | 24 | 
111.08126068115234 | 296.66845703125 | 126.36688232421875 | 307.1347961425781 | 1010
 | 25 | 
111.08126068115234 | 280.7830505371094 | 126.36688232421875 | 291.2493896484375 | 1011
 | 26 | 
99.79843139648438 | 299.1181640625 | 110.05168914794922 | 363.1922607421875 | Tokens Processed
 | 27 | 
135.98974609375 | 269.3857421875 | 262.136474609375 | 281.68963623046875 | Batch Size Scan - 3M Params
 | 28 | 
284.6068420410156 | 354.600830078125 | 289.08251953125 | 364.8540954589844 | 4
 | 29 | 
284.6068420410156 | 328.6133117675781 | 289.08251953125 | 338.8665771484375 | 6
 | 30 | 
284.6068420410156 | 302.62579345703125 | 289.08251953125 | 312.8790588378906 | 8
 | 31 | 
284.6068420410156 | 276.6382751464844 | 293.5581970214844 | 286.89154052734375 | 10
 | 32 | 
293.1083984375 | 314.32061767578125 | 303.3616638183594 | 347.9867858886719 | Test Loss
 | 33 | 
336.86614990234375 | 381.2104797363281 | 458.77679443359375 | 391.7427978515625 | 101
102
103
104
105
 | 34 | 
400.5619201660156 | 390.63165283203125 | 416.88128662109375 | 400.8849182128906 | Step
 | 35 | 
323.94622802734375 | 354.16546630859375 | 336.09771728515625 | 364.6318054199219 | 106
 | 36 | 
323.94622802734375 | 324.4462890625 | 336.09771728515625 | 334.9126281738281 | 108
 | 37 | 
320.9302673339844 | 294.72711181640625 | 336.21588134765625 | 305.1934509277344 | 1010
 | 38 | 
309.6474304199219 | 299.1181640625 | 319.90069580078125 | 363.1922607421875 | Tokens Processed
 | 39 | 
343.1604919433594 | 269.3857421875 | 474.6780700683594 | 281.68963623046875 | Batch Size Scan - 85M Params
 | 40 | 
494.4558410644531 | 354.600830078125 | 498.9315185546875 | 364.8540954589844 | 4
 | 41 | 
494.4558410644531 | 328.6133117675781 | 498.9315185546875 | 338.8665771484375 | 6
 | 42 | 
494.4558410644531 | 302.62579345703125 | 498.9315185546875 | 312.8790588378906 | 8
 | 43 | 
494.4558410644531 | 276.6382751464844 | 503.4071960449219 | 286.89154052734375 | 10
 | 44 | 
502.9573974609375 | 314.32061767578125 | 513.2106323242188 | 347.9867858886719 | Test Loss
 | 45 | 
89.99996948242188 | 410.346923828125 | 521.9989624023438 | 432.060546875 | Figure 18
These ﬁgures demonstrate ﬁts to Equation (5.1) for a large number of values of the loss L, and
for two different Transformer model sizes. These ﬁts were used to measure Bcrit(L) for Figure 10.
 | 46 | 
89.99996948242188 | 465.342529296875 | 251.13507080078125 | 475.30511474609375 | D.4
Sample Efﬁciency vs Model Size
 | 47 | 
89.99996948242188 | 490.261474609375 | 521.9982299804688 | 522.0420532226562 | It is easy to see from ﬁgure 2 that larger models train faster, and are therefore more sample efﬁcient. We
provide another way of looking at this phenomenon in ﬁgure 19, which shows when different models reach
various ﬁxed values of the loss.
 | 48 | 
114.1540756225586 | 649.601806640625 | 227.12660217285156 | 658.8999633789062 | 106
107
108
 | 49 | 
147.7669677734375 | 657.9190673828125 | 240.02464294433594 | 666.9708862304688 | Parameters (non-embedding)
 | 50 | 
105.5007553100586 | 630.676513671875 | 116.22838592529297 | 639.9163818359375 | 103
 | 51 | 
105.5007553100586 | 596.5693969726562 | 116.22838592529297 | 605.8092651367188 | 104
 | 52 | 
105.5007553100586 | 562.4039306640625 | 116.22838592529297 | 571.6438598632812 | 105
 | 53 | 
95.33892059326172 | 568.2885131835938 | 105.26773834228516 | 637.11328125 | Minimum Steps (Smin)
 | 54 | 
285.1462097167969 | 636.0097045898438 | 295.0242919921875 | 645.0615234375 | 2.5
 | 55 | 
285.1462097167969 | 621.9190063476562 | 295.0242919921875 | 630.9708251953125 | 3.0
 | 56 | 
285.1462097167969 | 607.828369140625 | 295.0242919921875 | 616.8801879882812 | 3.5
 | 57 | 
285.1462097167969 | 593.7376708984375 | 295.0242919921875 | 602.7894897460938 | 4.0
 | 58 | 
285.1462097167969 | 579.64697265625 | 295.0242919921875 | 588.6987915039062 | 4.5
 | 59 | 
285.1462097167969 | 565.5563354492188 | 295.0242919921875 | 574.608154296875 | 5.0
 | 60 | 
285.1462097167969 | 551.4656372070312 | 295.0242919921875 | 560.5174560546875 | 5.5
 | 61 | 
294.627685546875 | 595.3850708007812 | 303.67950439453125 | 609.6243896484375 | Loss
 | 62 | 
330.9475402832031 | 649.6388549804688 | 442.2400817871094 | 658.9196166992188 | 106
107
108
 | 63 | 
363.431640625 | 657.9405517578125 | 455.51678466796875 | 666.9754028320312 | Parameters (non-embedding)
 | 64 | 
322.3104248046875 | 628.9705200195312 | 333.0179748535156 | 638.193115234375 | 108
 | 65 | 
322.3104248046875 | 604.7154541015625 | 333.0179748535156 | 613.9380493164062 | 109
 | 66 | 
319.65283203125 | 580.46044921875 | 333.1220703125 | 589.6830444335938 | 1010
 | 67 | 
319.65283203125 | 556.2053833007812 | 333.1220703125 | 565.427978515625 | 1011
 | 68 | 
309.5100402832031 | 562.0741577148438 | 319.4202880859375 | 643.5961303710938 | Minimum Examples (Emin)
 | 69 | 
499.35479736328125 | 636.0722045898438 | 509.21441650390625 | 645.1070556640625 | 2.5
 | 70 | 
499.35479736328125 | 622.0078735351562 | 509.21441650390625 | 631.042724609375 | 3.0
 | 71 | 
499.35479736328125 | 607.9435424804688 | 509.21441650390625 | 616.9783935546875 | 3.5
 | 72 | 
499.35479736328125 | 593.8792114257812 | 509.21441650390625 | 602.9140625 | 4.0
 | 73 | 
499.35479736328125 | 579.8148803710938 | 509.21441650390625 | 588.8497314453125 | 4.5
 | 74 | 
499.35479736328125 | 565.7505493164062 | 509.21441650390625 | 574.785400390625 | 5.0
 | 75 | 
499.35479736328125 | 551.6862182617188 | 509.21441650390625 | 560.7210693359375 | 5.5
 | 76 | 
508.81854248046875 | 595.5234375 | 517.8533935546875 | 609.7362060546875 | Loss
 | 77 | 
90.0 | 676.353515625 | 522.0028686523438 | 719.134033203125 | Figure 19
The number of minimum serial steps needed to reach any ﬁxed value of the test loss decreases
precipitously with model size. Sample efﬁciency (show here for training far below the critical batch size)
improves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model
to a very large one.
 | 78 | 
301.01898193359375 | 742.3324584960938 | 310.9815673828125 | 752.2950439453125 | 24
 | 79 | 
100
101
102
103
Token Index
3
4
5
6
7
8
Per-Token Test Loss
4.0 + 3.2 T
0.47
3.4 + 4.0 T
0.56
2.9 + 4.5 T
0.56
2.7 + 4.9 T
0.60
2.4 + 5.1 T
0.61
2.3 + 5.4 T
0.62
106
107
108
Model Parameters
101
103
105
Step
2
4
6
8
10
Test Loss
Per-token Loss (774M Params)
100
101
102
103
Token Index
Figure 20
This ﬁgure provides information about the performance per token as a function of model size
and training time. Left: Loss per token as a function of its position T in the 1024-token context. Loss scales
predictably as a power-law in T. Right: Test loss per token as a function of training step.
104
105
106
107
108
109
Parameters (excl. embedding)
3.0
4.5
6.0
7.5
Test Loss
Token 1/1024
Token 2/1024
Token 4/1024
Token 8/1024
Token 16/1024
Token 64/1024
Token 256/1024
Token 1024/1024
Token 1/8
Token 2/8
Token 4/8
Token 8/8
Figure 21
In addition to the averaged loss, individual tokens within the 1024-token context also improve
smoothly as model size increases. Training runs with shorter context nctx = 8 (dashed lines) perform better
on early tokens, since they can allocate all of their capacity to them.
D.5
Context Dependence
The trends for loss as a function of model size are displayed for different tokens in the context in Figure 21.
We see that models trained on nctx = 1024 show steady improvement with model size on all but the ﬁrst
token.
Fixing model size, it appears that the loss scales as a power-law as a function of position T in the context, see
Figure 20. This may be a consequence of underlying power-law correlations in language [EP94, ACDE12,
LT16], or a more general feature of the model architecture and optimization. It provides some suggestion for
the potential beneﬁts (or lack thereof) from training on larger contexts. Not only do larger models converge
to better performance at T = 1024, but they also improve more quickly at early tokens, suggesting that larger
models are more efﬁcient at detecting patterns with less contextual information. In the right-hand plot we
show how per-token performance varies for a ﬁxed model as a function of the training step. The model begins
by learning short-range information, and only learns longer-range correlations later in training.
We have also included models trained with a tiny context nctx = 8 in order to compare with our longer
context models. Even modestly sized models trained on nctx = 8 can dominate our largest nctx = 1024
models on very early tokens. This also suggests that further improvements should be possible with much
larger models trained on large contexts.
D.6
Learning Rate Schedules and Error Analysis
We experimented with a variety of learning rates and schedules. A host of schedules and resulting test
performances for a small language model are plotted in Figure 22. We conclude that the choice of learning
rate schedule is mostly irrelevant, as long as the total summed learning rate is sufﬁciently large, and the
schedule includes a warmup period and a ﬁnal decay to near-vanishing learning rate. Variations among
25

114.16484069824219 | 187.16526794433594 | 267.7503662109375 | 196.54681396484375 | 100
101
102
103
 |  | 
171.63430786132812 | 195.55084228515625 | 211.1466064453125 | 204.7413787841797 | Token Index
 | 1 | 
105.47095489501953 | 166.88404846191406 | 109.48274230957031 | 176.0745849609375 | 3
 | 2 | 
105.47095489501953 | 150.9610595703125 | 109.48274230957031 | 160.15159606933594 | 4
 | 3 | 
105.47095489501953 | 135.0380859375 | 109.48274230957031 | 144.22862243652344 | 5
 | 4 | 
105.47095489501953 | 119.1151123046875 | 109.48274230957031 | 128.30563354492188 | 6
 | 5 | 
105.47095489501953 | 103.19212341308594 | 109.48274230957031 | 112.38264465332031 | 7
 | 6 | 
105.47095489501953 | 87.2691421508789 | 109.48274230957031 | 96.45966339111328 | 8
 | 7 | 
95.3575668334961 | 106.32608032226562 | 104.54808807373047 | 171.2589874267578 | Per-Token Test Loss
 | 8 | 
230.03956604003906 | 93.11504364013672 | 265.822021484375 | 100.8147201538086 | 4.0 + 3.2 T
0.47
 | 9 | 
230.03956604003906 | 100.32400512695312 | 265.822021484375 | 108.023681640625 | 3.4 + 4.0 T
0.56
 | 10 | 
230.03956604003906 | 107.53296661376953 | 265.822021484375 | 115.2326431274414 | 2.9 + 4.5 T
0.56
 | 11 | 
230.03956604003906 | 114.74192810058594 | 265.822021484375 | 122.44160461425781 | 2.7 + 4.9 T
0.60
 | 12 | 
230.03956604003906 | 121.95088195800781 | 265.822021484375 | 129.6505584716797 | 2.4 + 5.1 T
0.61
 | 13 | 
230.03956604003906 | 129.1598663330078 | 265.822021484375 | 136.85952758789062 | 2.3 + 5.4 T
0.62
 | 14 | 
287.9072265625 | 169.63221740722656 | 298.79925537109375 | 179.01376342773438 | 106
 | 15 | 
287.9072265625 | 140.2666015625 | 298.79925537109375 | 149.6481475830078 | 107
 | 16 | 
287.9072265625 | 111.01927947998047 | 298.79925537109375 | 120.40081024169922 | 108
 | 17 | 
298.7659912109375 | 109.26374053955078 | 307.9565124511719 | 168.33035278320312 | Model Parameters
 | 18 | 
361.4450378417969 | 184.7557373046875 | 468.03271484375 | 195.30587768554688 | 101
103
105
 | 19 | 
399.65435791015625 | 194.19288635253906 | 416.0013122558594 | 204.46347045898438 | Step
 | 20 | 
329.7566833496094 | 177.81130981445312 | 334.23992919921875 | 188.08189392089844 | 2
 | 21 | 
329.7566833496094 | 156.77743530273438 | 334.23992919921875 | 167.0480194091797 | 4
 | 22 | 
329.7566833496094 | 135.74356079101562 | 334.23992919921875 | 146.01414489746094 | 6
 | 23 | 
329.7566833496094 | 114.70967864990234 | 334.23992919921875 | 124.98027801513672 | 8
 | 24 | 
325.2723083496094 | 93.6758041381836 | 334.2388000488281 | 103.94640350341797 | 10
 | 25 | 
313.97039794921875 | 117.75277709960938 | 324.2409973144531 | 151.4758758544922 | Test Loss
 | 26 | 
341.6680908203125 | 72.74191284179688 | 474.380859375 | 85.06663513183594 | Per-token Loss (774M Params)
 | 27 | 
495.11981201171875 | 178.5043487548828 | 507.2918701171875 | 188.9884033203125 | 100
 | 28 | 
495.11981201171875 | 145.64398193359375 | 507.2918701171875 | 156.12803649902344 | 101
 | 29 | 
495.11981201171875 | 112.78360748291016 | 507.2918701171875 | 123.26766204833984 | 102
 | 30 | 
495.11981201171875 | 79.9232406616211 | 507.2918701171875 | 90.40729522705078 | 103
 | 31 | 
507.2546691894531 | 112.53217315673828 | 517.5252685546875 | 156.68798828125 | Token Index
 | 32 | 
90.0 | 214.06951904296875 | 522.0039672851562 | 245.94107055664062 | Figure 20
This ﬁgure provides information about the performance per token as a function of model size
and training time. Left: Loss per token as a function of its position T in the 1024-token context. Loss scales
predictably as a power-law in T. Right: Test loss per token as a function of training step.
 | 33 | 
221.63763427734375 | 366.50225830078125 | 361.31219482421875 | 376.5440673828125 | 104
105
106
107
108
109
 | 34 | 
235.72329711914062 | 375.48468017578125 | 338.33837890625 | 385.2604064941406 | Parameters (excl. embedding)
 | 35 | 
202.46693420410156 | 338.5919189453125 | 213.13504028320312 | 348.3676452636719 | 3.0
 | 36 | 
202.46693420410156 | 308.0421142578125 | 213.13504028320312 | 317.8178405761719 | 4.5
 | 37 | 
202.46693420410156 | 286.3666687011719 | 213.13504028320312 | 296.14239501953125 | 6.0
 | 38 | 
202.46693420410156 | 269.5538635253906 | 213.13504028320312 | 279.32958984375 | 7.5
 | 39 | 
191.70957946777344 | 298.268310546875 | 201.48532104492188 | 330.3665771484375 | Test Loss
 | 40 | 
376.0875244140625 | 271.64422607421875 | 418.520263671875 | 355.9733581542969 | Token 1/1024
Token 2/1024
Token 4/1024
Token 8/1024
Token 16/1024
Token 64/1024
Token 256/1024
Token 1024/1024
Token 1/8
Token 2/8
Token 4/8
Token 8/8
 | 41 | 
90.0 | 394.8365173339844 | 522.00048828125 | 426.70806884765625 | Figure 21
In addition to the averaged loss, individual tokens within the 1024-token context also improve
smoothly as model size increases. Training runs with shorter context nctx = 8 (dashed lines) perform better
on early tokens, since they can allocate all of their capacity to them.
 | 42 | 
90.00003051757812 | 450.38250732421875 | 202.13906860351562 | 460.3450927734375 | D.5
Context Dependence
 | 43 | 
90.00001525878906 | 470.7044372558594 | 521.9989013671875 | 502.48504638671875 | The trends for loss as a function of model size are displayed for different tokens in the context in Figure 21.
We see that models trained on nctx = 1024 show steady improvement with model size on all but the ﬁrst
token.
 | 44 | 
90.0 | 508.67987060546875 | 521.9995727539062 | 595.2369995117188 | Fixing model size, it appears that the loss scales as a power-law as a function of position T in the context, see
Figure 20. This may be a consequence of underlying power-law correlations in language [EP94, ACDE12,
LT16], or a more general feature of the model architecture and optimization. It provides some suggestion for
the potential beneﬁts (or lack thereof) from training on larger contexts. Not only do larger models converge
to better performance at T = 1024, but they also improve more quickly at early tokens, suggesting that larger
models are more efﬁcient at detecting patterns with less contextual information. In the right-hand plot we
show how per-token performance varies for a ﬁxed model as a function of the training step. The model begins
by learning short-range information, and only learns longer-range correlations later in training.
 | 45 | 
90.0 | 601.431884765625 | 521.999267578125 | 644.3529663085938 | We have also included models trained with a tiny context nctx = 8 in order to compare with our longer
context models. Even modestly sized models trained on nctx = 8 can dominate our largest nctx = 1024
models on very early tokens. This also suggests that further improvements should be possible with much
larger models trained on large contexts.
 | 46 | 
90.00003051757812 | 659.3954467773438 | 304.78369140625 | 669.3580322265625 | D.6
Learning Rate Schedules and Error Analysis
 | 47 | 
90.00003051757812 | 679.7174072265625 | 521.9983520507812 | 722.406982421875 | We experimented with a variety of learning rates and schedules. A host of schedules and resulting test
performances for a small language model are plotted in Figure 22. We conclude that the choice of learning
rate schedule is mostly irrelevant, as long as the total summed learning rate is sufﬁciently large, and the
schedule includes a warmup period and a ﬁnal decay to near-vanishing learning rate. Variations among
 | 48 | 
301.01904296875 | 742.3323974609375 | 310.98162841796875 | 752.2949829101562 | 25
 | 49 | 
0
50000
100000
150000
200000
250000
Step
0.0000
0.0002
0.0004
0.0006
0.0008
0.0010
Learning Rate
50
100
150
200
250
LR Summed Over Steps
3.65
3.70
3.75
3.80
3.85
3.90
Loss
Figure 22
We test a variety of learning rate schedules including cosine decay, linear decay, as well as other
faster/slower decays schedules on a 3 million parameter model, shown on the left. For these experiments we
do not decay to zero, since we ﬁnd that this tends to give a ﬁxed improvement close to the end of training.
We ﬁnd that, as long as the learning rate is not too small and does not decay too quickly, performance does
not depend strongly on learning rate. Run-to-run variation is at the level of 0.05 in the loss, so averaging
multiple runs is necessary to validate performance changes smaller than this level.
104
105
106
107
108
109
Parameters (non-embedding)
2
3
4
5
6
Test Loss (at convergence)
L = (N/8.8 1013)
0.076
L =
0.25log(N/7.1 1012)
Figure 23
The trend for performance as a function of parameter count, L(N), is ﬁt better by a power law
than by other functions such as a logarithm at a qualitative level.
schedules appear to be statistical noise, and provide a rough gauge for the scale of variation between different
training runs. Experiments on larger models suggest that the variation in the ﬁnal test loss between different
random seeds is roughly constant in magnitude for different model sizes.
We found that larger models require a smaller learning rate to prevent divergence, while smaller models can
tolerate a larger learning rate. To implement this, the following rule of thumb was used for most runs:
LR(N) ≈0.003239 + −0.0001395 log(N)
(D.1)
We expect that this formula could be improved. There may be a dependence on network width, likely set by
the initialization scale. The formula also breaks down for N > 1010 parameters. Nevertheless, we found that
it works sufﬁciently well for the models we considered.
D.7
Fit Details and Power Law Quality
We experimented with a number of functional forms for the ﬁts to L(N), L(C), and L(D); the power-law
ﬁts were qualitatively much more accurate than other functions such as logarithms (see Figure 23).
For L(C), we do not include small models with only 1 layer in the ﬁt, as the transition from 1 to 2 layers
causes a noticable lump in the data. For L(N) we also do not include very small models with only 1 layer in
the ﬁt, and we exclude the largest models that have not trained fully to convergence. Fit parameters change
marginally if we do include them, and the trend extrapolates well in both directions regardless.
D.8
Generalization and Architecture
In ﬁgure 24 we show that generalization to other data distributions does not depend on network depth when we
hold the total parameter count ﬁxed. It seems to depend only on the performance on the training distribution.
26

138.3563690185547 | 164.92642211914062 | 303.0555725097656 | 180.3866729736328 | 0
50000
100000
150000
200000
250000
Step
 |  | 
117.35987091064453 | 155.85519409179688 | 137.3130340576172 | 164.16615295410156 | 0.0000
 | 1 | 
117.35987091064453 | 139.6494598388672 | 137.3130340576172 | 147.96041870117188 | 0.0002
 | 2 | 
117.35987091064453 | 123.44371032714844 | 137.3130340576172 | 131.75466918945312 | 0.0004
 | 3 | 
117.35987091064453 | 107.23796844482422 | 137.3130340576172 | 115.54893493652344 | 0.0006
 | 4 | 
117.35987091064453 | 91.03223419189453 | 137.3130340576172 | 99.34320068359375 | 0.0008
 | 5 | 
117.35987091064453 | 74.82649230957031 | 137.3130340576172 | 83.13745880126953 | 0.0010
 | 6 | 
108.13160705566406 | 98.85807800292969 | 116.44257354736328 | 140.15049743652344 | Learning Rate
 | 7 | 
353.7236633300781 | 164.9138946533203 | 502.52349853515625 | 180.38516235351562 | 50
100
150
200
250
LR Summed Over Steps
 | 8 | 
318.5810546875 | 159.89053344726562 | 331.28753662109375 | 168.20741271972656 | 3.65
 | 9 | 
318.5810546875 | 142.50003051757812 | 331.28753662109375 | 150.81690979003906 | 3.70
 | 10 | 
318.5810546875 | 125.10952758789062 | 331.28753662109375 | 133.42640686035156 | 3.75
 | 11 | 
318.5810546875 | 107.71902465820312 | 331.28753662109375 | 116.03590393066406 | 3.80
 | 12 | 
318.5810546875 | 90.32852172851562 | 331.28753662109375 | 98.64540100097656 | 3.85
 | 13 | 
318.5810546875 | 72.93801879882812 | 331.28753662109375 | 81.25489807128906 | 3.90
 | 14 | 
309.4290466308594 | 114.04389190673828 | 317.74591064453125 | 127.12712860107422 | Loss
 | 15 | 
90.0 | 189.488525390625 | 522.0003051757812 | 254.08706665039062 | Figure 22
We test a variety of learning rate schedules including cosine decay, linear decay, as well as other
faster/slower decays schedules on a 3 million parameter model, shown on the left. For these experiments we
do not decay to zero, since we ﬁnd that this tends to give a ﬁxed improvement close to the end of training.
We ﬁnd that, as long as the learning rate is not too small and does not decay too quickly, performance does
not depend strongly on learning rate. Run-to-run variation is at the level of 0.05 in the loss, so averaging
multiple runs is necessary to validate performance changes smaller than this level.
 | 16 | 
247.85491943359375 | 378.63604736328125 | 381.8458557128906 | 389.17889404296875 | 104
105
106
107
108
109
 | 17 | 
263.39642333984375 | 388.066650390625 | 368.0033874511719 | 398.330078125 | Parameters (non-embedding)
 | 18 | 
233.40084838867188 | 372.8106689453125 | 237.8809814453125 | 383.0740966796875 | 2
 | 19 | 
233.40084838867188 | 347.04534912109375 | 237.8809814453125 | 357.30877685546875 | 3
 | 20 | 
233.40084838867188 | 321.2800598144531 | 237.8809814453125 | 331.54351806640625 | 4
 | 21 | 
233.40084838867188 | 295.5147399902344 | 237.8809814453125 | 305.7781982421875 | 5
 | 22 | 
233.40084838867188 | 269.74945068359375 | 237.8809814453125 | 280.01287841796875 | 6
 | 23 | 
222.00457763671875 | 275.7142028808594 | 232.26803588867188 | 371.9878234863281 | Test Loss (at convergence)
 | 24 | 
291.10101318359375 | 273.2021179199219 | 366.8106994628906 | 285.2401123046875 | L = (N/8.8 1013)
0.076
 | 25 | 
291.10101318359375 | 284.7276306152344 | 383.5574645996094 | 296.765625 | L =
0.25log(N/7.1 1012)
 | 26 | 
90.0 | 407.8969421386719 | 521.9976806640625 | 428.99908447265625 | Figure 23
The trend for performance as a function of parameter count, L(N), is ﬁt better by a power law
than by other functions such as a logarithm at a qualitative level.
 | 27 | 
90.0 | 450.6144714355469 | 521.9982299804688 | 482.39508056640625 | schedules appear to be statistical noise, and provide a rough gauge for the scale of variation between different
training runs. Experiments on larger models suggest that the variation in the ﬁnal test loss between different
random seeds is roughly constant in magnitude for different model sizes.
 | 28 | 
90.0 | 488.82147216796875 | 521.9982299804688 | 509.6930847167969 | We found that larger models require a smaller learning rate to prevent divergence, while smaller models can
tolerate a larger learning rate. To implement this, the following rule of thumb was used for most runs:
 | 29 | 
218.38400268554688 | 515.2747802734375 | 522.0000610351562 | 525.5970458984375 | LR(N) ≈0.003239 + −0.0001395 log(N)
(D.1)
 | 30 | 
90.0 | 531.5384521484375 | 521.9981689453125 | 563.3190307617188 | We expect that this formula could be improved. There may be a dependence on network width, likely set by
the initialization scale. The formula also breaks down for N > 1010 parameters. Nevertheless, we found that
it works sufﬁciently well for the models we considered.
 | 31 | 
90.0 | 577.3675537109375 | 262.1835632324219 | 587.3301391601562 | D.7
Fit Details and Power Law Quality
 | 32 | 
89.99996948242188 | 597.2029418945312 | 522.004150390625 | 618.3050537109375 | We experimented with a number of functional forms for the ﬁts to L(N), L(C), and L(D); the power-law
ﬁts were qualitatively much more accurate than other functions such as logarithms (see Figure 23).
 | 33 | 
89.99993896484375 | 624.5009155273438 | 522.0046997070312 | 667.4210815429688 | For L(C), we do not include small models with only 1 layer in the ﬁt, as the transition from 1 to 2 layers
causes a noticable lump in the data. For L(N) we also do not include very small models with only 1 layer in
the ﬁt, and we exclude the largest models that have not trained fully to convergence. Fit parameters change
marginally if we do include them, and the trend extrapolates well in both directions regardless.
 | 34 | 
89.99993896484375 | 681.4695434570312 | 252.4202423095703 | 691.43212890625 | D.8
Generalization and Architecture
 | 35 | 
89.99993896484375 | 701.5354614257812 | 521.998046875 | 722.4070434570312 | In ﬁgure 24 we show that generalization to other data distributions does not depend on network depth when we
hold the total parameter count ﬁxed. It seems to depend only on the performance on the training distribution.
 | 36 | 
301.0189208984375 | 742.3324584960938 | 310.98150634765625 | 752.2950439453125 | 26
 | 37 | 
101
102
Depth
2.3
2.4
2.5
2.6
2.7
2.8
Test Loss
Wikipedia
Books
Internet Books
Common Crawl
WebText2 (Train)
WebText2 (Test)
Figure 24
We show evaluations on a series of datasets for models with approximately 1.5 Billion param-
eters. We observe no effect of depth on generalization; generalization performance depends primarily on
training distribution performance. The 12-layer model overﬁt the Internet Books dataset and we show the
early-stopped performance; we have not seen this surprising result in other experiments.
List of Figures
1
Summary of simple power laws. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2
Illustration of sample efﬁciency and compute efﬁciency. . . . . . . . . . . . . . . . . . . . .
4
3
How to scale up model size, batch size, and serial steps . . . . . . . . . . . . . . . . . . . .
4
4
Performance when varying model and data size, or model and training steps, simultaneously
5
5
Weak dependence of performance on hyperparameter tuning . . . . . . . . . . . . . . . . .
8
6
Comparison of performance trend when including or excluding embeddings . . . . . . . . .
8
7
LSTM and Transformer performance comparison . . . . . . . . . . . . . . . . . . . . . . .
9
8
Generalization to other test datasets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
9
Universality of overﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
10
Critical batch size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
11
Performance versus compute budget or number of parameter updates . . . . . . . . . . . . .
14
12
Training on suboptimal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
13
Comparison between empirical and adjusted compute trends . . . . . . . . . . . . . . . . .
15
14
Optimal model size and serial number of steps versus compute budget . . . . . . . . . . . .
16
15
Contradiction between compute and data trends . . . . . . . . . . . . . . . . . . . . . . . .
17
16
Early stopping lower bound and training curves for overﬁt models
. . . . . . . . . . . . . .
23
17
Universal transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
18
Batch size scans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
19
Another look at sample efﬁciency
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
20
Power-law dependence of performance on position in context . . . . . . . . . . . . . . . . .
25
21
Performance at different context positions versus model size
. . . . . . . . . . . . . . . . .
25
22
Learning rate schedule scan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
23
Comparison of Power-Law and Logarithmic Fits
. . . . . . . . . . . . . . . . . . . . . . .
26
24
Generalization versus depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
27

241.39755249023438 | 188.6668701171875 | 322.75360107421875 | 199.50469970703125 | 101
102
 |  | 
274.3077697753906 | 198.3541259765625 | 296.7517395019531 | 208.97129821777344 | Depth
 | 1 | 
212.12574768066406 | 176.56227111816406 | 223.7120819091797 | 187.179443359375 | 2.3
 | 2 | 
212.12574768066406 | 158.130126953125 | 223.7120819091797 | 168.74729919433594 | 2.4
 | 3 | 
212.12574768066406 | 139.69798278808594 | 223.7120819091797 | 150.31515502929688 | 2.5
 | 4 | 
212.12574768066406 | 121.26586151123047 | 223.7120819091797 | 131.88302612304688 | 2.6
 | 5 | 
212.12574768066406 | 102.83372497558594 | 223.7120819091797 | 113.45088195800781 | 2.7
 | 6 | 
212.12574768066406 | 84.4015884399414 | 223.7120819091797 | 95.01874542236328 | 2.8
 | 7 | 
200.4424591064453 | 114.38996124267578 | 211.05963134765625 | 149.2509765625 | Test Loss
 | 8 | 
362.9539489746094 | 108.38440704345703 | 409.4034118652344 | 154.15164184570312 | Wikipedia
Books
Internet Books
Common Crawl
WebText2 (Train)
WebText2 (Test)
 | 9 | 
89.99999237060547 | 218.66754150390625 | 521.999267578125 | 261.4480895996094 | Figure 24
We show evaluations on a series of datasets for models with approximately 1.5 Billion param-
eters. We observe no effect of depth on generalization; generalization performance depends primarily on
training distribution performance. The 12-layer model overﬁt the Internet Books dataset and we show the
early-stopped performance; we have not seen this surprising result in other experiments.
 | 10 | 
89.99999237060547 | 289.8351745605469 | 164.17005920410156 | 301.7903747558594 | List of Figures
 | 11 | 
104.9439926147461 | 319.8464660644531 | 521.99853515625 | 329.80908203125 | 1
Summary of simple power laws. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
 | 12 | 
104.9439926147461 | 336.91546630859375 | 521.99853515625 | 346.8780822753906 | 2
Illustration of sample efﬁciency and compute efﬁciency. . . . . . . . . . . . . . . . . . . . .
4
 | 13 | 
104.9439926147461 | 353.9854736328125 | 521.99853515625 | 363.9480895996094 | 3
How to scale up model size, batch size, and serial steps . . . . . . . . . . . . . . . . . . . .
4
 | 14 | 
104.9439926147461 | 371.0544738769531 | 521.9983520507812 | 381.01708984375 | 4
Performance when varying model and data size, or model and training steps, simultaneously
5
 | 15 | 
104.9439926147461 | 388.1244812011719 | 521.9984130859375 | 398.08709716796875 | 5
Weak dependence of performance on hyperparameter tuning . . . . . . . . . . . . . . . . .
8
 | 16 | 
104.9439926147461 | 405.1934814453125 | 521.9983520507812 | 415.1560974121094 | 6
Comparison of performance trend when including or excluding embeddings . . . . . . . . .
8
 | 17 | 
104.9439926147461 | 422.26348876953125 | 521.9984741210938 | 432.2261047363281 | 7
LSTM and Transformer performance comparison . . . . . . . . . . . . . . . . . . . . . . .
9
 | 18 | 
104.9439926147461 | 439.3324890136719 | 521.9985961914062 | 449.29510498046875 | 8
Generalization to other test datasets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
 | 19 | 
104.9439926147461 | 456.4024963378906 | 521.99853515625 | 466.3651123046875 | 9
Universality of overﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
 | 20 | 
104.9439926147461 | 473.47149658203125 | 521.99853515625 | 483.4341125488281 | 10
Critical batch size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
 | 21 | 
104.9439926147461 | 490.54150390625 | 521.9984741210938 | 500.5041198730469 | 11
Performance versus compute budget or number of parameter updates . . . . . . . . . . . . .
14
 | 22 | 
104.9439926147461 | 507.6104736328125 | 521.99853515625 | 517.5730590820312 | 12
Training on suboptimal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
 | 23 | 
104.9439926147461 | 524.6805419921875 | 521.9984741210938 | 534.6431274414062 | 13
Comparison between empirical and adjusted compute trends . . . . . . . . . . . . . . . . .
15
 | 24 | 
104.9439926147461 | 541.74951171875 | 521.9984741210938 | 551.7120971679688 | 14
Optimal model size and serial number of steps versus compute budget . . . . . . . . . . . .
16
 | 25 | 
104.9439926147461 | 558.8195190429688 | 521.99853515625 | 568.7821044921875 | 15
Contradiction between compute and data trends . . . . . . . . . . . . . . . . . . . . . . . .
17
 | 26 | 
104.9439926147461 | 575.8885498046875 | 521.9983520507812 | 585.8511352539062 | 16
Early stopping lower bound and training curves for overﬁt models
. . . . . . . . . . . . . .
23
 | 27 | 
104.9439926147461 | 592.95849609375 | 521.99853515625 | 602.9210815429688 | 17
Universal transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
 | 28 | 
104.9439926147461 | 610.0275268554688 | 521.99853515625 | 619.9901123046875 | 18
Batch size scans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
 | 29 | 
104.9439926147461 | 627.0975341796875 | 521.99853515625 | 637.0601196289062 | 19
Another look at sample efﬁciency
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
 | 30 | 
104.9439926147461 | 644.16650390625 | 521.9984130859375 | 654.1290893554688 | 20
Power-law dependence of performance on position in context . . . . . . . . . . . . . . . . .
25
 | 31 | 
104.9439926147461 | 661.2365112304688 | 521.9984741210938 | 671.1990966796875 | 21
Performance at different context positions versus model size
. . . . . . . . . . . . . . . . .
25
 | 32 | 
104.9439926147461 | 678.3055419921875 | 521.9985961914062 | 688.2681274414062 | 22
Learning rate schedule scan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
 | 33 | 
104.9439926147461 | 695.3755493164062 | 521.9984130859375 | 705.338134765625 | 23
Comparison of Power-Law and Logarithmic Fits
. . . . . . . . . . . . . . . . . . . . . . .
26
 | 34 | 
104.9439926147461 | 712.4445190429688 | 521.9985961914062 | 722.4071044921875 | 24
Generalization versus depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
 | 35 | 
301.01898193359375 | 742.33251953125 | 310.9815673828125 | 752.2951049804688 | 27
 | 36 | 
List of Tables
1
Parameter and compute counts for Transformer . . . . . . . . . . . . . . . . . . . . . . . .
7
2
Fits to L(N, D) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
3
Fits to L(N, S) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
4
Key trend equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
5
Key parameters to trend ﬁts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
6
Trends for compute-efﬁcient training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
References
[ACDE12]
Eduardo G Altmann, Giampaolo Cristadoro, and Mirko Degli Esposti. On the origin of long-
range correlations in texts. Proceedings of the National Academy of Sciences, 109(29):11582–
11587, 2012. 25
[AS17]
Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in
neural networks. arXiv, 2017, 1710.03667. 11, 18, 22
[BB01]
Michele Banko and Eric Brill. Scaling to very very large corpora for natural language disam-
biguation. In Proceedings of the 39th annual meeting on association for computational linguis-
tics, pages 26–33. Association for Computational Linguistics, 2001. 18
[BHMM18] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine
learning and the bias-variance trade-off. arXiv, 2018, 1812.11118. 18
[Bia12]
GÃŠrard Biau. Analysis of a random forests model. Journal of Machine Learning Research,
13(Apr):1063–1095, 2012. 18
[CGRS19]
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with
sparse transformers. CoRR, abs/1904.10509, 2019, 1904.10509. URL http://arxiv.org/
abs/1904.10509. 19
[DCLT18]
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding, 2018, arXiv:1810.04805. 2
[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Uni-
versal transformers. CoRR, abs/1807.03819, 2018, 1807.03819. URL http://arxiv.org/
abs/1807.03819. 6, 9, 23, 24
[EP94]
Werner Ebeling and Thorsten Pöschel. Entropy and long-range correlations in literary english.
EPL (Europhysics Letters), 26(4):241, 1994. 25
[Fou]
The Common Crawl Foundation. Common crawl. URL http://commoncrawl.org. 7
[GARD18] Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.
2018, arXiv:1812.04754. 18
[GJS+19]
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stéphane d’Ascoli,
Giulio Biroli, Clément Hongler, and Matthieu Wyart. Scaling description of generalization with
number of parameters in deep learning. arXiv, 2019, 1901.01608. 18
[GKX19]
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net op-
timization via hessian eigenvalue density. CoRR, abs/1901.10159, 2019, 1901.10159. URL
http://arxiv.org/abs/1901.10159. 18
[Goo01]
Joshua Goodman. A bit of progress in language modeling. CoRR, cs.CL/0108005, 2001. URL
http://arxiv.org/abs/cs.CL/0108005. 18
[GRK17]
Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. ope-
nai.com, 2017. 19
[HAD19]
Joel Hestness, Newsha Ardalani, and Gregory Diamos. Beyond human-level accuracy: Compu-
tational challenges in deep learning. In Proceedings of the 24th Symposium on Principles and
Practice of Parallel Programming, PPoPP ’19, pages 1–14, New York, NY, USA, 2019. ACM.
doi:10.1145/3293883.3295710. 18
28

90.0 | 72.78716278076172 | 158.64675903320312 | 84.74236297607422 | List of Tables
 |  | 
104.94400024414062 | 97.91450500488281 | 521.9984741210938 | 107.87710571289062 | 1
Parameter and compute counts for Transformer . . . . . . . . . . . . . . . . . . . . . . . .
7
 | 1 | 
104.94400024414062 | 114.12297058105469 | 521.9974365234375 | 124.31613159179688 | 2
Fits to L(N, D) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
 | 2 | 
104.94400024414062 | 130.56297302246094 | 521.9974365234375 | 140.75613403320312 | 3
Fits to L(N, S) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
 | 3 | 
104.94400024414062 | 147.23255920410156 | 521.9984741210938 | 157.19515991210938 | 4
Key trend equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
 | 4 | 
104.94400024414062 | 163.6725616455078 | 521.9985961914062 | 173.63516235351562 | 5
Key parameters to trend ﬁts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
 | 5 | 
104.94400024414062 | 180.11158752441406 | 521.99853515625 | 190.07418823242188 | 6
Trends for compute-efﬁcient training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
 | 6 | 
90.0 | 207.456298828125 | 145.54383850097656 | 219.4114990234375 | References
 | 7 | 
90.0 | 229.3006134033203 | 521.9981079101562 | 261.0812072753906 | [ACDE12]
Eduardo G Altmann, Giampaolo Cristadoro, and Mirko Degli Esposti. On the origin of long-
range correlations in texts. Proceedings of the National Academy of Sciences, 109(29):11582–
11587, 2012. 25
 | 8 | 
90.0 | 266.5216064453125 | 521.998046875 | 287.3932189941406 | [AS17]
Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in
neural networks. arXiv, 2017, 1710.03667. 11, 18, 22
 | 9 | 
89.99998474121094 | 292.8336181640625 | 522.002197265625 | 324.6142272949219 | [BB01]
Michele Banko and Eric Brill. Scaling to very very large corpora for natural language disam-
biguation. In Proceedings of the 39th annual meeting on association for computational linguis-
tics, pages 26–33. Association for Computational Linguistics, 2001. 18
 | 10 | 
89.9999771118164 | 330.05462646484375 | 521.9982299804688 | 350.9262390136719 | [BHMM18] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine
learning and the bias-variance trade-off. arXiv, 2018, 1812.11118. 18
 | 11 | 
89.99996948242188 | 357.3851623535156 | 521.9996337890625 | 378.4332275390625 | [Bia12]
GÃŠrard Biau. Analysis of a random forests model. Journal of Machine Learning Research,
13(Apr):1063–1095, 2012. 18
 | 12 | 
89.99996948242188 | 383.87261962890625 | 522.0213623046875 | 415.9022521972656 | [CGRS19]
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with
sparse transformers. CoRR, abs/1904.10509, 2019, 1904.10509. URL http://arxiv.org/
abs/1904.10509. 19
 | 13 | 
89.9999771118164 | 421.0936279296875 | 521.9981079101562 | 441.9652404785156 | [DCLT18]
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding, 2018, arXiv:1810.04805. 2
 | 14 | 
89.99996948242188 | 445.8949279785156 | 522.0213623046875 | 479.43426513671875 | [DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Uni-
versal transformers. CoRR, abs/1807.03819, 2018, 1807.03819. URL http://arxiv.org/
abs/1807.03819. 6, 9, 23, 24
 | 15 | 
89.9999771118164 | 484.62664794921875 | 521.998046875 | 505.4982604980469 | [EP94]
Werner Ebeling and Thorsten Pöschel. Entropy and long-range correlations in literary english.
EPL (Europhysics Letters), 26(4):241, 1994. 25
 | 16 | 
89.99998474121094 | 510.9376220703125 | 493.7554931640625 | 521.1482543945312 | [Fou]
The Common Crawl Foundation. Common crawl. URL http://commoncrawl.org. 7
 | 17 | 
90.0 | 526.3406372070312 | 521.9981079101562 | 547.2122192382812 | [GARD18] Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.
2018, arXiv:1812.04754. 18
 | 18 | 
90.0 | 551.1419067382812 | 521.9984130859375 | 584.4332275390625 | [GJS+19]
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stéphane d’Ascoli,
Giulio Biroli, Clément Hongler, and Matthieu Wyart. Scaling description of generalization with
number of parameters in deep learning. arXiv, 2019, 1901.01608. 18
 | 19 | 
90.0 | 589.8726196289062 | 521.998046875 | 621.9022827148438 | [GKX19]
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net op-
timization via hessian eigenvalue density. CoRR, abs/1901.10159, 2019, 1901.10159. URL
http://arxiv.org/abs/1901.10159. 18
 | 20 | 
89.99996948242188 | 626.9171752929688 | 522.0014038085938 | 648.2132568359375 | [Goo01]
Joshua Goodman. A bit of progress in language modeling. CoRR, cs.CL/0108005, 2001. URL
http://arxiv.org/abs/cs.CL/0108005. 18
 | 21 | 
89.99993896484375 | 653.2291870117188 | 522.00048828125 | 674.2772216796875 | [GRK17]
Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. ope-
nai.com, 2017. 19
 | 22 | 
89.99994659423828 | 679.7176513671875 | 522.0016479492188 | 722.4071655273438 | [HAD19]
Joel Hestness, Newsha Ardalani, and Gregory Diamos. Beyond human-level accuracy: Compu-
tational challenges in deep learning. In Proceedings of the 24th Symposium on Principles and
Practice of Parallel Programming, PPoPP ’19, pages 1–14, New York, NY, USA, 2019. ACM.
doi:10.1145/3293883.3295710. 18
 | 23 | 
301.0189208984375 | 742.3326416015625 | 310.98150634765625 | 752.2952270507812 | 28
 | 24 | 
[HCC+18]
Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,
and Zhifeng Chen. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism.
CoRR, abs/1811.06965, 2018, 1811.06965. URL http://arxiv.org/abs/1811.06965. 19
[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-
ninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-
dictable, empirically, 2017, 1712.00409. 18
[JGH18]
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pages
8571–8580, 2018. 18
[KB14]
Diederik P. Kingma and Jimmy Ba.
Adam: A method for stochastic optimization, 2014,
1412.6980. 7
[Kom19]
Aran Komatsuzaki. One epoch is all you need, 2019, arXiv:1906.06669. 18
[KSH12]
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Proceedings of the 25th International Conference on Neural
Information Processing Systems - Volume 1, NIPS’12, pages 1097–1105, USA, 2012. Curran
Associates Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257. 19
[LCG+19]
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut.
Albert: A lite bert for self-supervised learning of language representations, 2019,
1909.11942. 9
[LOG+19]
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretrain-
ing approach. CoRR, abs/1907.11692, 2019, 1907.11692. URL http://arxiv.org/abs/
1907.11692. 2
[LSP+18]
Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and
Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs],
2018, 1801.10198. URL http://arxiv.org/abs/1801.10198. 2, 6
[LT16]
Henry W Lin and Max Tegmark. Criticality in formal languages and statistical physics. arXiv
preprint arXiv:1606.06737, 2016. 25
[LXS+19]
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent, 2019, arXiv:1902.06720. 18
[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model
of large-batch training, 2018, arXiv:1812.06162. 3, 5, 6, 12, 13, 21
[Pap18]
Vardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size.
CoRR, abs/1811.07062, 2018, 1811.07062. URL http://arxiv.org/abs/1811.07062. 18
[RNSS18]
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018. 2, 6
[RRBS19a] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
A constructive
prediction of the generalization error across scales, 2019, 1909.12673. 18
[RRBS19b] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
A constructive
prediction of the generalization error across scales, 2019, arXiv:1909.12673. 18
[RSR+19]
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer, 2019, arXiv:1910.10683. 2
[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. openai.com, 2019. 2, 5, 6, 7, 8
[SCP+18]
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-
takool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and
Blake Hechtman. Mesh-tensorﬂow: Deep learning for supercomputers, 2018, 1811.02084. 19
[SHB15]
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
with subword units. CoRR, 2015, 1508.07909. 6
29

90.0 | 72.89674377441406 | 521.998291015625 | 106.43610382080078 | [HCC+18]
Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,
and Zhifeng Chen. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism.
CoRR, abs/1811.06965, 2018, 1811.06965. URL http://arxiv.org/abs/1811.06965. 19
 |  | 
90.00003051757812 | 109.92274475097656 | 521.99853515625 | 143.21408081054688 | [HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-
ninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-
dictable, empirically, 2017, 1712.00409. 18
 | 1 | 
90.00003051757812 | 148.4584503173828 | 522.004638671875 | 180.24002075195312 | [JGH18]
Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pages
8571–8580, 2018. 18
 | 2 | 
90.0 | 185.48439025878906 | 521.9982299804688 | 206.35598754882812 | [KB14]
Diederik P. Kingma and Jimmy Ba.
Adam: A method for stochastic optimization, 2014,
1412.6980. 7
 | 3 | 
90.0 | 211.6013946533203 | 444.3995056152344 | 221.56399536132812 | [Kom19]
Aran Komatsuzaki. One epoch is all you need, 2019, arXiv:1906.06669. 18
 | 4 | 
90.0 | 226.8094024658203 | 522.0045166015625 | 269.74700927734375 | [KSH12]
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Proceedings of the 25th International Conference on Neural
Information Processing Systems - Volume 1, NIPS’12, pages 1097–1105, USA, 2012. Curran
Associates Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257. 19
 | 5 | 
90.00006103515625 | 273.233642578125 | 521.99853515625 | 306.52496337890625 | [LCG+19]
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut.
Albert: A lite bert for self-supervised learning of language representations, 2019,
1909.11942. 9
 | 6 | 
90.00004577636719 | 310.2596435546875 | 522.0169677734375 | 354.7079772949219 | [LOG+19]
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretrain-
ing approach. CoRR, abs/1907.11692, 2019, 1907.11692. URL http://arxiv.org/abs/
1907.11692. 2
 | 7 | 
90.00006866455078 | 358.19464111328125 | 521.9996948242188 | 391.7339782714844 | [LSP+18]
Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and
Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs],
2018, 1801.10198. URL http://arxiv.org/abs/1801.10198. 2, 6
 | 8 | 
90.00003051757812 | 396.5549011230469 | 521.9999389648438 | 417.60296630859375 | [LT16]
Henry W Lin and Max Tegmark. Criticality in formal languages and statistical physics. arXiv
preprint arXiv:1606.06737, 2016. 25
 | 9 | 
90.00003051757812 | 421.337646484375 | 521.99853515625 | 454.6279602050781 | [LXS+19]
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent, 2019, arXiv:1902.06720. 18
 | 10 | 
90.00003051757812 | 459.87335205078125 | 521.9982299804688 | 480.7449645996094 | [MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model
of large-batch training, 2018, arXiv:1812.06162. 3, 5, 6, 12, 13, 21
 | 11 | 
90.00003051757812 | 485.9903564453125 | 521.9982299804688 | 507.1099853515625 | [Pap18]
Vardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size.
CoRR, abs/1811.07062, 2018, 1811.07062. URL http://arxiv.org/abs/1811.07062. 18
 | 12 | 
90.00003051757812 | 512.1073608398438 | 522.001953125 | 543.887939453125 | [RNSS18]
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-
assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018. 2, 6
 | 13 | 
90.0 | 549.1333618164062 | 521.998046875 | 570.0049438476562 | [RRBS19a] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
A constructive
prediction of the generalization error across scales, 2019, 1909.12673. 18
 | 14 | 
90.0 | 575.2493896484375 | 521.998046875 | 596.1219482421875 | [RRBS19b] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
A constructive
prediction of the generalization error across scales, 2019, arXiv:1909.12673. 18
 | 15 | 
90.0 | 599.8566284179688 | 521.9984741210938 | 633.1469116210938 | [RSR+19]
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer, 2019, arXiv:1910.10683. 2
 | 16 | 
90.0 | 636.8826293945312 | 521.9976196289062 | 659.2639770507812 | [RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. openai.com, 2019. 2, 5, 6, 7, 8
 | 17 | 
90.0 | 662.9996337890625 | 521.99853515625 | 696.2899780273438 | [SCP+18]
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-
takool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and
Blake Hechtman. Mesh-tensorﬂow: Deep learning for supercomputers, 2018, 1811.02084. 19
 | 18 | 
90.0 | 701.5353393554688 | 521.9981689453125 | 722.4069213867188 | [SHB15]
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
with subword units. CoRR, 2015, 1508.07909. 6
 | 19 | 
301.0190124511719 | 742.3323974609375 | 310.9815979003906 | 752.2949829101562 | 29
 | 20 | 
[SLA+18]
Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E. Dahl. Measuring the effects of data parallelism on neural network training, 2018,
arXiv:1811.03600. 12
[SS18]
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory
cost. CoRR, abs/1804.04235, 2018, 1804.04235. URL http://arxiv.org/abs/1804.04235.
7
[THK18]
Stefan Thurner, Rudolf Hanel, and Peter Klimek. Introduction to the theory of complex systems.
Oxford University Press, 2018. 18
[TL19]
Mingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural
networks. CoRR, abs/1905.11946, 2019, 1905.11946. URL http://arxiv.org/abs/1905.
11946. 18
[VSP+17]
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf. 2, 6
[VWB16]
Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles
of relatively shallow networks, 2016, arXiv:1605.06431. 8, 18
[Was06]
Larry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.
18
[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose
language understanding systems, 2019, 1905.00537. 2
[WRH17]
Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by in-
creasing model capacity. 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), Jul 2017. doi:10.1109/cvpr.2017.323. 19
[WYL19]
Wei Wen, Feng Yan, and Hai Li. Autogrow: Automatic layer growing in deep convolutional
networks, 2019, 1906.02909. 19
[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V.
Le.
Xlnet:
Generalized autoregressive pretraining for language understanding, 2019,
arXiv:1906.08237. 2
[ZK16]
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British
Machine Vision Conference 2016, 2016. doi:10.5244/c.30.87. 18
[ZKZ+15]
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-
ralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. 2015 IEEE International Conference on Computer Vision
(ICCV), Dec 2015. doi:10.1109/iccv.2015.11. 7
[ZLN+19]
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,
Christopher J. Shallue, and Roger B. Grosse. Which algorithmic choices matter at which batch
sizes? insights from a noisy quadratic model. CoRR, abs/1907.04164, 2019, 1907.04164. URL
http://arxiv.org/abs/1907.04164. 12, 18
30

89.99996948242188 | 72.89674377441406 | 522.0227661132812 | 572.7869262695312 | [SLA+18]
Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and
George E. Dahl. Measuring the effects of data parallelism on neural network training, 2018,
arXiv:1811.03600. 12
[SS18]
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory
cost. CoRR, abs/1804.04235, 2018, 1804.04235. URL http://arxiv.org/abs/1804.04235.
7
[THK18]
Stefan Thurner, Rudolf Hanel, and Peter Klimek. Introduction to the theory of complex systems.
Oxford University Press, 2018. 18
[TL19]
Mingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural
networks. CoRR, abs/1905.11946, 2019, 1905.11946. URL http://arxiv.org/abs/1905.
11946. 18
[VSP+17]
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural
Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf. 2, 6
[VWB16]
Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles
of relatively shallow networks, 2016, arXiv:1605.06431. 8, 18
[Was06]
Larry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.
18
[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose
language understanding systems, 2019, 1905.00537. 2
[WRH17]
Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by in-
creasing model capacity. 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), Jul 2017. doi:10.1109/cvpr.2017.323. 19
[WYL19]
Wei Wen, Feng Yan, and Hai Li. Autogrow: Automatic layer growing in deep convolutional
networks, 2019, 1906.02909. 19
[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V.
Le.
Xlnet:
Generalized autoregressive pretraining for language understanding, 2019,
arXiv:1906.08237. 2
[ZK16]
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British
Machine Vision Conference 2016, 2016. doi:10.5244/c.30.87. 18
[ZKZ+15]
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-
ralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. 2015 IEEE International Conference on Computer Vision
(ICCV), Dec 2015. doi:10.1109/iccv.2015.11. 7
[ZLN+19]
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,
Christopher J. Shallue, and Roger B. Grosse. Which algorithmic choices matter at which batch
sizes? insights from a noisy quadratic model. CoRR, abs/1907.04164, 2019, 1907.04164. URL
http://arxiv.org/abs/1907.04164. 12, 18
 |  | 
301.0189514160156 | 742.332275390625 | 310.9815368652344 | 752.2948608398438 | 30
 | 1 | 











